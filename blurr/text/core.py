# AUTOGENERATED! DO NOT EDIT! File to edit: ../../nbs/10_text-core.ipynb.

# %% ../../nbs/10_text-core.ipynb 3
from __future__ import annotations

import gc, importlib, sys, traceback

from accelerate.logging import get_logger
from dataclasses import dataclass
from datasets import concatenate_datasets, load_dataset
from dotenv import load_dotenv
from fastai.callback.all import *
from fastai.imports import *
from fastai.learner import *
from fastai.losses import BaseLoss, BCEWithLogitsLossFlat, CrossEntropyLossFlat
from fastai.data.transforms import DataLoaders, Datasets, ColSplitter, ItemTransform
from fastai.optimizer import Adam, OptimWrapper, params
from fastai.metrics import accuracy, F1Score, accuracy_multi, F1ScoreMulti
from fastai.test_utils import show_install
from fastai.torch_core import *
from fastai.torch_imports import *
from transformers import (
    AutoConfig,
    AutoTokenizer,
    PretrainedConfig,
    PreTrainedTokenizerBase,
    PreTrainedModel,
)
from transformers import AutoModelForSequenceClassification
from transformers import logging as hf_logging
from transformers.data.data_collator import DataCollatorWithPadding

from ..utils import clean_memory, get_hf_objects, set_seed, PreCalculatedLoss

# %% auto 0
__all__ = ['logger', 'TextCollatorWithPadding', 'blurr_splitter', 'BaseModelWrapper', 'BaseModelCallback']

# %% ../../nbs/10_text-core.ipynb 6
# silence all the HF warnings and load environment variables
warnings.simplefilter("ignore")
hf_logging.set_verbosity_error()
logger = get_logger(__name__)

load_dotenv()

# %% ../../nbs/10_text-core.ipynb 14
@dataclass
class TextCollatorWithPadding:
    def __init__(
        self,
        # A Hugging Face tokenizer
        hf_tokenizer: PreTrainedTokenizerBase,
        # The abbreviation/name of your Hugging Face transformer architecture (e.b., bert, bart, etc..)
        hf_arch: str = None,
        # A specific configuration instance you want to use
        hf_config: PretrainedConfig = None,
        # A Hugging Face model
        hf_model: PreTrainedModel = None,
        # The number of inputs expected by your model
        n_inp: int = 1,
        # Defaults to use Hugging Face's DataCollatorWithPadding(tokenizer=hf_tokenizer)
        data_collator_cls: type = DataCollatorWithPadding,
        # kwyargs specific for the instantiation of the `data_collator`
        data_collator_kwargs: dict = {},
    ):
        store_attr()

        self.hf_tokenizer = data_collator_kwargs.pop("tokenizer", self.hf_tokenizer)
        self.data_collator = data_collator_cls(
            tokenizer=self.hf_tokenizer, **data_collator_kwargs
        )

    def __call__(self, features):
        features = L(features)
        inputs, labels, targs = [], [], []

        if isinstance(features[0], dict):
            feature_keys = list(features[0].keys())
            inputs = [
                {
                    fwd_arg_name: list(features.attrgot(fwd_arg_name))
                    for fwd_arg_name in self.hf_tokenizer.model_input_names
                    if fwd_arg_name in feature_keys
                }
            ]
            labels = [
                torch.tensor(
                    list(features.attrgot("label")) if "label" in feature_keys else []
                )
            ]
            targs = labels
        elif isinstance(features[0], tuple):
            for f_idx in range(self.n_inp):
                feature_keys = list(features[0][f_idx].keys())
                inputs.append(
                    {
                        fwd_arg_name: list(
                            features.itemgot(f_idx).attrgot(fwd_arg_name)
                        )
                        for fwd_arg_name in self.hf_tokenizer.model_input_names
                        if fwd_arg_name in feature_keys
                    }
                )
                labels.append(
                    torch.tensor(
                        list(features.itemgot(f_idx).attrgot("label"))
                        if "label" in feature_keys
                        else []
                    )
                )

            targs = [
                torch.tensor(list(features.itemgot(f_idx)))
                for f_idx in range(self.n_inp, len(features[0]))
            ]

        return self._build_batch(inputs, labels, targs)

    def _build_batch(self, inputs, labels, targs):
        batch = []

        for input, input_labels in zip(inputs, labels):
            if len(input_labels) > 0:
                input["labels"] = input_labels
            batch.append(dict(self.data_collator(input)))

        for targ in targs:
            batch.append(targ)

        return batch

# %% ../../nbs/10_text-core.ipynb 17
def blurr_splitter(m: Module):
    """Splits the Hugging Face model based on various model architecture conventions"""
    model = m.hf_model if (hasattr(m, "hf_model")) else m
    root_modules = list(model.named_children())
    top_module_name, top_module = root_modules[0]

    groups = L([m for m_name, m in list(top_module.named_children())])
    groups += L([m for m_name, m in root_modules[1:]])

    return groups.map(params).filter(lambda el: len(el) > 0)

# %% ../../nbs/10_text-core.ipynb 22
class BaseModelWrapper(Module):
    def __init__(
        self,
        # Your Hugging Face model
        hf_model: PreTrainedModel,
        # If True, hidden_states will be returned and accessed from Learner
        output_hidden_states: bool = False,
        # If True, attentions will be returned and accessed from Learner
        output_attentions: bool = False,
        # Any additional keyword arguments you want passed into your models forward method
        hf_model_kwargs={},
    ):
        super().__init__()

        store_attr()
        self.hf_model = hf_model.cuda() if torch.cuda.is_available() else hf_model
        self.hf_model_fwd_args = list(
            inspect.signature(self.hf_model.forward).parameters.keys()
        )

    def forward(self, x):
        for k in list(x):
            if k not in self.hf_model_fwd_args:
                del x[k]

        return self.hf_model(
            **x,
            output_hidden_states=self.output_hidden_states,
            output_attentions=self.output_attentions,
            return_dict=True,
            **self.hf_model_kwargs
        )

# %% ../../nbs/10_text-core.ipynb 25
class BaseModelCallback(Callback):
    def __init__(
        self,
        # Additional keyword arguments passed to `BaseModelWrapper`
        base_model_wrapper_kwargs: dict = {},
    ):
        self.base_model_wrapper_kwargs = base_model_wrapper_kwargs

    def after_create(self):
        if isinstance(self.learn.model, PreTrainedModel):
            self.learn.model = BaseModelWrapper(
                self.learn.model, **self.base_model_wrapper_kwargs
            )

    def before_batch(self):
        self.hf_loss = None

    def after_pred(self):
        model_outputs = self.pred
        self.learn.blurr_model_outputs = {}

        for k, v in model_outputs.items():
            # if the "labels" are included, we are training with target labels in which case the loss is returned
            if k == "loss" and isinstance(self.learn.loss_func, PreCalculatedLoss):
                self.hf_loss = to_float(v)
            # the logits represent the prediction
            elif k == "logits":
                self.learn.pred = v
            # add any other things included in model_outputs as blurr_{model_output_key}
            else:
                self.learn.blurr_model_outputs[k] = v

    def after_loss(self):
        # if we already have the loss from the model, update the Learner's loss to be it
        if self.hf_loss is not None:
            self.learn.loss_grad = self.hf_loss
            self.learn.loss = self.learn.loss_grad.clone()
