<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.1.75">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="description" content="The text.data.core module contains the core bits required to use the fastai DataBlock API and/or mid-level data processing pipelines to turn your raw datasets into modelable DataLoaders for text/NLP tasks">

<title>blurr - Data</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: 1;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./favicon.ico" rel="icon">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-dark.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="site_libs/bootstrap/bootstrap-dark.min.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" integrity="sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script>
<script type="application/javascript">define('jquery', [],function() {return window.jQuery;})</script>
<script src="https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js" crossorigin="anonymous"></script>


<link rel="stylesheet" href="styles.css">
<meta property="og:title" content="blurr - Data">
<meta property="og:description" content="The text.data.core module contains the core bits required to use the fastai DataBlock API and/or mid-level data processing pipelines to turn your raw datasets into modelable DataLoaders for text/NLP tasks">
<meta property="og:site-name" content="blurr">
<meta name="twitter:title" content="blurr - Data">
<meta name="twitter:description" content="The text.data.core module contains the core bits required to use the fastai DataBlock API and/or mid-level data processing pipelines to turn your raw datasets into modelable DataLoaders for text/NLP tasks">
<meta name="twitter:card" content="summary">
</head>

<body class="nav-sidebar floating nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <a class="navbar-brand" href="./index.html">
    <span class="navbar-title">blurr</span>
  </a>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link active" href="./index.html" aria-current="page">Getting Started</a>
  </li>  
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-resources" role="button" data-bs-toggle="dropdown" aria-expanded="false">Resources</a>
    <ul class="dropdown-menu" aria-labelledby="nav-menu-resources">    
        <li>
    <a class="dropdown-item" href="https://www.youtube.com/playlist?list=PLD80i8An1OEF8UOb9N9uSoidOGIMKW96t"><i class="bi bi-play-btn-fill" role="img">
</i> 
 <span class="dropdown-text">fastai x Hugging Face Study Group</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="https://huggingface.co/course/chapter1/1"><i class="bi bi-journal-bookmark-fill" role="img">
</i> 
 <span class="dropdown-text">Hugging Face Course</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="https://docs.fast.ai/"><i class="bi bi-link" role="img">
</i> 
 <span class="dropdown-text">fast.ai (docs)</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="https://huggingface.co/docs/transformers/index"><i class="bi bi-link" role="img">
</i> 
 <span class="dropdown-text">transformers (docs)</span></a>
  </li>  
    </ul>
  </li>
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-help" role="button" data-bs-toggle="dropdown" aria-expanded="false">Help</a>
    <ul class="dropdown-menu" aria-labelledby="nav-menu-help">    
        <li>
    <a class="dropdown-item" href="https://github.com/ohmeow/blurr/issues"><i class="bi bi-bug" role="img">
</i> 
 <span class="dropdown-text">Report an Issue</span></a>
  </li>  
    </ul>
  </li>
</ul>
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/ohmeow/blurr/"><i class="bi bi-github" role="img">
</i> 
 </a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://twitter.com/waydegilliam"><i class="bi bi-twitter" role="img">
</i> 
 </a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://www.ohmeow.com/"><i class="bi bi-house" role="img" aria-label="Twitter">
</i> 
 </a>
  </li>  
</ul>
              <div class="quarto-toggle-container">
                  <a href="" class="quarto-color-scheme-toggle nav-link" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
              </div>
              <div id="quarto-search" class="" title="Search"></div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
  <nav class="quarto-secondary-nav" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
    <div class="container-fluid d-flex justify-content-between">
      <h1 class="quarto-secondary-nav-title">Data</h1>
      <button type="button" class="quarto-btn-toggle btn" aria-label="Show secondary navigation">
        <i class="bi bi-chevron-right"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-full page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse sidebar-navigation floating overflow-auto">
      <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
      </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true">Overview</a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">Getting Started</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./callbacks.html" class="sidebar-item-text sidebar-link">callbacks</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./utils.html" class="sidebar-item-text sidebar-link">utils</a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="true">Text</a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" aria-expanded="true">Sequence Classification</a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth2 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./text.data.core.html" class="sidebar-item-text sidebar-link active">Data</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./text.modeling.core.html" class="sidebar-item-text sidebar-link">Modeling</a>
  </div>
</li>
      </ul>
  </li>
          <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" aria-expanded="false">Token Classification</a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" aria-expanded="false">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth2 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./text.data.token_classification.html" class="sidebar-item-text sidebar-link">Data</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./text.modeling.token_classification.html" class="sidebar-item-text sidebar-link">Modeling</a>
  </div>
</li>
      </ul>
  </li>
          <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" aria-expanded="false">Question &amp; Answering</a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" aria-expanded="false">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-5" class="collapse list-unstyled sidebar-section depth2 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./text.data.question_answering.html" class="sidebar-item-text sidebar-link">Data</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./text.modeling.question_answering.html" class="sidebar-item-text sidebar-link">Modeling</a>
  </div>
</li>
      </ul>
  </li>
          <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" aria-expanded="false">Language Modeling</a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" aria-expanded="false">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-6" class="collapse list-unstyled sidebar-section depth2 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./text.data.language_modeling.html" class="sidebar-item-text sidebar-link">Data</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./text.modeling.language_modeling.html" class="sidebar-item-text sidebar-link">Modeling</a>
  </div>
</li>
      </ul>
  </li>
          <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-7" aria-expanded="false">Seq2Seq: Core</a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-7" aria-expanded="false">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-7" class="collapse list-unstyled sidebar-section depth2 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./text.data.seq2seq.core.html" class="sidebar-item-text sidebar-link">Data</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./text.modeling.seq2seq.core.html" class="sidebar-item-text sidebar-link">Modeling</a>
  </div>
</li>
      </ul>
  </li>
          <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-8" aria-expanded="false">Seq2Seq: Summarization</a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-8" aria-expanded="false">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-8" class="collapse list-unstyled sidebar-section depth2 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./text.data.seq2seq.summarization.html" class="sidebar-item-text sidebar-link">Data</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./text.modeling.seq2seq.summarization.html" class="sidebar-item-text sidebar-link">Modeling</a>
  </div>
</li>
      </ul>
  </li>
          <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-9" aria-expanded="false">Seq2Seq: Translation</a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-9" aria-expanded="false">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-9" class="collapse list-unstyled sidebar-section depth2 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./text.data.seq2seq.translation.html" class="sidebar-item-text sidebar-link">Data</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./text.modeling.seq2seq.translation.html" class="sidebar-item-text sidebar-link">Modeling</a>
  </div>
</li>
      </ul>
  </li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./text.callbacks.html" class="sidebar-item-text sidebar-link">callbacks</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./text.utils.html" class="sidebar-item-text sidebar-link">utils</a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-10" aria-expanded="true">Examples</a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-10" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-10" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./examples.text.high_level_api.html" class="sidebar-item-text sidebar-link">Using the high-level Blurr API</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./examples.text.glue.html" class="sidebar-item-text sidebar-link">GLUE classification tasks</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./examples.text.glue_low_level_api.html" class="sidebar-item-text sidebar-link">Using the Low-level fastai API</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./examples.text.multilabel_classification.html" class="sidebar-item-text sidebar-link">Multi-label classification</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./examples.text.causal_lm_gpt2.html" class="sidebar-item-text sidebar-link">Causal Language Modeling with GPT-2</a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#setup" id="toc-setup" class="nav-link active" data-scroll-target="#setup">Setup</a></li>
  <li><a href="#preprocessing" id="toc-preprocessing" class="nav-link" data-scroll-target="#preprocessing">Preprocessing</a>
  <ul>
  <li><a href="#preprocessor" id="toc-preprocessor" class="nav-link" data-scroll-target="#preprocessor">Preprocessor</a></li>
  <li><a href="#classificationpreprocessor" id="toc-classificationpreprocessor" class="nav-link" data-scroll-target="#classificationpreprocessor">ClassificationPreprocessor</a>
  <ul class="collapse">
  <li><a href="#using-a-dataframe" id="toc-using-a-dataframe" class="nav-link" data-scroll-target="#using-a-dataframe">Using a <code>DataFrame</code></a></li>
  <li><a href="#using-a-hugging-face-dataset" id="toc-using-a-hugging-face-dataset" class="nav-link" data-scroll-target="#using-a-hugging-face-dataset">Using a Hugging Face <code>Dataset</code></a></li>
  </ul></li>
  </ul></li>
  <li><a href="#mid-level-api" id="toc-mid-level-api" class="nav-link" data-scroll-target="#mid-level-api">Mid-level API</a>
  <ul>
  <li><a href="#textinput" id="toc-textinput" class="nav-link" data-scroll-target="#textinput">TextInput</a></li>
  <li><a href="#batchtokenizetransform" id="toc-batchtokenizetransform" class="nav-link" data-scroll-target="#batchtokenizetransform">BatchTokenizeTransform</a></li>
  <li><a href="#batchdecodetransform" id="toc-batchdecodetransform" class="nav-link" data-scroll-target="#batchdecodetransform">BatchDecodeTransform</a></li>
  <li><a href="#blurr_sort_func" id="toc-blurr_sort_func" class="nav-link" data-scroll-target="#blurr_sort_func">blurr_sort_func</a></li>
  <li><a href="#textblock" id="toc-textblock" class="nav-link" data-scroll-target="#textblock">TextBlock</a></li>
  </ul></li>
  <li><a href="#utility-classes-and-methods" id="toc-utility-classes-and-methods" class="nav-link" data-scroll-target="#utility-classes-and-methods">Utility classes and methods</a>
  <ul>
  <li><a href="#get_blurr_tfm" id="toc-get_blurr_tfm" class="nav-link" data-scroll-target="#get_blurr_tfm">get_blurr_tfm</a></li>
  <li><a href="#first_blurr_tfm" id="toc-first_blurr_tfm" class="nav-link" data-scroll-target="#first_blurr_tfm">first_blurr_tfm</a></li>
  </ul></li>
  <li><a href="#mid-level-examples" id="toc-mid-level-examples" class="nav-link" data-scroll-target="#mid-level-examples">Mid-level Examples</a>
  <ul>
  <li><a href="#batch-time-tokenization" id="toc-batch-time-tokenization" class="nav-link" data-scroll-target="#batch-time-tokenization">Batch-Time Tokenization</a>
  <ul class="collapse">
  <li><a href="#step-1-get-your-hugging-face-objects." id="toc-step-1-get-your-hugging-face-objects." class="nav-link" data-scroll-target="#step-1-get-your-hugging-face-objects.">Step 1: Get your Hugging Face objects.</a></li>
  <li><a href="#step-2-create-your-datablock" id="toc-step-2-create-your-datablock" class="nav-link" data-scroll-target="#step-2-create-your-datablock">Step 2: Create your <code>DataBlock</code></a></li>
  <li><a href="#step-3-build-your-dataloaders" id="toc-step-3-build-your-dataloaders" class="nav-link" data-scroll-target="#step-3-build-your-dataloaders">Step 3: Build your <code>DataLoaders</code></a></li>
  </ul></li>
  <li><a href="#using-a-preprocessed-dataset" id="toc-using-a-preprocessed-dataset" class="nav-link" data-scroll-target="#using-a-preprocessed-dataset">Using a preprocessed dataset</a>
  <ul class="collapse">
  <li><a href="#step-1a-get-your-hugging-face-objects." id="toc-step-1a-get-your-hugging-face-objects." class="nav-link" data-scroll-target="#step-1a-get-your-hugging-face-objects.">Step 1a: Get your Hugging Face objects.</a></li>
  <li><a href="#step-1b.-preprocess-dataset" id="toc-step-1b.-preprocess-dataset" class="nav-link" data-scroll-target="#step-1b.-preprocess-dataset">Step 1b. Preprocess dataset</a></li>
  <li><a href="#step-2-create-your-datablock-1" id="toc-step-2-create-your-datablock-1" class="nav-link" data-scroll-target="#step-2-create-your-datablock-1">Step 2: Create your <code>DataBlock</code></a></li>
  <li><a href="#step-3-build-your-dataloaders-1" id="toc-step-3-build-your-dataloaders-1" class="nav-link" data-scroll-target="#step-3-build-your-dataloaders-1">Step 3: Build your <code>DataLoaders</code></a></li>
  </ul></li>
  <li><a href="#passing-extra-information" id="toc-passing-extra-information" class="nav-link" data-scroll-target="#passing-extra-information">Passing extra information</a></li>
  </ul></li>
  <li><a href="#low-level-api" id="toc-low-level-api" class="nav-link" data-scroll-target="#low-level-api">Low-level API</a>
  <ul>
  <li><a href="#textbatchcreator" id="toc-textbatchcreator" class="nav-link" data-scroll-target="#textbatchcreator">TextBatchCreator</a></li>
  <li><a href="#textdataloader" id="toc-textdataloader" class="nav-link" data-scroll-target="#textdataloader">TextDataLoader</a></li>
  </ul></li>
  <li><a href="#low-level-examples" id="toc-low-level-examples" class="nav-link" data-scroll-target="#low-level-examples">Low-level Examples</a>
  <ul>
  <li><a href="#step-1-build-your-datasets" id="toc-step-1-build-your-datasets" class="nav-link" data-scroll-target="#step-1-build-your-datasets">Step 1: Build your datasets</a></li>
  <li><a href="#step-2-dataset-pre-processing-optional" id="toc-step-2-dataset-pre-processing-optional" class="nav-link" data-scroll-target="#step-2-dataset-pre-processing-optional">Step 2: Dataset pre-processing (optional)</a>
  <ul class="collapse">
  <li><a href="#preproc_hf_dataset" id="toc-preproc_hf_dataset" class="nav-link" data-scroll-target="#preproc_hf_dataset">preproc_hf_dataset</a></li>
  </ul></li>
  <li><a href="#step-3-build-your-dataloaders." id="toc-step-3-build-your-dataloaders." class="nav-link" data-scroll-target="#step-3-build-your-dataloaders.">Step 3: Build your <code>DataLoaders</code>.</a></li>
  </ul></li>
  <li><a href="#tests" id="toc-tests" class="nav-link" data-scroll-target="#tests">Tests</a></li>
  </ul>
<div class="toc-actions"><div><i class="bi bi-github"></i></div><div class="action-links"><p><a href="https://github.com/ohmeow/blurr/issues/new" class="toc-action">Report an issue</a></p></div></div></nav>
    </div>
<!-- main -->
<main class="content column-body" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title d-none d-lg-block">Data</h1>
</div>

<div>
  <div class="description">
    The <code>text.data.core</code> module contains the core bits required to use the fastai DataBlock API and/or mid-level data processing pipelines to turn your raw datasets into modelable <code>DataLoaders</code> for text/NLP tasks
  </div>
</div>


<div class="quarto-title-meta">

    
    
  </div>
  

</header>

<!-- WARNING: THIS FILE WAS AUTOGENERATED! DO NOT EDIT! -->
<div class="cell">
<div class="cell-output cell-output-stdout">
<pre><code>What we're running with at the time this documentation was generated:
torch: 1.9.0+cu102
fastai: 2.7.9
transformers: 4.21.2</code></pre>
</div>
</div>
<section id="setup" class="level2">
<h2 class="anchored" data-anchor-id="setup">Setup</h2>
<p>We’ll use a subset of <code>imdb</code> to demonstrate how to configure your BLURR for sequence classification tasks</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a>raw_datasets <span class="op">=</span> load_dataset(<span class="st">"imdb"</span>, split<span class="op">=</span>[<span class="st">"train"</span>, <span class="st">"test"</span>])</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>raw_datasets[<span class="dv">0</span>] <span class="op">=</span> raw_datasets[<span class="dv">0</span>].add_column(<span class="st">"is_valid"</span>, [<span class="va">False</span>] <span class="op">*</span> <span class="bu">len</span>(raw_datasets[<span class="dv">0</span>]))</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>raw_datasets[<span class="dv">1</span>] <span class="op">=</span> raw_datasets[<span class="dv">1</span>].add_column(<span class="st">"is_valid"</span>, [<span class="va">True</span>] <span class="op">*</span> <span class="bu">len</span>(raw_datasets[<span class="dv">1</span>]))</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>final_ds <span class="op">=</span> concatenate_datasets([raw_datasets[<span class="dv">0</span>].shuffle().select(<span class="bu">range</span>(<span class="dv">1000</span>)), raw_datasets[<span class="dv">1</span>].shuffle().select(<span class="bu">range</span>(<span class="dv">200</span>))])</span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a>imdb_df <span class="op">=</span> pd.DataFrame(final_ds)</span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a>imdb_df.head()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>Reusing dataset imdb (/home/wgilliam/.cache/huggingface/datasets/imdb/plain_text/1.0.0/2fdd8b9bcadd6e7055e742a706876ba43f19faee861df134affd7a3f60fc38a1)</code></pre>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"b627baa9438e4578a663956c366d07b3","version_major":2,"version_minor":0}
</script>
</div>
<div class="cell-output cell-output-display">

<div>

<table class="dataframe table table-sm table-striped">
  <thead>
    <tr>
      <th></th>
      <th>text</th>
      <th>label</th>
      <th>is_valid</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>What an overlooked 80's soundtrack. I imagine John Travolta sang some of the songs but in watching the movie it did seem to personify everything that was 80s cheese. Clearly movies that rely on mechanical bulls, bartenders and immature relationships were in style. The best was his lousy Texas accent. Compare that to Friday Night Lights.I suggest watching Cocktail and Stir Crazy to start really getting into the dumbing down of film. Also, as a side note Made in America with Ted Danson and Whoopie Goldberg is an awesomely bad movie. I was so shocked to realize I had never watched it. One mor...</td>
      <td>1</td>
      <td>False</td>
    </tr>
    <tr>
      <th>1</th>
      <td>An archaeologist (Casper Van Dien) stumbles accidentally upon an ancient, 40 foot mummy, well preserved underground in the Nevada desert. They are determined to keep this a secret and call in a Jewish translator to assist in figuring out the history of it. The mummy, as explained at the beginning, is the son of a fallen angel and is one of several giants that apparently existed in "those days". In order to save his son from a devastating flood which was predicted to kill everything, he mummifies his son, burying him with several servants for centuries - planning to awaken him years from th...</td>
      <td>0</td>
      <td>False</td>
    </tr>
    <tr>
      <th>2</th>
      <td>Sudden Impact is a two pronged story. Harry is targeted by the mob who want to kill him and Harry is very glad to return the favour and show them how it's done. This little war puts Harry on suspension which he doesn't care about but he goes away on a little vacation. Now the second part of the story. Someone is killing some punks and Harry gets dragged into this situation where he meets Jennifer spencer a woman with a secret that the little tourist town wants to keep quiet. The police Chief is not a subtle man and he warns Harry to not get involved or cause any trouble. This is Harry Call...</td>
      <td>1</td>
      <td>False</td>
    </tr>
    <tr>
      <th>3</th>
      <td>It is a superb Swedish film .. it was the first Swedish film I've seen .. it is simple &amp; deep .. what a great combination!.&lt;br /&gt;&lt;br /&gt;Michael Nyqvist did a great performance as a famous conductor who seeks peace in his hometown.&lt;br /&gt;&lt;br /&gt;Frida Hallgren was great as his inspirational girlfriend to help him to carry on &amp; never give up.&lt;br /&gt;&lt;br /&gt;The fight between the conductor and the hypocrite priest who loses his battle with Michael when his wife confronts him And defends Michael's noble cause to help his hometown people finding their own peace in music.&lt;br /&gt;&lt;br /&gt;The only thing that ...</td>
      <td>1</td>
      <td>False</td>
    </tr>
    <tr>
      <th>4</th>
      <td>The plot is about a female nurse, named Anna, is caught in the middle of a world-wide chaos as flesh-eating zombies begin rising up and taking over the world and attacking the living. She escapes into the streets and is rescued by a black police officer. So far, so good! I usually enjoy horror movies, but this piece of film doesn't deserve to be called horror. It's not even thrilling, just ridiculous.Even "the Flintstones" or "Kukla, Fran and Ollie" will give you more excitement. It's like watching a bunch of bloodthirsty drunkards not being able to get into a shopping mall to by more liqu...</td>
      <td>0</td>
      <td>False</td>
    </tr>
  </tbody>
</table>
</div>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a>labels <span class="op">=</span> raw_datasets[<span class="dv">0</span>].features[<span class="st">"label"</span>].names</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>labels</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>['neg', 'pos']</code></pre>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a>model_cls <span class="op">=</span> AutoModelForSequenceClassification</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>hf_logging.set_verbosity_error()</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a>pretrained_model_name <span class="op">=</span> <span class="st">"roberta-base"</span>  <span class="co"># "bert-base-multilingual-cased"</span></span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a>n_labels <span class="op">=</span> <span class="bu">len</span>(labels)</span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a>hf_arch, hf_config, hf_tokenizer, hf_model <span class="op">=</span> get_hf_objects(</span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a>    pretrained_model_name, model_cls<span class="op">=</span>model_cls, config_kwargs<span class="op">=</span>{<span class="st">"num_labels"</span>: n_labels}</span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-11"><a href="#cb6-11" aria-hidden="true" tabindex="-1"></a>hf_arch, <span class="bu">type</span>(hf_config), <span class="bu">type</span>(hf_tokenizer), <span class="bu">type</span>(hf_model)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>('roberta',
 transformers.models.roberta.configuration_roberta.RobertaConfig,
 transformers.models.roberta.tokenization_roberta_fast.RobertaTokenizerFast,
 transformers.models.roberta.modeling_roberta.RobertaForSequenceClassification)</code></pre>
</div>
</div>
</section>
<section id="preprocessing" class="level2">
<h2 class="anchored" data-anchor-id="preprocessing">Preprocessing</h2>
<p>Starting with version 2.0, <code>BLURR</code> provides a preprocessing base class that can be used to build task specific pre-processed datasets from pandas DataFrames or Hugging Face Datasets</p>
<hr>
<p><a href="https://github.com/ohmeow/blurr/blob/master/blurr/text/data/core.py#L44" target="_blank" style="float:right; font-size:smaller">source</a></p>
<section id="preprocessor" class="level3">
<h3 class="anchored" data-anchor-id="preprocessor">Preprocessor</h3>
<blockquote class="blockquote">
<pre><code> Preprocessor (hf_tokenizer:transformers.tokenization_utils_base.PreTraine
               dTokenizerBase, batch_size:int=1000, text_attr:str='text',
               text_pair_attr:str=None, is_valid_attr:str='is_valid',
               tok_kwargs:dict={})</code></pre>
</blockquote>
<p>Initialize self. See help(type(self)) for accurate signature.</p>
<table class="table">
<colgroup>
<col style="width: 6%">
<col style="width: 25%">
<col style="width: 34%">
<col style="width: 34%">
</colgroup>
<thead>
<tr class="header">
<th></th>
<th><strong>Type</strong></th>
<th><strong>Default</strong></th>
<th><strong>Details</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>hf_tokenizer</td>
<td>PreTrainedTokenizerBase</td>
<td></td>
<td>A Hugging Face tokenizer</td>
</tr>
<tr class="even">
<td>batch_size</td>
<td>int</td>
<td>1000</td>
<td>The number of examples to process at a time</td>
</tr>
<tr class="odd">
<td>text_attr</td>
<td>str</td>
<td>text</td>
<td>The attribute holding the text</td>
</tr>
<tr class="even">
<td>text_pair_attr</td>
<td>str</td>
<td>None</td>
<td>The attribute holding the text_pair</td>
</tr>
<tr class="odd">
<td>is_valid_attr</td>
<td>str</td>
<td>is_valid</td>
<td>The attribute that should be created if your are processing individual training and validation<br>
datasets into a single dataset, and will indicate to which each example is associated</td>
</tr>
<tr class="even">
<td>tok_kwargs</td>
<td>dict</td>
<td>{}</td>
<td>Tokenization kwargs that will be applied with calling the tokenizer</td>
</tr>
</tbody>
</table>
<hr>
<p><a href="https://github.com/ohmeow/blurr/blob/master/blurr/text/data/core.py#L108" target="_blank" style="float:right; font-size:smaller">source</a></p>
</section>
<section id="classificationpreprocessor" class="level3">
<h3 class="anchored" data-anchor-id="classificationpreprocessor">ClassificationPreprocessor</h3>
<blockquote class="blockquote">
<pre><code> ClassificationPreprocessor (hf_tokenizer:transformers.tokenization_utils_
                             base.PreTrainedTokenizerBase,
                             batch_size:int=1000,
                             is_multilabel:bool=False, id_attr:str=None,
                             text_attr:str='text',
                             text_pair_attr:str=None,
                             label_attrs:str|list[str]='label',
                             is_valid_attr:str='is_valid',
                             label_mapping:list[str]=None,
                             tok_kwargs:dict={})</code></pre>
</blockquote>
<p>Initialize self. See help(type(self)) for accurate signature.</p>
<table class="table">
<colgroup>
<col style="width: 6%">
<col style="width: 25%">
<col style="width: 34%">
<col style="width: 34%">
</colgroup>
<thead>
<tr class="header">
<th></th>
<th><strong>Type</strong></th>
<th><strong>Default</strong></th>
<th><strong>Details</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>hf_tokenizer</td>
<td>PreTrainedTokenizerBase</td>
<td></td>
<td>A Hugging Face tokenizer</td>
</tr>
<tr class="even">
<td>batch_size</td>
<td>int</td>
<td>1000</td>
<td>The number of examples to process at a time</td>
</tr>
<tr class="odd">
<td>is_multilabel</td>
<td>bool</td>
<td>False</td>
<td>Whether the dataset should be processed for multi-label; if True, will ensure <code>label_attrs</code> are<br>
converted to a value of either 0 or 1 indiciating the existence of the class in the example</td>
</tr>
<tr class="even">
<td>id_attr</td>
<td>str</td>
<td>None</td>
<td>The unique identifier in the dataset</td>
</tr>
<tr class="odd">
<td>text_attr</td>
<td>str</td>
<td>text</td>
<td>The attribute holding the text</td>
</tr>
<tr class="even">
<td>text_pair_attr</td>
<td>str</td>
<td>None</td>
<td>The attribute holding the text_pair</td>
</tr>
<tr class="odd">
<td>label_attrs</td>
<td>str | list[str]</td>
<td>label</td>
<td>The attribute holding the label(s) of the example</td>
</tr>
<tr class="even">
<td>is_valid_attr</td>
<td>str</td>
<td>is_valid</td>
<td>The attribute that should be created if your are processing individual training and validation<br>
datasets into a single dataset, and will indicate to which each example is associated</td>
</tr>
<tr class="odd">
<td>label_mapping</td>
<td>list[str]</td>
<td>None</td>
<td>A list indicating the valid labels for the dataset (optional, defaults to the unique set of labels<br>
found in the full dataset)</td>
</tr>
<tr class="even">
<td>tok_kwargs</td>
<td>dict</td>
<td>{}</td>
<td>Tokenization kwargs that will be applied with calling the tokenizer</td>
</tr>
</tbody>
</table>
<p>Starting with version 2.0, <code>BLURR</code> provides a sequence classification preprocessing class that can be used to preprocess DataFrames or Hugging Face Datasets.</p>
<p>This class can be used for preprocessing both multiclass and multilabel classification datasets, and includes a <code>proc_{your_text_attr}</code> and <code>proc_{your_text_pair_attr}</code> (optional) attributes containing your modified text as a result of tokenization (e.g., if you specify a <code>max_length</code> the <code>proc_{your_text_attr}</code> may contain truncated text).</p>
<p><strong>Note</strong>: This class works for both slow and fast tokenizers</p>
<section id="using-a-dataframe" class="level4">
<h4 class="anchored" data-anchor-id="using-a-dataframe">Using a <code>DataFrame</code></h4>
<div class="cell">
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a>preprocessor <span class="op">=</span> ClassificationPreprocessor(hf_tokenizer, label_mapping<span class="op">=</span>labels, tok_kwargs<span class="op">=</span>{<span class="st">"max_length"</span>: <span class="dv">24</span>})</span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a>proc_df <span class="op">=</span> preprocessor.process_df(imdb_df)</span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a>proc_df.columns, <span class="bu">len</span>(proc_df)</span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a>proc_df.head(<span class="dv">2</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">

<div>

<table class="dataframe table table-sm table-striped">
  <thead>
    <tr>
      <th></th>
      <th>proc_text</th>
      <th>text</th>
      <th>label</th>
      <th>is_valid</th>
      <th>label_name</th>
      <th>text_start_char_idx</th>
      <th>text_end_char_idx</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>What an overlooked 80's soundtrack. I imagine John Travolta sang some of the songs but in watching</td>
      <td>What an overlooked 80's soundtrack. I imagine John Travolta sang some of the songs but in watching the movie it did seem to personify everything that was 80s cheese. Clearly movies that rely on mechanical bulls, bartenders and immature relationships were in style. The best was his lousy Texas accent. Compare that to Friday Night Lights.I suggest watching Cocktail and Stir Crazy to start really getting into the dumbing down of film. Also, as a side note Made in America with Ted Danson and Whoopie Goldberg is an awesomely bad movie. I was so shocked to realize I had never watched it. One mor...</td>
      <td>1</td>
      <td>False</td>
      <td>pos</td>
      <td>0</td>
      <td>98</td>
    </tr>
    <tr>
      <th>1</th>
      <td>An archaeologist (Casper Van Dien) stumbles accidentally upon an ancient, 40 foot mummy, well</td>
      <td>An archaeologist (Casper Van Dien) stumbles accidentally upon an ancient, 40 foot mummy, well preserved underground in the Nevada desert. They are determined to keep this a secret and call in a Jewish translator to assist in figuring out the history of it. The mummy, as explained at the beginning, is the son of a fallen angel and is one of several giants that apparently existed in "those days". In order to save his son from a devastating flood which was predicted to kill everything, he mummifies his son, burying him with several servants for centuries - planning to awaken him years from th...</td>
      <td>0</td>
      <td>False</td>
      <td>neg</td>
      <td>0</td>
      <td>93</td>
    </tr>
  </tbody>
</table>
</div>
</div>
</div>
</section>
<section id="using-a-hugging-face-dataset" class="level4">
<h4 class="anchored" data-anchor-id="using-a-hugging-face-dataset">Using a Hugging Face <code>Dataset</code></h4>
<div class="cell">
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a>preprocessor <span class="op">=</span> ClassificationPreprocessor(hf_tokenizer, label_mapping<span class="op">=</span>labels)</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a>proc_ds <span class="op">=</span> preprocessor.process_hf_dataset(final_ds)</span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a>proc_ds</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>Dataset({
    features: ['proc_text', 'text', 'label', 'is_valid', 'label_name', 'text_start_char_idx', 'text_end_char_idx'],
    num_rows: 1200
})</code></pre>
</div>
</div>
</section>
</section>
</section>
<section id="mid-level-api" class="level2">
<h2 class="anchored" data-anchor-id="mid-level-api">Mid-level API</h2>
<p>Base tokenization, batch transform, and DataBlock methods</p>
<hr>
<p><a href="https://github.com/ohmeow/blurr/blob/master/blurr/text/data/core.py#L200" target="_blank" style="float:right; font-size:smaller">source</a></p>
<section id="textinput" class="level3">
<h3 class="anchored" data-anchor-id="textinput">TextInput</h3>
<blockquote class="blockquote">
<pre><code> TextInput (x, **kwargs)</code></pre>
</blockquote>
<p>The base represenation of your inputs; used by the various fastai <code>show</code> methods</p>
<p>A <a href="https://ohmeow.github.io/blurr/text-data-core.html#textinput"><code>TextInput</code></a> object is returned from the decodes method of <a href="https://ohmeow.github.io/blurr/text-data-core.html#batchdecodetransform"><code>BatchDecodeTransform</code></a> as a means to customize <code>@typedispatch</code>ed functions like <code>DataLoaders.show_batch</code> and <code>Learner.show_results</code>. The value will the your “input_ids”.</p>
<hr>
<p><a href="https://github.com/ohmeow/blurr/blob/master/blurr/text/data/core.py#L207" target="_blank" style="float:right; font-size:smaller">source</a></p>
</section>
<section id="batchtokenizetransform" class="level3">
<h3 class="anchored" data-anchor-id="batchtokenizetransform">BatchTokenizeTransform</h3>
<blockquote class="blockquote">
<pre><code> BatchTokenizeTransform (hf_arch:str, hf_config:PretrainedConfig,
                         hf_tokenizer:PreTrainedTokenizerBase,
                         hf_model:PreTrainedModel,
                         include_labels:bool=True,
                         ignore_token_id:int=-100, max_length:int=None,
                         padding:bool|str=True, truncation:bool|str=True,
                         is_split_into_words:bool=False,
                         tok_kwargs:dict={}, **kwargs)</code></pre>
</blockquote>
<p>Handles everything you need to assemble a mini-batch of inputs and targets, as well as decode the dictionary produced as a byproduct of the tokenization process in the <code>encodes</code> method.</p>
<table class="table">
<thead>
<tr class="header">
<th></th>
<th><strong>Type</strong></th>
<th><strong>Default</strong></th>
<th><strong>Details</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>hf_arch</td>
<td>str</td>
<td></td>
<td>The abbreviation/name of your Hugging Face transformer architecture (e.b., bert, bart, etc..)</td>
</tr>
<tr class="even">
<td>hf_config</td>
<td>PretrainedConfig</td>
<td></td>
<td>A specific configuration instance you want to use</td>
</tr>
<tr class="odd">
<td>hf_tokenizer</td>
<td>PreTrainedTokenizerBase</td>
<td></td>
<td>A Hugging Face tokenizer</td>
</tr>
<tr class="even">
<td>hf_model</td>
<td>PreTrainedModel</td>
<td></td>
<td>A Hugging Face model</td>
</tr>
<tr class="odd">
<td>include_labels</td>
<td>bool</td>
<td>True</td>
<td>To control whether the “labels” are included in your inputs. If they are, the loss will be calculated in<br>
the model’s forward function and you can simply use <code>PreCalculatedLoss</code> as your <code>Learner</code>’s loss function to use it</td>
</tr>
<tr class="even">
<td>ignore_token_id</td>
<td>int</td>
<td>-100</td>
<td>The token ID that should be ignored when calculating the loss</td>
</tr>
<tr class="odd">
<td>max_length</td>
<td>int</td>
<td>None</td>
<td>To control the length of the padding/truncation. It can be an integer or None,<br>
in which case it will default to the maximum length the model can accept.<br>
If the model has no specific maximum input length, truncation/padding to max_length is deactivated.<br>
See <a href="https://huggingface.co/transformers/preprocessing.html#everything-you-always-wanted-to-know-about-padding-and-truncation">Everything you always wanted to know about padding and truncation</a></td>
</tr>
<tr class="even">
<td>padding</td>
<td>bool | str</td>
<td>True</td>
<td>To control the <code>padding</code> applied to your <code>hf_tokenizer</code> during tokenization.<br>
If None, will default to ‘False’ or ‘do_not_pad’.<br>
See <a href="https://huggingface.co/transformers/preprocessing.html#everything-you-always-wanted-to-know-about-padding-and-truncation">Everything you always wanted to know about padding and truncation</a></td>
</tr>
<tr class="odd">
<td>truncation</td>
<td>bool | str</td>
<td>True</td>
<td>To control <code>truncation</code> applied to your <code>hf_tokenizer</code> during tokenization.<br>
If None, will default to ‘False’ or ‘do_not_truncate’.<br>
See <a href="https://huggingface.co/transformers/preprocessing.html#everything-you-always-wanted-to-know-about-padding-and-truncation">Everything you always wanted to know about padding and truncation</a></td>
</tr>
<tr class="even">
<td>is_split_into_words</td>
<td>bool</td>
<td>False</td>
<td>The <code>is_split_into_words</code> argument applied to your <code>hf_tokenizer</code> during tokenization.<br>
Set this to ‘True’ if your inputs are pre-tokenized (not numericalized) \</td>
</tr>
<tr class="odd">
<td>tok_kwargs</td>
<td>dict</td>
<td>{}</td>
<td>Any other keyword arguments you want included when using your <code>hf_tokenizer</code> to tokenize your inputs</td>
</tr>
<tr class="even">
<td>kwargs</td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<p>Inspired by this <a href="https://docs.fast.ai/tutorial.transformers.html">article</a>, <a href="https://ohmeow.github.io/blurr/text-data-core.html#batchtokenizetransform"><code>BatchTokenizeTransform</code></a> inputs can come in as raw <strong>text</strong>, <strong>a list of words</strong> (e.g., tasks like Named Entity Recognition (NER), where you want to predict the label of each token), or as a <strong>dictionary</strong> that includes extra information you want to use during post-processing.</p>
<p><strong>On-the-fly Batch-Time Tokenization</strong>:</p>
<p>Part of the inspiration for this derives from the mechanics of Hugging Face tokenizers, in particular it can return a collated mini-batch of data given a list of sequences. As such, the collating required for our inputs can be done during tokenization <strong><em>before</em></strong> our batch transforms run in a <code>before_batch_tfms</code> transform (where we get a list of examples)! This allows users of BLURR to have everything done dynamically at batch-time without prior preprocessing with at least four potential benefits: 1. Less code 2. Faster mini-batch creation 3. Less RAM utilization and time spent tokenizing beforehand (this really helps with very large datasets) 4. Flexibility</p>
<hr>
<p><a href="https://github.com/ohmeow/blurr/blob/master/blurr/text/data/core.py#L306" target="_blank" style="float:right; font-size:smaller">source</a></p>
</section>
<section id="batchdecodetransform" class="level3">
<h3 class="anchored" data-anchor-id="batchdecodetransform">BatchDecodeTransform</h3>
<blockquote class="blockquote">
<pre><code> BatchDecodeTransform (input_return_type:type=&lt;class
                       '__main__.TextInput'&gt;, hf_arch:str=None,
                       hf_config:PretrainedConfig=None,
                       hf_tokenizer:PreTrainedTokenizerBase=None,
                       hf_model:PreTrainedModel=None, **kwargs)</code></pre>
</blockquote>
<p>A class used to cast your inputs as <code>input_return_type</code> for fastai <code>show</code> methods</p>
<table class="table">
<colgroup>
<col style="width: 6%">
<col style="width: 25%">
<col style="width: 34%">
<col style="width: 34%">
</colgroup>
<thead>
<tr class="header">
<th></th>
<th><strong>Type</strong></th>
<th><strong>Default</strong></th>
<th><strong>Details</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>input_return_type</td>
<td>type</td>
<td>TextInput</td>
<td>Used by typedispatched show methods</td>
</tr>
<tr class="even">
<td>hf_arch</td>
<td>str</td>
<td>None</td>
<td>The abbreviation/name of your Hugging Face transformer architecture (not required if passing in an instance of <code>BatchTokenizeTransform</code> to <code>before_batch_tfm</code>)</td>
</tr>
<tr class="odd">
<td>hf_config</td>
<td>PretrainedConfig</td>
<td>None</td>
<td>A Hugging Face configuration object (not required if passing in an instance of <code>BatchTokenizeTransform</code> to <code>before_batch_tfm</code>)</td>
</tr>
<tr class="even">
<td>hf_tokenizer</td>
<td>PreTrainedTokenizerBase</td>
<td>None</td>
<td>A Hugging Face tokenizer (not required if passing in an instance of <code>BatchTokenizeTransform</code> to <code>before_batch_tfm</code>)</td>
</tr>
<tr class="odd">
<td>hf_model</td>
<td>PreTrainedModel</td>
<td>None</td>
<td>A Hugging Face model (not required if passing in an instance of <code>BatchTokenizeTransform</code> to <code>before_batch_tfm</code>)</td>
</tr>
<tr class="even">
<td>kwargs</td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<p>As of fastai 2.1.5, before batch transforms no longer have a <code>decodes</code> method … and so, I’ve introduced a standard batch transform here, <a href="https://ohmeow.github.io/blurr/text-data-core.html#batchdecodetransform"><code>BatchDecodeTransform</code></a>, (one that occurs “after” the batch has been created) that will do the decoding for us.</p>
<hr>
<p><a href="https://github.com/ohmeow/blurr/blob/master/blurr/text/data/core.py#L333" target="_blank" style="float:right; font-size:smaller">source</a></p>
</section>
<section id="blurr_sort_func" class="level3">
<h3 class="anchored" data-anchor-id="blurr_sort_func">blurr_sort_func</h3>
<blockquote class="blockquote">
<pre><code> blurr_sort_func (example, hf_tokenizer:transformers.tokenization_utils_ba
                  se.PreTrainedTokenizerBase,
                  is_split_into_words:bool=False, tok_kwargs:dict={})</code></pre>
</blockquote>
<p>This method is used by the <code>SortedDL</code> to ensure your dataset is sorted <em>after</em> tokenization</p>
<table class="table">
<colgroup>
<col style="width: 6%">
<col style="width: 25%">
<col style="width: 34%">
<col style="width: 34%">
</colgroup>
<thead>
<tr class="header">
<th></th>
<th><strong>Type</strong></th>
<th><strong>Default</strong></th>
<th><strong>Details</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>example</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr class="even">
<td>hf_tokenizer</td>
<td>PreTrainedTokenizerBase</td>
<td></td>
<td>A Hugging Face tokenizer</td>
</tr>
<tr class="odd">
<td>is_split_into_words</td>
<td>bool</td>
<td>False</td>
<td>The <code>is_split_into_words</code> argument applied to your <code>hf_tokenizer</code> during tokenization.<br>
Set this to ‘True’ if your inputs are pre-tokenized (not numericalized)</td>
</tr>
<tr class="even">
<td>tok_kwargs</td>
<td>dict</td>
<td>{}</td>
<td>Any other keyword arguments you want to include during tokenization</td>
</tr>
</tbody>
</table>
<hr>
<p><a href="https://github.com/ohmeow/blurr/blob/master/blurr/text/data/core.py#L349" target="_blank" style="float:right; font-size:smaller">source</a></p>
</section>
<section id="textblock" class="level3">
<h3 class="anchored" data-anchor-id="textblock">TextBlock</h3>
<blockquote class="blockquote">
<pre><code> TextBlock (hf_arch:str=None,
            hf_config:transformers.configuration_utils.PretrainedConfig=No
            ne, hf_tokenizer:transformers.tokenization_utils_base.PreTrain
            edTokenizerBase=None,
            hf_model:transformers.modeling_utils.PreTrainedModel=None,
            include_labels:bool=True, ignore_token_id=-100,
            batch_tokenize_tfm:__main__.BatchTokenizeTransform=None,
            batch_decode_tfm:__main__.BatchDecodeTransform=None,
            max_length:int=None, padding:bool|str=True,
            truncation:bool|str=True, is_split_into_words:bool=False,
            input_return_type:type=&lt;class '__main__.TextInput'&gt;,
            dl_type:fastai.data.load.DataLoader=None,
            batch_tokenize_kwargs:dict={}, batch_decode_kwargs:dict={},
            tok_kwargs:dict={}, text_gen_kwargs:dict={}, **kwargs)</code></pre>
</blockquote>
<p>The core <code>TransformBlock</code> to prepare your inputs for training in Blurr with fastai’s <code>DataBlock</code> API</p>
<table class="table">
<thead>
<tr class="header">
<th></th>
<th><strong>Type</strong></th>
<th><strong>Default</strong></th>
<th><strong>Details</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>hf_arch</td>
<td>str</td>
<td>None</td>
<td>The abbreviation/name of your Hugging Face transformer architecture (not required if passing in an<br>
instance of <code>BatchTokenizeTransform</code> to <code>before_batch_tfm</code>)</td>
</tr>
<tr class="even">
<td>hf_config</td>
<td>PretrainedConfig</td>
<td>None</td>
<td>A Hugging Face configuration object (not required if passing in an<br>
instance of <code>BatchTokenizeTransform</code> to <code>before_batch_tfm</code>)</td>
</tr>
<tr class="odd">
<td>hf_tokenizer</td>
<td>PreTrainedTokenizerBase</td>
<td>None</td>
<td>A Hugging Face tokenizer (not required if passing in an<br>
instance of <code>BatchTokenizeTransform</code> to <code>before_batch_tfm</code>)</td>
</tr>
<tr class="even">
<td>hf_model</td>
<td>PreTrainedModel</td>
<td>None</td>
<td>A Hugging Face model (not required if passing in an<br>
instance of <code>BatchTokenizeTransform</code> to <code>before_batch_tfm</code>)</td>
</tr>
<tr class="odd">
<td>include_labels</td>
<td>bool</td>
<td>True</td>
<td>To control whether the “labels” are included in your inputs. If they are, the loss will be calculated in<br>
the model’s forward function and you can simply use <code>PreCalculatedLoss</code> as your <code>Learner</code>’s loss function to use it</td>
</tr>
<tr class="even">
<td>ignore_token_id</td>
<td>int</td>
<td>-100</td>
<td>The token ID that should be ignored when calculating the loss</td>
</tr>
<tr class="odd">
<td>batch_tokenize_tfm</td>
<td>BatchTokenizeTransform</td>
<td>None</td>
<td>The before_batch_tfm you want to use to tokenize your raw data on the fly<br>
(defaults to an instance of <code>BatchTokenizeTransform</code>)</td>
</tr>
<tr class="even">
<td>batch_decode_tfm</td>
<td>BatchDecodeTransform</td>
<td>None</td>
<td>The batch_tfm you want to decode your inputs into a type that can be used in the fastai show methods,<br>
(defaults to BatchDecodeTransform)</td>
</tr>
<tr class="odd">
<td>max_length</td>
<td>int</td>
<td>None</td>
<td>To control the length of the padding/truncation. It can be an integer or None,<br>
in which case it will default to the maximum length the model can accept. If the model has no<br>
specific maximum input length, truncation/padding to max_length is deactivated.<br>
See <a href="https://huggingface.co/transformers/preprocessing.html#everything-you-always-wanted-to-know-about-padding-and-truncation">Everything you always wanted to know about padding and truncation</a></td>
</tr>
<tr class="even">
<td>padding</td>
<td>bool | str</td>
<td>True</td>
<td>To control the ‘padding’ applied to your <code>hf_tokenizer</code> during tokenization.<br>
If None, will default to ‘False’ or ‘do_not_pad’.<br>
See <a href="https://huggingface.co/transformers/preprocessing.html#everything-you-always-wanted-to-know-about-padding-and-truncation">Everything you always wanted to know about padding and truncation</a></td>
</tr>
<tr class="odd">
<td>truncation</td>
<td>bool | str</td>
<td>True</td>
<td>To control ‘truncation’ applied to your <code>hf_tokenizer</code> during tokenization.<br>
If None, will default to ‘False’ or ‘do_not_truncate’.<br>
See <a href="https://huggingface.co/transformers/preprocessing.html#everything-you-always-wanted-to-know-about-padding-and-truncation">Everything you always wanted to know about padding and truncation</a></td>
</tr>
<tr class="even">
<td>is_split_into_words</td>
<td>bool</td>
<td>False</td>
<td>The <code>is_split_into_words</code> argument applied to your <code>hf_tokenizer</code> during tokenization.<br>
Set this to <code>True</code> if your inputs are pre-tokenized (not numericalized)</td>
</tr>
<tr class="odd">
<td>input_return_type</td>
<td>type</td>
<td>TextInput</td>
<td>The return type your decoded inputs should be cast too (used by methods such as <code>show_batch</code>)</td>
</tr>
<tr class="even">
<td>dl_type</td>
<td>DataLoader</td>
<td>None</td>
<td>The type of <code>DataLoader</code> you want created (defaults to <code>SortedDL</code>)</td>
</tr>
<tr class="odd">
<td>batch_tokenize_kwargs</td>
<td>dict</td>
<td>{}</td>
<td>Any keyword arguments you want applied to your <code>batch_tokenize_tfm</code></td>
</tr>
<tr class="even">
<td>batch_decode_kwargs</td>
<td>dict</td>
<td>{}</td>
<td>Any keyword arguments you want applied to your <code>batch_decode_tfm</code> (will be set as a fastai <code>batch_tfms</code>)</td>
</tr>
<tr class="odd">
<td>tok_kwargs</td>
<td>dict</td>
<td>{}</td>
<td>Any keyword arguments you want your Hugging Face tokenizer to use during tokenization</td>
</tr>
<tr class="even">
<td>text_gen_kwargs</td>
<td>dict</td>
<td>{}</td>
<td>Any keyword arguments you want to have applied with generating text</td>
</tr>
<tr class="odd">
<td>kwargs</td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<p>A basic <code>DataBlock</code> for our inputs, <a href="https://ohmeow.github.io/blurr/text-data-core.html#textblock"><code>TextBlock</code></a> is designed with sensible defaults to minimize user effort in defining their transforms pipeline. It handles setting up your <a href="https://ohmeow.github.io/blurr/text-data-core.html#batchtokenizetransform"><code>BatchTokenizeTransform</code></a> and <a href="https://ohmeow.github.io/blurr/text-data-core.html#batchdecodetransform"><code>BatchDecodeTransform</code></a> transforms regardless of data source (e.g., this will work with files, DataFrames, whatever).</p>
<p><strong>Note</strong>: You must either pass in your own instance of a <a href="https://ohmeow.github.io/blurr/text-data-core.html#batchtokenizetransform"><code>BatchTokenizeTransform</code></a> class or the Hugging Face objects returned from <code>BLURR.get_hf_objects</code> (e.g.,architecture, config, tokenizer, and model). The other args are optional.</p>
<p>We also include a <a href="https://ohmeow.github.io/blurr/text-data-core.html#blurr_sort_func"><code>blurr_sort_func</code></a> that works with <code>SortedDL</code> to properly sort based on the number of tokens in each example.</p>
</section>
</section>
<section id="utility-classes-and-methods" class="level2">
<h2 class="anchored" data-anchor-id="utility-classes-and-methods">Utility classes and methods</h2>
<p>These methods are use internally for getting blurr transforms associated to your <code>DataLoaders</code></p>
<hr>
<p><a href="https://github.com/ohmeow/blurr/blob/master/blurr/text/data/core.py#L444" target="_blank" style="float:right; font-size:smaller">source</a></p>
<section id="get_blurr_tfm" class="level3">
<h3 class="anchored" data-anchor-id="get_blurr_tfm">get_blurr_tfm</h3>
<blockquote class="blockquote">
<pre><code> get_blurr_tfm (tfms_list:fastcore.transform.Pipeline,
                tfm_class:fastcore.transform.Transform=&lt;class
                '__main__.BatchTokenizeTransform'&gt;)</code></pre>
</blockquote>
<p>Given a fastai DataLoaders batch transforms, this method can be used to get at a transform instance used in your Blurr DataBlock</p>
<table class="table">
<colgroup>
<col style="width: 6%">
<col style="width: 25%">
<col style="width: 34%">
<col style="width: 34%">
</colgroup>
<thead>
<tr class="header">
<th></th>
<th><strong>Type</strong></th>
<th><strong>Default</strong></th>
<th><strong>Details</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>tfms_list</td>
<td>Pipeline</td>
<td></td>
<td>A list of transforms (e.g., dls.after_batch, dls.before_batch, etc…)</td>
</tr>
<tr class="even">
<td>tfm_class</td>
<td>Transform</td>
<td>BatchTokenizeTransform</td>
<td>The transform to find</td>
</tr>
</tbody>
</table>
<hr>
<p><a href="https://github.com/ohmeow/blurr/blob/master/blurr/text/data/core.py#L458" target="_blank" style="float:right; font-size:smaller">source</a></p>
</section>
<section id="first_blurr_tfm" class="level3">
<h3 class="anchored" data-anchor-id="first_blurr_tfm">first_blurr_tfm</h3>
<blockquote class="blockquote">
<pre><code> first_blurr_tfm (dls:fastai.data.core.DataLoaders,
                  tfms:list[fastcore.transform.Transform]=[&lt;class
                  '__main__.BatchTokenizeTransform'&gt;, &lt;class
                  '__main__.BatchDecodeTransform'&gt;])</code></pre>
</blockquote>
<p>This convenience method will find the first Blurr transform required for methods such as <code>show_batch</code> and <code>show_results</code>. The returned transform should have everything you need to properly decode and ‘show’ your Hugging Face inputs/targets</p>
<table class="table">
<colgroup>
<col style="width: 6%">
<col style="width: 25%">
<col style="width: 34%">
<col style="width: 34%">
</colgroup>
<thead>
<tr class="header">
<th></th>
<th><strong>Type</strong></th>
<th><strong>Default</strong></th>
<th><strong>Details</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>dls</td>
<td>DataLoaders</td>
<td></td>
<td>Your fast.ai `DataLoaders</td>
</tr>
<tr class="even">
<td>tfms</td>
<td>list[Transform]</td>
<td>[&lt;class ‘<strong>main</strong>.BatchTokenizeTransform’&gt;, &lt;class ‘<strong>main</strong>.BatchDecodeTransform’&gt;]</td>
<td>The Blurr transforms to look for in order</td>
</tr>
</tbody>
</table>
</section>
</section>
<section id="mid-level-examples" class="level2">
<h2 class="anchored" data-anchor-id="mid-level-examples">Mid-level Examples</h2>
<p>The following eamples demonstrate several approaches to construct your <code>DataBlock</code> for sequence classication tasks using the mid-level API.</p>
<section id="batch-time-tokenization" class="level3">
<h3 class="anchored" data-anchor-id="batch-time-tokenization">Batch-Time Tokenization</h3>
<section id="step-1-get-your-hugging-face-objects." class="level4">
<h4 class="anchored" data-anchor-id="step-1-get-your-hugging-face-objects.">Step 1: Get your Hugging Face objects.</h4>
<p>There are a bunch of ways we can get at the four Hugging Face elements we need (e.g., architecture name, tokenizer, config, and model). We can just create them directly, or we can use one of the helper methods available via <code>NLP</code>.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb20"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a>model_cls <span class="op">=</span> AutoModelForSequenceClassification</span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-3"><a href="#cb20-3" aria-hidden="true" tabindex="-1"></a>pretrained_model_name <span class="op">=</span> <span class="st">"distilroberta-base"</span>  <span class="co"># "distilbert-base-uncased" "bert-base-uncased"</span></span>
<span id="cb20-4"><a href="#cb20-4" aria-hidden="true" tabindex="-1"></a>hf_arch, hf_config, hf_tokenizer, hf_model <span class="op">=</span> get_hf_objects(pretrained_model_name, model_cls<span class="op">=</span>model_cls)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="step-2-create-your-datablock" class="level4">
<h4 class="anchored" data-anchor-id="step-2-create-your-datablock">Step 2: Create your <code>DataBlock</code></h4>
<div class="cell">
<div class="sourceCode cell-code" id="cb21"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a>blocks <span class="op">=</span> (TextBlock(hf_arch, hf_config, hf_tokenizer, hf_model, batch_tokenize_kwargs<span class="op">=</span>{<span class="st">"labels"</span>: labels}), CategoryBlock)</span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a>dblock <span class="op">=</span> DataBlock(blocks<span class="op">=</span>blocks, get_x<span class="op">=</span>ColReader(<span class="st">"text"</span>), get_y<span class="op">=</span>ColReader(<span class="st">"label"</span>), splitter<span class="op">=</span>ColSplitter())</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="step-3-build-your-dataloaders" class="level4">
<h4 class="anchored" data-anchor-id="step-3-build-your-dataloaders">Step 3: Build your <code>DataLoaders</code></h4>
<div class="cell">
<div class="sourceCode cell-code" id="cb22"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a>dls <span class="op">=</span> dblock.dataloaders(imdb_df, bs<span class="op">=</span><span class="dv">4</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb23"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a>b <span class="op">=</span> dls.one_batch()</span>
<span id="cb23-2"><a href="#cb23-2" aria-hidden="true" tabindex="-1"></a><span class="bu">len</span>(b), <span class="bu">len</span>(b[<span class="dv">0</span>][<span class="st">"input_ids"</span>]), b[<span class="dv">0</span>][<span class="st">"input_ids"</span>].shape, <span class="bu">len</span>(b[<span class="dv">1</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>(2, 4, torch.Size([4, 512]), 4)</code></pre>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb25"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a>b[<span class="dv">0</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>{'input_ids': tensor([[   0, 5102, 3764,  ..., 1530,   36,    2],
         [   0,   22,  250,  ..., 5422,  278,    2],
         [   0, 9342, 1864,  ...,   80,    6,    2],
         [   0,  318,   47,  ..., 5320,  853,    2]], device='cuda:1'),
 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],
         [1, 1, 1,  ..., 1, 1, 1],
         [1, 1, 1,  ..., 1, 1, 1],
         [1, 1, 1,  ..., 1, 1, 1]], device='cuda:1'),
 'labels': TensorCategory([1, 0, 1, 1], device='cuda:1')}</code></pre>
</div>
</div>
<p>Let’s take a look at the actual types represented by our batch</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb27"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a>explode_types(b)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>{tuple: [dict, fastai.torch_core.TensorCategory]}</code></pre>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb29"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb29-1"><a href="#cb29-1" aria-hidden="true" tabindex="-1"></a>dls.show_batch(dataloaders<span class="op">=</span>dls, max_n<span class="op">=</span><span class="dv">2</span>, trunc_at<span class="op">=</span><span class="dv">500</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">

<table class="dataframe table table-sm table-striped">
  <thead>
    <tr>
      <th></th>
      <th>text</th>
      <th>target</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>ANCHORS AWEIGH sees two eager young sailors, Joe Brady (Gene Kelly) and Clarence Doolittle/Brooklyn (Frank Sinatra), get a special four-day shore leave. Eager to get to the girls, particularly Joe's Lola, neither Joe nor Brooklyn figure on the interruption of little Navy-mad Donald (Dean Stockwell) and his Aunt Susie (Kathryn Grayson). Unexperienced in the ways of females and courting, Brooklyn quickly enlists Joe to help him win Aunt Susie over. Along the way, however, Joe finds himself fallin</td>
      <td>pos</td>
    </tr>
    <tr>
      <th>1</th>
      <td>WARNING: POSSIBLE SPOILERS (Not that you should care. Also, sorry for the caps.)&lt;br /&gt;&lt;br /&gt;Starting with an unnecessarily dramatic voice that's all the more annoying for talking nonsense, it goes on with nonsense and unnecessary drama. That's badly but accurately put.&lt;br /&gt;&lt;br /&gt;We know space travel is a risky enterprise. There's a complicated system with a lot of potential for malfunctions, radiation, stress-related symptoms etc, and unexpected things are bound to happen in largely unknown en</td>
      <td>neg</td>
    </tr>
  </tbody>
</table>
</div>
</div>
</section>
</section>
<section id="using-a-preprocessed-dataset" class="level3">
<h3 class="anchored" data-anchor-id="using-a-preprocessed-dataset">Using a preprocessed dataset</h3>
<p>Preprocessing your raw data is the more traditional approach to using Transformers. It is required, for example, when you want to work with documents longer than your model will allow. A preprocessed dataset is used in the same way a non-preprocessed dataset is.</p>
<section id="step-1a-get-your-hugging-face-objects." class="level4">
<h4 class="anchored" data-anchor-id="step-1a-get-your-hugging-face-objects.">Step 1a: Get your Hugging Face objects.</h4>
<div class="cell">
<div class="sourceCode cell-code" id="cb30"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb30-1"><a href="#cb30-1" aria-hidden="true" tabindex="-1"></a>hf_arch, hf_config, hf_tokenizer, hf_model <span class="op">=</span> get_hf_objects(pretrained_model_name, model_cls<span class="op">=</span>model_cls)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="step-1b.-preprocess-dataset" class="level4">
<h4 class="anchored" data-anchor-id="step-1b.-preprocess-dataset">Step 1b. Preprocess dataset</h4>
<div class="cell">
<div class="sourceCode cell-code" id="cb31"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb31-1"><a href="#cb31-1" aria-hidden="true" tabindex="-1"></a>preprocessor <span class="op">=</span> ClassificationPreprocessor(hf_tokenizer, label_mapping<span class="op">=</span>labels)</span>
<span id="cb31-2"><a href="#cb31-2" aria-hidden="true" tabindex="-1"></a>proc_ds <span class="op">=</span> preprocessor.process_hf_dataset(final_ds)</span>
<span id="cb31-3"><a href="#cb31-3" aria-hidden="true" tabindex="-1"></a>proc_ds</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>Dataset({
    features: ['proc_text', 'text', 'label', 'is_valid', 'label_name', 'text_start_char_idx', 'text_end_char_idx'],
    num_rows: 1200
})</code></pre>
</div>
</div>
</section>
<section id="step-2-create-your-datablock-1" class="level4">
<h4 class="anchored" data-anchor-id="step-2-create-your-datablock-1">Step 2: Create your <code>DataBlock</code></h4>
<div class="cell">
<div class="sourceCode cell-code" id="cb33"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb33-1"><a href="#cb33-1" aria-hidden="true" tabindex="-1"></a>blocks <span class="op">=</span> (TextBlock(hf_arch, hf_config, hf_tokenizer, hf_model, batch_tokenize_kwargs<span class="op">=</span>{<span class="st">"labels"</span>: labels}), CategoryBlock)</span>
<span id="cb33-2"><a href="#cb33-2" aria-hidden="true" tabindex="-1"></a>dblock <span class="op">=</span> DataBlock(blocks<span class="op">=</span>blocks, get_x<span class="op">=</span>ItemGetter(<span class="st">"proc_text"</span>), get_y<span class="op">=</span>ItemGetter(<span class="st">"label"</span>), splitter<span class="op">=</span>RandomSplitter())</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="step-3-build-your-dataloaders-1" class="level4">
<h4 class="anchored" data-anchor-id="step-3-build-your-dataloaders-1">Step 3: Build your <code>DataLoaders</code></h4>
<div class="cell">
<div class="sourceCode cell-code" id="cb34"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb34-1"><a href="#cb34-1" aria-hidden="true" tabindex="-1"></a>dls <span class="op">=</span> dblock.dataloaders(proc_ds, bs<span class="op">=</span><span class="dv">4</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb35"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb35-1"><a href="#cb35-1" aria-hidden="true" tabindex="-1"></a>dls.show_batch(dataloaders<span class="op">=</span>dls, max_n<span class="op">=</span><span class="dv">2</span>, trunc_at<span class="op">=</span><span class="dv">500</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">

<table class="dataframe table table-sm table-striped">
  <thead>
    <tr>
      <th></th>
      <th>text</th>
      <th>target</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>I saw this film at the Adelaide Film Festival '07 and was thoroughly intrigued for all 106 minutes. I like documentaries, but often find them dragging with about 25 minutes to go. Forbidden Lie$ powered on though, never losing my interest.&lt;br /&gt;&lt;br /&gt;The film's subject is Norma Khoury, a Jordanian woman who found fame and fortune in 2001 with the publication of her book Forbidden Love, a biographical story of sorts concerning a Muslim friend of hers who was murdered by her family for having a r</td>
      <td>pos</td>
    </tr>
    <tr>
      <th>1</th>
      <td>ANCHORS AWEIGH sees two eager young sailors, Joe Brady (Gene Kelly) and Clarence Doolittle/Brooklyn (Frank Sinatra), get a special four-day shore leave. Eager to get to the girls, particularly Joe's Lola, neither Joe nor Brooklyn figure on the interruption of little Navy-mad Donald (Dean Stockwell) and his Aunt Susie (Kathryn Grayson). Unexperienced in the ways of females and courting, Brooklyn quickly enlists Joe to help him win Aunt Susie over. Along the way, however, Joe finds himself fallin</td>
      <td>pos</td>
    </tr>
  </tbody>
</table>
</div>
</div>
</section>
</section>
<section id="passing-extra-information" class="level3">
<h3 class="anchored" data-anchor-id="passing-extra-information">Passing extra information</h3>
<p>As of v.2, <code>BLURR</code> now also allows you to pass extra information alongside your inputs in the form of a dictionary. If you use this approach, you must assign your text(s) to the <code>text</code> attribute of the dictionary. This is a useful approach when splitting long documents into chunks, but wanting to score/predict by example rather than chunk (for example in extractive question answering tasks).</p>
<p><strong>Note</strong>: A good place to access to this extra information during training/validation is in the <code>before_batch</code> method of a <code>Callback</code>.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb36"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb36-1"><a href="#cb36-1" aria-hidden="true" tabindex="-1"></a>blocks <span class="op">=</span> (TextBlock(hf_arch, hf_config, hf_tokenizer, hf_model, batch_tokenize_kwargs<span class="op">=</span>{<span class="st">"labels"</span>: labels}), CategoryBlock)</span>
<span id="cb36-2"><a href="#cb36-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-3"><a href="#cb36-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-4"><a href="#cb36-4" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> get_x(item):</span>
<span id="cb36-5"><a href="#cb36-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> {<span class="st">"text"</span>: item.text, <span class="st">"another_val"</span>: <span class="st">"testing123"</span>}</span>
<span id="cb36-6"><a href="#cb36-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-7"><a href="#cb36-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-8"><a href="#cb36-8" aria-hidden="true" tabindex="-1"></a>dblock <span class="op">=</span> DataBlock(blocks<span class="op">=</span>blocks, get_x<span class="op">=</span>get_x, get_y<span class="op">=</span>ColReader(<span class="st">"label"</span>), splitter<span class="op">=</span>ColSplitter())</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb37"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb37-1"><a href="#cb37-1" aria-hidden="true" tabindex="-1"></a>dls <span class="op">=</span> dblock.dataloaders(imdb_df, bs<span class="op">=</span><span class="dv">4</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb38"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb38-1"><a href="#cb38-1" aria-hidden="true" tabindex="-1"></a>b <span class="op">=</span> dls.one_batch()</span>
<span id="cb38-2"><a href="#cb38-2" aria-hidden="true" tabindex="-1"></a><span class="bu">len</span>(b), <span class="bu">len</span>(b[<span class="dv">0</span>][<span class="st">"input_ids"</span>]), b[<span class="dv">0</span>][<span class="st">"input_ids"</span>].shape, <span class="bu">len</span>(b[<span class="dv">1</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>(2, 4, torch.Size([4, 512]), 4)</code></pre>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb40"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb40-1"><a href="#cb40-1" aria-hidden="true" tabindex="-1"></a>dls.show_batch(dataloaders<span class="op">=</span>dls, max_n<span class="op">=</span><span class="dv">2</span>, trunc_at<span class="op">=</span><span class="dv">500</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">

<table class="dataframe table table-sm table-striped">
  <thead>
    <tr>
      <th></th>
      <th>text</th>
      <th>target</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>ANCHORS AWEIGH sees two eager young sailors, Joe Brady (Gene Kelly) and Clarence Doolittle/Brooklyn (Frank Sinatra), get a special four-day shore leave. Eager to get to the girls, particularly Joe's Lola, neither Joe nor Brooklyn figure on the interruption of little Navy-mad Donald (Dean Stockwell) and his Aunt Susie (Kathryn Grayson). Unexperienced in the ways of females and courting, Brooklyn quickly enlists Joe to help him win Aunt Susie over. Along the way, however, Joe finds himself fallin</td>
      <td>pos</td>
    </tr>
    <tr>
      <th>1</th>
      <td>I've rented and watched this movie for the 1st time on DVD without reading any reviews about it. So, after 15 minutes of watching I've noticed that something is wrong with this movie; it's TERRIBLE! I mean, in the trailers it looked scary and serious!&lt;br /&gt;&lt;br /&gt;I think that Eli Roth (Mr. Director) thought that if all the characters in this film were stupid, the movie would be funny...(So stupid, it's funny...? WRONG!) He should watch and learn from better horror-comedies such as:"Fright Night"</td>
      <td>neg</td>
    </tr>
  </tbody>
</table>
</div>
</div>
</section>
</section>
<section id="low-level-api" class="level2">
<h2 class="anchored" data-anchor-id="low-level-api">Low-level API</h2>
<p>For working with PyTorch and/or fast.ai Datasets &amp; DataLoaders, the low-level API allows you to get back fast.ai specific features such as <a href="https://ohmeow.github.io/blurr/text-data-token-classification.html#show_batch"><code>show_batch</code></a>, <a href="https://ohmeow.github.io/blurr/text-modeling-token-classification.html#show_results"><code>show_results</code></a>, etc… when using plain ol’ PyTorch Datasets, Hugging Face Datasets, etc…</p>
<hr>
<p><a href="https://github.com/ohmeow/blurr/blob/master/blurr/text/data/core.py#L533" target="_blank" style="float:right; font-size:smaller">source</a></p>
<section id="textbatchcreator" class="level3">
<h3 class="anchored" data-anchor-id="textbatchcreator">TextBatchCreator</h3>
<blockquote class="blockquote">
<pre><code> TextBatchCreator (hf_arch:str,
                   hf_config:transformers.configuration_utils.PretrainedCo
                   nfig, hf_tokenizer:transformers.tokenization_utils_base
                   .PreTrainedTokenizerBase,
                   hf_model:transformers.modeling_utils.PreTrainedModel,
                   data_collator:type=None)</code></pre>
</blockquote>
<p>A class that can be assigned to a <code>TfmdDL.create_batch</code> method; used to in Blurr’s low-level API to create batches that can be used in the Blurr library</p>
<hr>
<p><a href="https://github.com/ohmeow/blurr/blob/master/blurr/text/data/core.py#L568" target="_blank" style="float:right; font-size:smaller">source</a></p>
</section>
<section id="textdataloader" class="level3">
<h3 class="anchored" data-anchor-id="textdataloader">TextDataLoader</h3>
<blockquote class="blockquote">
<pre><code> TextDataLoader (dataset:torch.utils.data.dataset.Dataset|Datasets,
                 hf_arch:str, hf_config:PretrainedConfig,
                 hf_tokenizer:PreTrainedTokenizerBase,
                 hf_model:PreTrainedModel,
                 batch_creator:TextBatchCreator=None,
                 batch_decode_tfm:BatchDecodeTransform=None,
                 input_return_type:type=&lt;class '__main__.TextInput'&gt;,
                 preproccesing_func:Callable=None,
                 batch_decode_kwargs:dict={}, bs:int=64,
                 shuffle:bool=False, num_workers:int=None,
                 verbose:bool=False, do_setup:bool=True, pin_memory=False,
                 timeout=0, batch_size=None, drop_last=False,
                 indexed=None, n=None, device=None,
                 persistent_workers=False, pin_memory_device='', wif=None,
                 before_iter=None, after_item=None, before_batch=None,
                 after_batch=None, after_iter=None, create_batches=None,
                 create_item=None, create_batch=None, retain=None,
                 get_idxs=None, sample=None, shuffle_fn=None,
                 do_batch=None)</code></pre>
</blockquote>
<p>A transformed <code>DataLoader</code> that works with Blurr. From the fastai docs: A <code>TfmDL</code> is described as “a DataLoader that creates Pipeline from a list of Transforms for the callbacks <code>after_item</code>, <code>before_batch</code> and <code>after_batch</code>. As a result, it can decode or show a processed batch.</p>
<table class="table">
<colgroup>
<col style="width: 6%">
<col style="width: 25%">
<col style="width: 34%">
<col style="width: 34%">
</colgroup>
<thead>
<tr class="header">
<th></th>
<th><strong>Type</strong></th>
<th><strong>Default</strong></th>
<th><strong>Details</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>dataset</td>
<td>torch.utils.data.dataset.Dataset | Datasets</td>
<td></td>
<td>A standard PyTorch Dataset</td>
</tr>
<tr class="even">
<td>hf_arch</td>
<td>str</td>
<td></td>
<td>The abbreviation/name of your Hugging Face transformer architecture (not required if passing in an<br>
instance of <code>BatchTokenizeTransform</code> to <code>before_batch_tfm</code>)</td>
</tr>
<tr class="odd">
<td>hf_config</td>
<td>PretrainedConfig</td>
<td></td>
<td>A Hugging Face configuration object (not required if passing in an<br>
instance of <code>BatchTokenizeTransform</code> to <code>before_batch_tfm</code>)</td>
</tr>
<tr class="even">
<td>hf_tokenizer</td>
<td>PreTrainedTokenizerBase</td>
<td></td>
<td>A Hugging Face tokenizer (not required if passing in an instance of <code>BatchTokenizeTransform</code> to <code>before_batch_tfm</code>)</td>
</tr>
<tr class="odd">
<td>hf_model</td>
<td>PreTrainedModel</td>
<td></td>
<td>A Hugging Face model (not required if passing in an instance of <code>BatchTokenizeTransform</code> to <code>before_batch_tfm</code>)</td>
</tr>
<tr class="even">
<td>batch_creator</td>
<td>TextBatchCreator</td>
<td>None</td>
<td>An instance of <code>BlurrBatchCreator</code> or equivalent (defaults to <code>BlurrBatchCreator</code>)</td>
</tr>
<tr class="odd">
<td>batch_decode_tfm</td>
<td>BatchDecodeTransform</td>
<td>None</td>
<td>The batch_tfm used to decode Blurr batches (defaults to <code>BatchDecodeTransform</code>)</td>
</tr>
<tr class="even">
<td>input_return_type</td>
<td>type</td>
<td>TextInput</td>
<td>Used by typedispatched show methods</td>
</tr>
<tr class="odd">
<td>preproccesing_func</td>
<td>Callable</td>
<td>None</td>
<td>(optional) A preprocessing function that will be applied to your dataset</td>
</tr>
<tr class="even">
<td>batch_decode_kwargs</td>
<td>dict</td>
<td>{}</td>
<td>Keyword arguments to be applied to your <code>batch_decode_tfm</code></td>
</tr>
<tr class="odd">
<td>bs</td>
<td>int</td>
<td>64</td>
<td></td>
</tr>
<tr class="even">
<td>shuffle</td>
<td>bool</td>
<td>False</td>
<td></td>
</tr>
<tr class="odd">
<td>num_workers</td>
<td>int</td>
<td>None</td>
<td></td>
</tr>
<tr class="even">
<td>verbose</td>
<td>bool</td>
<td>False</td>
<td></td>
</tr>
<tr class="odd">
<td>do_setup</td>
<td>bool</td>
<td>True</td>
<td></td>
</tr>
<tr class="even">
<td>pin_memory</td>
<td>bool</td>
<td>False</td>
<td></td>
</tr>
<tr class="odd">
<td>timeout</td>
<td>int</td>
<td>0</td>
<td></td>
</tr>
<tr class="even">
<td>batch_size</td>
<td>NoneType</td>
<td>None</td>
<td></td>
</tr>
<tr class="odd">
<td>drop_last</td>
<td>bool</td>
<td>False</td>
<td></td>
</tr>
<tr class="even">
<td>indexed</td>
<td>NoneType</td>
<td>None</td>
<td></td>
</tr>
<tr class="odd">
<td>n</td>
<td>NoneType</td>
<td>None</td>
<td></td>
</tr>
<tr class="even">
<td>device</td>
<td>NoneType</td>
<td>None</td>
<td></td>
</tr>
<tr class="odd">
<td>persistent_workers</td>
<td>bool</td>
<td>False</td>
<td></td>
</tr>
<tr class="even">
<td>pin_memory_device</td>
<td>str</td>
<td></td>
<td></td>
</tr>
<tr class="odd">
<td>wif</td>
<td>NoneType</td>
<td>None</td>
<td></td>
</tr>
<tr class="even">
<td>before_iter</td>
<td>NoneType</td>
<td>None</td>
<td></td>
</tr>
<tr class="odd">
<td>after_item</td>
<td>NoneType</td>
<td>None</td>
<td></td>
</tr>
<tr class="even">
<td>before_batch</td>
<td>NoneType</td>
<td>None</td>
<td></td>
</tr>
<tr class="odd">
<td>after_batch</td>
<td>NoneType</td>
<td>None</td>
<td></td>
</tr>
<tr class="even">
<td>after_iter</td>
<td>NoneType</td>
<td>None</td>
<td></td>
</tr>
<tr class="odd">
<td>create_batches</td>
<td>NoneType</td>
<td>None</td>
<td></td>
</tr>
<tr class="even">
<td>create_item</td>
<td>NoneType</td>
<td>None</td>
<td></td>
</tr>
<tr class="odd">
<td>create_batch</td>
<td>NoneType</td>
<td>None</td>
<td></td>
</tr>
<tr class="even">
<td>retain</td>
<td>NoneType</td>
<td>None</td>
<td></td>
</tr>
<tr class="odd">
<td>get_idxs</td>
<td>NoneType</td>
<td>None</td>
<td></td>
</tr>
<tr class="even">
<td>sample</td>
<td>NoneType</td>
<td>None</td>
<td></td>
</tr>
<tr class="odd">
<td>shuffle_fn</td>
<td>NoneType</td>
<td>None</td>
<td></td>
</tr>
<tr class="even">
<td>do_batch</td>
<td>NoneType</td>
<td>None</td>
<td></td>
</tr>
</tbody>
</table>
</section>
</section>
<section id="low-level-examples" class="level2">
<h2 class="anchored" data-anchor-id="low-level-examples">Low-level Examples</h2>
<p>The following example demonstrates how to use the low-level API with standard PyTorch/Hugging Face/fast.ai Datasets and DataLoaders.</p>
<section id="step-1-build-your-datasets" class="level3">
<h3 class="anchored" data-anchor-id="step-1-build-your-datasets">Step 1: Build your datasets</h3>
<div class="cell">
<div class="sourceCode cell-code" id="cb43"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb43-1"><a href="#cb43-1" aria-hidden="true" tabindex="-1"></a>raw_datasets <span class="op">=</span> load_dataset(<span class="st">"glue"</span>, <span class="st">"mrpc"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>Reusing dataset glue (/home/wgilliam/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)</code></pre>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"54df9a1c83b34ef2a021221511093aea","version_major":2,"version_minor":0}
</script>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb45"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb45-1"><a href="#cb45-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> tokenize_function(example):</span>
<span id="cb45-2"><a href="#cb45-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> hf_tokenizer(example[<span class="st">"sentence1"</span>], example[<span class="st">"sentence2"</span>], truncation<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb45-3"><a href="#cb45-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb45-4"><a href="#cb45-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb45-5"><a href="#cb45-5" aria-hidden="true" tabindex="-1"></a>tokenized_datasets <span class="op">=</span> raw_datasets.<span class="bu">map</span>(tokenize_function, batched<span class="op">=</span><span class="va">True</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"cfa764c9215b4e00929d5824b4eba209","version_major":2,"version_minor":0}
</script>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"7d56ee0f63ef49f0826215aa5d84c6e7","version_major":2,"version_minor":0}
</script>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"0915a818078647a9bef8fb9465905433","version_major":2,"version_minor":0}
</script>
</div>
</div>
</section>
<section id="step-2-dataset-pre-processing-optional" class="level3">
<h3 class="anchored" data-anchor-id="step-2-dataset-pre-processing-optional">Step 2: Dataset pre-processing (optional)</h3>
<hr>
<p><a href="https://github.com/ohmeow/blurr/blob/master/blurr/text/data/core.py#L647" target="_blank" style="float:right; font-size:smaller">source</a></p>
<section id="preproc_hf_dataset" class="level4">
<h4 class="anchored" data-anchor-id="preproc_hf_dataset">preproc_hf_dataset</h4>
<blockquote class="blockquote">
<pre><code> preproc_hf_dataset
                     (dataset:torch.utils.data.dataset.Dataset|fastai.data
                     .core.Datasets, hf_tokenizer:transformers.tokenizatio
                     n_utils_base.PreTrainedTokenizerBase,
                     hf_model:transformers.modeling_utils.PreTrainedModel)</code></pre>
</blockquote>
<p>This method can be used to preprocess most Hugging Face Datasets for use in Blurr and other training libraries</p>
<table class="table">
<colgroup>
<col style="width: 9%">
<col style="width: 38%">
<col style="width: 52%">
</colgroup>
<thead>
<tr class="header">
<th></th>
<th><strong>Type</strong></th>
<th><strong>Details</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>dataset</td>
<td>torch.utils.data.dataset.Dataset | Datasets</td>
<td>A standard PyTorch Dataset or fast.ai Datasets</td>
</tr>
<tr class="even">
<td>hf_tokenizer</td>
<td>PreTrainedTokenizerBase</td>
<td>A Hugging Face tokenizer</td>
</tr>
<tr class="odd">
<td>hf_model</td>
<td>PreTrainedModel</td>
<td>A Hugging Face model</td>
</tr>
</tbody>
</table>
</section>
</section>
<section id="step-3-build-your-dataloaders." class="level3">
<h3 class="anchored" data-anchor-id="step-3-build-your-dataloaders.">Step 3: Build your <code>DataLoaders</code>.</h3>
<p>Use <code>BlurrDataLoader</code> to build Blurr friendly dataloaders from your datasets. Passing <code>{'labels': label_names}</code> to your <code>batch_tfm_kwargs</code> will ensure that your lable/target names will be displayed in methods like <a href="https://ohmeow.github.io/blurr/text-data-token-classification.html#show_batch"><code>show_batch</code></a> and <a href="https://ohmeow.github.io/blurr/text-modeling-token-classification.html#show_results"><code>show_results</code></a> (just as it works with the mid-level API)</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb47"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb47-1"><a href="#cb47-1" aria-hidden="true" tabindex="-1"></a>label_names <span class="op">=</span> raw_datasets[<span class="st">"train"</span>].features[<span class="st">"label"</span>].names</span>
<span id="cb47-2"><a href="#cb47-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-3"><a href="#cb47-3" aria-hidden="true" tabindex="-1"></a>trn_dl <span class="op">=</span> TextDataLoader(</span>
<span id="cb47-4"><a href="#cb47-4" aria-hidden="true" tabindex="-1"></a>    tokenized_datasets[<span class="st">"train"</span>],</span>
<span id="cb47-5"><a href="#cb47-5" aria-hidden="true" tabindex="-1"></a>    hf_arch,</span>
<span id="cb47-6"><a href="#cb47-6" aria-hidden="true" tabindex="-1"></a>    hf_config,</span>
<span id="cb47-7"><a href="#cb47-7" aria-hidden="true" tabindex="-1"></a>    hf_tokenizer,</span>
<span id="cb47-8"><a href="#cb47-8" aria-hidden="true" tabindex="-1"></a>    hf_model,</span>
<span id="cb47-9"><a href="#cb47-9" aria-hidden="true" tabindex="-1"></a>    preproccesing_func<span class="op">=</span>preproc_hf_dataset,</span>
<span id="cb47-10"><a href="#cb47-10" aria-hidden="true" tabindex="-1"></a>    batch_decode_kwargs<span class="op">=</span>{<span class="st">"labels"</span>: label_names},</span>
<span id="cb47-11"><a href="#cb47-11" aria-hidden="true" tabindex="-1"></a>    shuffle<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb47-12"><a href="#cb47-12" aria-hidden="true" tabindex="-1"></a>    batch_size<span class="op">=</span><span class="dv">8</span>,</span>
<span id="cb47-13"><a href="#cb47-13" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb47-14"><a href="#cb47-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-15"><a href="#cb47-15" aria-hidden="true" tabindex="-1"></a>val_dl <span class="op">=</span> TextDataLoader(</span>
<span id="cb47-16"><a href="#cb47-16" aria-hidden="true" tabindex="-1"></a>    tokenized_datasets[<span class="st">"validation"</span>],</span>
<span id="cb47-17"><a href="#cb47-17" aria-hidden="true" tabindex="-1"></a>    hf_arch,</span>
<span id="cb47-18"><a href="#cb47-18" aria-hidden="true" tabindex="-1"></a>    hf_config,</span>
<span id="cb47-19"><a href="#cb47-19" aria-hidden="true" tabindex="-1"></a>    hf_tokenizer,</span>
<span id="cb47-20"><a href="#cb47-20" aria-hidden="true" tabindex="-1"></a>    hf_model,</span>
<span id="cb47-21"><a href="#cb47-21" aria-hidden="true" tabindex="-1"></a>    preproccesing_func<span class="op">=</span>preproc_hf_dataset,</span>
<span id="cb47-22"><a href="#cb47-22" aria-hidden="true" tabindex="-1"></a>    batch_decode_kwargs<span class="op">=</span>{<span class="st">"labels"</span>: label_names},</span>
<span id="cb47-23"><a href="#cb47-23" aria-hidden="true" tabindex="-1"></a>    batch_size<span class="op">=</span><span class="dv">16</span>,</span>
<span id="cb47-24"><a href="#cb47-24" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb47-25"><a href="#cb47-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-26"><a href="#cb47-26" aria-hidden="true" tabindex="-1"></a>dls <span class="op">=</span> DataLoaders(trn_dl, val_dl)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb48"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb48-1"><a href="#cb48-1" aria-hidden="true" tabindex="-1"></a>b <span class="op">=</span> dls.one_batch()</span>
<span id="cb48-2"><a href="#cb48-2" aria-hidden="true" tabindex="-1"></a>b[<span class="dv">0</span>][<span class="st">"input_ids"</span>].shape</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>torch.Size([8, 65])</code></pre>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb50"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb50-1"><a href="#cb50-1" aria-hidden="true" tabindex="-1"></a>dls.show_batch(dataloaders<span class="op">=</span>dls, max_n<span class="op">=</span><span class="dv">2</span>, trunc_at<span class="op">=</span><span class="dv">800</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">

<table class="dataframe table table-sm table-striped">
  <thead>
    <tr>
      <th></th>
      <th>text</th>
      <th>target</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>The technology-laced Nasdaq Composite Index.IXIC inched down 1 point, or 0.11 percent, to 1,650. The broad Standard &amp; Poor's 500 Index.SPX inched up 3 points, or 0.32 percent, to 970.</td>
      <td>not_equivalent</td>
    </tr>
    <tr>
      <th>1</th>
      <td>His 1996 Chevrolet Tahoe was found abandoned June 25 in a Virginia Beach, Va., parking lot. His sport utility vehicle was found June 25, abandoned without its license plates in Virginia Beach, Va.</td>
      <td>equivalent</td>
    </tr>
  </tbody>
</table>
</div>
</div>
</section>
</section>
<section id="tests" class="level2">
<h2 class="anchored" data-anchor-id="tests">Tests</h2>
<p>The tests below to ensure the core DataBlock code above works for <strong>all</strong> pretrained sequence classification models available in Hugging Face. These tests are excluded from the CI workflow because of how long they would take to run and the amount of data that would be required to download.</p>
<p><strong>Note</strong>: Feel free to modify the code below to test whatever pretrained classification models you are working with … and if any of your pretrained sequence classification models fail, please submit a github issue <em>(or a PR if you’d like to fix it yourself)</em></p>
<div class="cell">
<div class="cell-output cell-output-display">

<table class="dataframe table table-sm table-striped">
  <thead>
    <tr>
      <th></th>
      <th>arch</th>
      <th>tokenizer</th>
      <th>model_name</th>
      <th>result</th>
      <th>error</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>albert</td>
      <td>AlbertTokenizerFast</td>
      <td>hf-internal-testing/tiny-albert</td>
      <td>PASSED</td>
      <td></td>
    </tr>
    <tr>
      <th>1</th>
      <td>bart</td>
      <td>BartTokenizerFast</td>
      <td>hf-internal-testing/tiny-random-bart</td>
      <td>PASSED</td>
      <td></td>
    </tr>
    <tr>
      <th>2</th>
      <td>bert</td>
      <td>BertTokenizerFast</td>
      <td>hf-internal-testing/tiny-bert</td>
      <td>PASSED</td>
      <td></td>
    </tr>
    <tr>
      <th>3</th>
      <td>big_bird</td>
      <td>BigBirdTokenizerFast</td>
      <td>google/bigbird-roberta-base</td>
      <td>PASSED</td>
      <td></td>
    </tr>
    <tr>
      <th>4</th>
      <td>bigbird_pegasus</td>
      <td>PegasusTokenizerFast</td>
      <td>google/bigbird-pegasus-large-arxiv</td>
      <td>PASSED</td>
      <td></td>
    </tr>
    <tr>
      <th>5</th>
      <td>ctrl</td>
      <td>CTRLTokenizer</td>
      <td>hf-internal-testing/tiny-random-ctrl</td>
      <td>PASSED</td>
      <td></td>
    </tr>
    <tr>
      <th>6</th>
      <td>camembert</td>
      <td>CamembertTokenizerFast</td>
      <td>camembert-base</td>
      <td>PASSED</td>
      <td></td>
    </tr>
    <tr>
      <th>7</th>
      <td>canine</td>
      <td>CanineTokenizer</td>
      <td>hf-internal-testing/tiny-random-canine</td>
      <td>PASSED</td>
      <td></td>
    </tr>
    <tr>
      <th>8</th>
      <td>convbert</td>
      <td>ConvBertTokenizerFast</td>
      <td>YituTech/conv-bert-base</td>
      <td>PASSED</td>
      <td></td>
    </tr>
    <tr>
      <th>9</th>
      <td>deberta</td>
      <td>DebertaTokenizerFast</td>
      <td>hf-internal-testing/tiny-deberta</td>
      <td>PASSED</td>
      <td></td>
    </tr>
    <tr>
      <th>10</th>
      <td>deberta_v2</td>
      <td>DebertaV2TokenizerFast</td>
      <td>hf-internal-testing/tiny-random-deberta-v2</td>
      <td>PASSED</td>
      <td></td>
    </tr>
    <tr>
      <th>11</th>
      <td>distilbert</td>
      <td>DistilBertTokenizerFast</td>
      <td>hf-internal-testing/tiny-random-distilbert</td>
      <td>PASSED</td>
      <td></td>
    </tr>
    <tr>
      <th>12</th>
      <td>electra</td>
      <td>ElectraTokenizerFast</td>
      <td>hf-internal-testing/tiny-electra</td>
      <td>PASSED</td>
      <td></td>
    </tr>
    <tr>
      <th>13</th>
      <td>fnet</td>
      <td>FNetTokenizerFast</td>
      <td>google/fnet-base</td>
      <td>PASSED</td>
      <td></td>
    </tr>
    <tr>
      <th>14</th>
      <td>flaubert</td>
      <td>FlaubertTokenizer</td>
      <td>hf-internal-testing/tiny-random-flaubert</td>
      <td>PASSED</td>
      <td></td>
    </tr>
    <tr>
      <th>15</th>
      <td>funnel</td>
      <td>FunnelTokenizerFast</td>
      <td>hf-internal-testing/tiny-random-funnel</td>
      <td>PASSED</td>
      <td></td>
    </tr>
    <tr>
      <th>16</th>
      <td>gpt2</td>
      <td>GPT2TokenizerFast</td>
      <td>hf-internal-testing/tiny-random-gpt2</td>
      <td>PASSED</td>
      <td></td>
    </tr>
    <tr>
      <th>17</th>
      <td>gptj</td>
      <td>GPT2TokenizerFast</td>
      <td>anton-l/gpt-j-tiny-random</td>
      <td>PASSED</td>
      <td></td>
    </tr>
    <tr>
      <th>18</th>
      <td>gpt_neo</td>
      <td>GPT2TokenizerFast</td>
      <td>hf-internal-testing/tiny-random-gpt_neo</td>
      <td>PASSED</td>
      <td></td>
    </tr>
    <tr>
      <th>19</th>
      <td>ibert</td>
      <td>RobertaTokenizer</td>
      <td>kssteven/ibert-roberta-base</td>
      <td>PASSED</td>
      <td></td>
    </tr>
    <tr>
      <th>20</th>
      <td>led</td>
      <td>LEDTokenizerFast</td>
      <td>hf-internal-testing/tiny-random-led</td>
      <td>PASSED</td>
      <td></td>
    </tr>
    <tr>
      <th>21</th>
      <td>longformer</td>
      <td>LongformerTokenizerFast</td>
      <td>hf-internal-testing/tiny-random-longformer</td>
      <td>PASSED</td>
      <td></td>
    </tr>
    <tr>
      <th>22</th>
      <td>mbart</td>
      <td>MBartTokenizerFast</td>
      <td>hf-internal-testing/tiny-random-mbart</td>
      <td>PASSED</td>
      <td></td>
    </tr>
    <tr>
      <th>23</th>
      <td>mpnet</td>
      <td>MPNetTokenizerFast</td>
      <td>hf-internal-testing/tiny-random-mpnet</td>
      <td>PASSED</td>
      <td></td>
    </tr>
    <tr>
      <th>24</th>
      <td>mobilebert</td>
      <td>MobileBertTokenizerFast</td>
      <td>hf-internal-testing/tiny-random-mobilebert</td>
      <td>PASSED</td>
      <td></td>
    </tr>
    <tr>
      <th>25</th>
      <td>openai</td>
      <td>OpenAIGPTTokenizerFast</td>
      <td>openai-gpt</td>
      <td>PASSED</td>
      <td></td>
    </tr>
    <tr>
      <th>26</th>
      <td>reformer</td>
      <td>ReformerTokenizerFast</td>
      <td>google/reformer-crime-and-punishment</td>
      <td>PASSED</td>
      <td></td>
    </tr>
    <tr>
      <th>27</th>
      <td>rembert</td>
      <td>RemBertTokenizerFast</td>
      <td>google/rembert</td>
      <td>PASSED</td>
      <td></td>
    </tr>
    <tr>
      <th>28</th>
      <td>roformer</td>
      <td>RoFormerTokenizerFast</td>
      <td>junnyu/roformer_chinese_sim_char_ft_small</td>
      <td>PASSED</td>
      <td></td>
    </tr>
    <tr>
      <th>29</th>
      <td>roberta</td>
      <td>RobertaTokenizerFast</td>
      <td>roberta-base</td>
      <td>PASSED</td>
      <td></td>
    </tr>
    <tr>
      <th>30</th>
      <td>squeezebert</td>
      <td>SqueezeBertTokenizerFast</td>
      <td>squeezebert/squeezebert-uncased</td>
      <td>PASSED</td>
      <td></td>
    </tr>
    <tr>
      <th>31</th>
      <td>transfo_xl</td>
      <td>TransfoXLTokenizer</td>
      <td>hf-internal-testing/tiny-random-transfo-xl</td>
      <td>PASSED</td>
      <td></td>
    </tr>
    <tr>
      <th>32</th>
      <td>xlm</td>
      <td>XLMTokenizer</td>
      <td>xlm-mlm-en-2048</td>
      <td>PASSED</td>
      <td></td>
    </tr>
    <tr>
      <th>33</th>
      <td>xlm_roberta</td>
      <td>XLMRobertaTokenizerFast</td>
      <td>xlm-roberta-base</td>
      <td>PASSED</td>
      <td></td>
    </tr>
    <tr>
      <th>34</th>
      <td>xlnet</td>
      <td>XLNetTokenizerFast</td>
      <td>xlnet-base-cased</td>
      <td>PASSED</td>
      <td></td>
    </tr>
  </tbody>
</table>
</div>
</div>
<p>The <code>text.data.core</code> module contains the fundamental bits for all data preprocessing tasks</p>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const disableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'prefetch';
    }
  }
  const enableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'stylesheet';
    }
  }
  const manageTransitions = (selector, allowTransitions) => {
    const els = window.document.querySelectorAll(selector);
    for (let i=0; i < els.length; i++) {
      const el = els[i];
      if (allowTransitions) {
        el.classList.remove('notransition');
      } else {
        el.classList.add('notransition');
      }
    }
  }
  const toggleColorMode = (alternate) => {
    // Switch the stylesheets
    const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
    manageTransitions('#quarto-margin-sidebar .nav-link', false);
    if (alternate) {
      enableStylesheet(alternateStylesheets);
      for (const sheetNode of alternateStylesheets) {
        if (sheetNode.id === "quarto-bootstrap") {
          toggleBodyColorMode(sheetNode);
        }
      }
    } else {
      disableStylesheet(alternateStylesheets);
      toggleBodyColorPrimary();
    }
    manageTransitions('#quarto-margin-sidebar .nav-link', true);
    // Switch the toggles
    const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
    for (let i=0; i < toggles.length; i++) {
      const toggle = toggles[i];
      if (toggle) {
        if (alternate) {
          toggle.classList.add("alternate");     
        } else {
          toggle.classList.remove("alternate");
        }
      }
    }
    // Hack to workaround the fact that safari doesn't
    // properly recolor the scrollbar when toggling (#1455)
    if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
      manageTransitions("body", false);
      window.scrollTo(0, 1);
      setTimeout(() => {
        window.scrollTo(0, 0);
        manageTransitions("body", true);
      }, 40);  
    }
  }
  const isFileUrl = () => { 
    return window.location.protocol === 'file:';
  }
  const hasAlternateSentinel = () => {  
    let styleSentinel = getColorSchemeSentinel();
    if (styleSentinel !== null) {
      return styleSentinel === "alternate";
    } else {
      return false;
    }
  }
  const setStyleSentinel = (alternate) => {
    const value = alternate ? "alternate" : "default";
    if (!isFileUrl()) {
      window.localStorage.setItem("quarto-color-scheme", value);
    } else {
      localAlternateSentinel = value;
    }
  }
  const getColorSchemeSentinel = () => {
    if (!isFileUrl()) {
      const storageValue = window.localStorage.getItem("quarto-color-scheme");
      return storageValue != null ? storageValue : localAlternateSentinel;
    } else {
      return localAlternateSentinel;
    }
  }
  let localAlternateSentinel = 'default';
  // Dark / light mode switch
  window.quartoToggleColorScheme = () => {
    // Read the current dark / light value 
    let toAlternate = !hasAlternateSentinel();
    toggleColorMode(toAlternate);
    setStyleSentinel(toAlternate);
  };
  // Ensure there is a toggle, if there isn't float one in the top right
  if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
    const a = window.document.createElement('a');
    a.classList.add('top-right');
    a.classList.add('quarto-color-scheme-toggle');
    a.href = "";
    a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
    const i = window.document.createElement("i");
    i.classList.add('bi');
    a.appendChild(i);
    window.document.body.appendChild(a);
  }
  // Switch to dark mode if need be
  if (hasAlternateSentinel()) {
    toggleColorMode(true);
  } 
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    setTimeout(function() {
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const cites = ref.parentNode.getAttribute('data-cites').split(' ');
    tippyHover(ref, function() {
      var popup = window.document.createElement('div');
      cites.forEach(function(cite) {
        var citeDiv = window.document.createElement('div');
        citeDiv.classList.add('hanging-indent');
        citeDiv.classList.add('csl-entry');
        var biblioDiv = window.document.getElementById('ref-' + cite);
        if (biblioDiv) {
          citeDiv.innerHTML = biblioDiv.innerHTML;
        }
        popup.appendChild(citeDiv);
      });
      return popup.innerHTML;
    });
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var filterRegex = new RegExp(/https:\/\/ohmeow\.github\.io\/blurr\//);
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
    var links = window.document.querySelectorAll('a:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
          // target, if specified
          link.setAttribute("target", "_blank");
      }
    }
});
</script>
</div> <!-- /content -->



</body></html>