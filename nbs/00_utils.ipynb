{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "skip_exec: true\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils\n",
    "\n",
    "> Library-wide utility classes and functions used within the `BLURR` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |default_exp utils\n",
    "# |default_cls_lvl 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "from __future__ import annotations\n",
    "\n",
    "import gc, importlib, sys, traceback\n",
    "\n",
    "from accelerate.logging import get_logger\n",
    "from dotenv import load_dotenv\n",
    "from fastai.callback.all import *\n",
    "from fastai.imports import *\n",
    "from fastai.learner import *\n",
    "from fastai.losses import BaseLoss, BCEWithLogitsLossFlat, CrossEntropyLossFlat\n",
    "from fastai.test_utils import show_install\n",
    "from fastai.torch_core import *\n",
    "from fastai.torch_imports import *\n",
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoTokenizer,\n",
    "    PretrainedConfig,\n",
    "    PreTrainedTokenizerBase,\n",
    "    PreTrainedModel,\n",
    ")\n",
    "from transformers import logging as hf_logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |hide\n",
    "import pdb, nbdev\n",
    "\n",
    "from fastcore.test import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# |export\n",
    "# silence all the HF warnings and load environment variables\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "warnings.simplefilter(\"ignore\")\n",
    "hf_logging.set_verbosity_error()\n",
    "logger = get_logger(__name__)\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU #0: NVIDIA GeForce RTX 3090\n"
     ]
    }
   ],
   "source": [
    "# |hide\n",
    "# |notest\n",
    "torch.cuda.set_device(0)\n",
    "print(f\"Using GPU #{torch.cuda.current_device()}: {torch.cuda.get_device_name()}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defaults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "DEFAULT_SEED = int(os.getenv(\"RANDOM_SEED\", 2023))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General\n",
    "\n",
    "Inclues an implementation of the `Singleton` pattern that can be used as a python decorator.  Use this above any class to turn that class into a singleton (see [here](https://python-3-patterns-idioms-test.readthedocs.io/en/latest/Singleton.html) for more info on the singleton pattern)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "class Singleton:\n",
    "    def __init__(self, cls):\n",
    "        self._cls, self._instance = cls, None\n",
    "\n",
    "    def __call__(self, *args, **kwargs):\n",
    "        if self._instance == None:\n",
    "            self._instance = self._cls(*args, **kwargs)\n",
    "        return self._instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@Singleton\n",
    "class TestSingleton:\n",
    "    pass\n",
    "\n",
    "\n",
    "a = TestSingleton()\n",
    "b = TestSingleton()\n",
    "test_eq(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "def str_to_type(\n",
    "    typename: str,\n",
    ") -> type:  # The name of a type as a string  # Returns the actual type\n",
    "    \"Converts a type represented as a string to the actual class\"\n",
    "    return getattr(sys.modules[__name__], typename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/ohmeow/blurr/blob/dev-3.0.0 #master/blurr/utils.py#L53){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### str_to_type\n",
       "\n",
       ">      str_to_type (typename:str)\n",
       "\n",
       "Converts a type represented as a string to the actual class\n",
       "\n",
       "|    | **Type** | **Details** |\n",
       "| -- | -------- | ----------- |\n",
       "| typename | str |  |\n",
       "| **Returns** | **type** | **The name of a type as a string  # Returns the actual type** |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/ohmeow/blurr/blob/dev-3.0.0 #master/blurr/utils.py#L53){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### str_to_type\n",
       "\n",
       ">      str_to_type (typename:str)\n",
       "\n",
       "Converts a type represented as a string to the actual class\n",
       "\n",
       "|    | **Type** | **Details** |\n",
       "| -- | -------- | ----------- |\n",
       "| typename | str |  |\n",
       "| **Returns** | **type** | **The name of a type as a string  # Returns the actual type** |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nbdev.show_doc(str_to_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<function test_eq>\n",
      "<__main__.Singleton object>\n"
     ]
    }
   ],
   "source": [
    "print(str_to_type(\"test_eq\"))\n",
    "print(str_to_type(\"TestSingleton\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "# see the following threads for more info:\n",
    "# - https://forums.fast.ai/t/solved-reproducibility-where-is-the-randomness-coming-in/31628?u=wgpubs\n",
    "# - https://docs.fast.ai/dev/test.html#getting-reproducible-results\n",
    "def set_seed(seed_value: int = 2023):\n",
    "    \"\"\"This needs to be ran before creating your DataLoaders, before creating your Learner, and before each call\n",
    "    to your fit function to help ensure reproducibility.\n",
    "    \"\"\"\n",
    "    np.random.seed(seed_value)  # cpu vars\n",
    "    torch.manual_seed(seed_value)  # cpu vars\n",
    "    random.seed(seed_value)  # python\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed_value)\n",
    "        torch.cuda.manual_seed_all(seed_value)  # gpu vars\n",
    "        torch.backends.cudnn.deterministic = True  # needed\n",
    "        torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/ohmeow/blurr/blob/dev-3.0.0 #master/blurr/utils.py#L63){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### set_seed\n",
       "\n",
       ">      set_seed (seed_value:int=2023)\n",
       "\n",
       "This needs to be ran before creating your DataLoaders, before creating your Learner, and before each call\n",
       "to your fit function to help ensure reproducibility."
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/ohmeow/blurr/blob/dev-3.0.0 #master/blurr/utils.py#L63){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### set_seed\n",
       "\n",
       ">      set_seed (seed_value:int=2023)\n",
       "\n",
       "This needs to be ran before creating your DataLoaders, before creating your Learner, and before each call\n",
       "to your fit function to help ensure reproducibility."
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nbdev.show_doc(set_seed, title_level=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(DEFAULT_SEED)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Development Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "def print_versions(\n",
    "    # A string of space delimited package names or a list of package names\n",
    "    packages: str\n",
    "    | list[str],\n",
    "):\n",
    "    \"\"\"Prints the name and version of one or more packages in your environment\"\"\"\n",
    "    packages = packages.split(\" \") if isinstance(packages, str) else packages\n",
    "\n",
    "    for item in packages:\n",
    "        item = item.strip()\n",
    "        print(f\"{item}: {importlib.import_module(item).__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/ohmeow/blurr/blob/dev-3.0.0 #master/blurr/utils.py#L78){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### print_versions\n",
       "\n",
       ">      print_versions (packages:str|list[str])\n",
       "\n",
       "Prints the name and version of one or more packages in your environment\n",
       "\n",
       "|    | **Type** | **Details** |\n",
       "| -- | -------- | ----------- |\n",
       "| packages | str \\| list[str] | A string of space delimited package names or a list of package names |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/ohmeow/blurr/blob/dev-3.0.0 #master/blurr/utils.py#L78){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### print_versions\n",
       "\n",
       ">      print_versions (packages:str|list[str])\n",
       "\n",
       "Prints the name and version of one or more packages in your environment\n",
       "\n",
       "|    | **Type** | **Details** |\n",
       "| -- | -------- | ----------- |\n",
       "| packages | str \\| list[str] | A string of space delimited package names or a list of package names |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nbdev.show_doc(print_versions, title_level=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch: 1.13.1\n",
      "transformers: 4.26.1\n",
      "fastai: 2.7.11\n",
      "---\n",
      "torch: 1.13.1\n",
      "transformers: 4.26.1\n",
      "fastai: 2.7.11\n"
     ]
    }
   ],
   "source": [
    "print_versions(\"torch transformers fastai\")\n",
    "print(\"---\")\n",
    "print_versions([\"torch\", \"transformers\", \"fastai\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "def print_dev_environment():\n",
    "    \"\"\"Provides details on your development environment including packages installed, cuda/cudnn availability, GPUs, etc.\"\"\"\n",
    "    print(show_install())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/ohmeow/blurr/blob/dev-3.0.0 #master/blurr/utils.py#L91){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### print_dev_environment\n",
       "\n",
       ">      print_dev_environment ()\n",
       "\n",
       "Provides details on your development environment including packages installed, cuda/cudnn availability, GPUs, etc."
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/ohmeow/blurr/blob/dev-3.0.0 #master/blurr/utils.py#L91){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### print_dev_environment\n",
       "\n",
       ">      print_dev_environment ()\n",
       "\n",
       "Provides details on your development environment including packages installed, cuda/cudnn availability, GPUs, etc."
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nbdev.show_doc(print_dev_environment, title_level=3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memory Management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "def clean_ipython_hist():\n",
    "    # Code in this function mainly copied from IPython source\n",
    "    if not \"get_ipython\" in globals():\n",
    "        return\n",
    "\n",
    "    ip = get_ipython()\n",
    "    user_ns = ip.user_ns\n",
    "    ip.displayhook.flush()\n",
    "    pc = ip.displayhook.prompt_count + 1\n",
    "\n",
    "    for n in range(1, pc):\n",
    "        user_ns.pop(\"_i\" + repr(n), None)\n",
    "\n",
    "    user_ns.update(dict(_i=\"\", _ii=\"\", _iii=\"\"))\n",
    "    hm = ip.history_manager\n",
    "    hm.input_hist_parsed[:] = [\"\"] * pc\n",
    "    hm.input_hist_raw[:] = [\"\"] * pc\n",
    "    hm._i = hm._ii = hm._iii = hm._i00 = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "def clean_tb():\n",
    "    # h/t Piotr Czapla\n",
    "    if hasattr(sys, \"last_traceback\"):\n",
    "        traceback.clear_frames(sys.last_traceback)\n",
    "        delattr(sys, \"last_traceback\")\n",
    "    if hasattr(sys, \"last_type\"):\n",
    "        delattr(sys, \"last_type\")\n",
    "    if hasattr(sys, \"last_value\"):\n",
    "        delattr(sys, \"last_value\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "def clean_memory(\n",
    "    # The fastai learner to delete\n",
    "    learn: Learner = None,\n",
    "):\n",
    "    \"\"\"A function which clears gpu memory.\"\"\"\n",
    "    if learn is not None:\n",
    "        del learn\n",
    "    clean_tb()\n",
    "    clean_ipython_hist()\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/ohmeow/blurr/blob/dev-3.0.0 #master/blurr/utils.py#L127){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### clean_memory\n",
       "\n",
       ">      clean_memory (learn:fastai.learner.Learner=None)\n",
       "\n",
       "A function which clears gpu memory.\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| learn | Learner | None | The fastai learner to delete |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/ohmeow/blurr/blob/dev-3.0.0 #master/blurr/utils.py#L127){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### clean_memory\n",
       "\n",
       ">      clean_memory (learn:fastai.learner.Learner=None)\n",
       "\n",
       "A function which clears gpu memory.\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| learn | Learner | None | The fastai learner to delete |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nbdev.show_doc(clean_memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_memory()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "class PreCalculatedLoss(BaseLoss):\n",
    "    \"\"\"\n",
    "    If you want to let your Hugging Face model calculate the loss for you, make sure you include the `labels` argument in your inputs and use\n",
    "    `PreCalculatedLoss` as your loss function. Even though we don't really need a loss function per se, we have to provide a custom loss class/function\n",
    "    for fastai to function properly (e.g. one with a `decodes` and `activation` methods).  Why?  Because these methods will get called in methods\n",
    "    like `show_results` to get the actual predictions.\n",
    "\n",
    "    Note: The Hugging Face models ***will always*** calculate the loss for you ***if*** you pass a `labels` dictionary along with your other inputs\n",
    "    (so only include it if that is what you intend to happen)\n",
    "    \"\"\"\n",
    "\n",
    "    def __call__(self, inp, targ, **kwargs):\n",
    "        return tensor(0.0)\n",
    "\n",
    "\n",
    "class PreCalculatedCrossEntropyLoss(PreCalculatedLoss, CrossEntropyLossFlat):\n",
    "    pass\n",
    "\n",
    "\n",
    "class PreCalculatedBCELoss(PreCalculatedLoss, BCEWithLogitsLossFlat):\n",
    "    pass\n",
    "\n",
    "\n",
    "class PreCalculatedMSELoss(PreCalculatedLoss):\n",
    "    def __init__(self, *args, axis=-1, floatify=True, **kwargs):\n",
    "        super().__init__(nn.MSELoss, *args, axis=axis, floatify=floatify, is_2d=False, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "class MultiTargetLoss(Module):\n",
    "    \"\"\"\n",
    "    Provides the ability to apply different loss functions to multi-modal targets/predictions.\n",
    "\n",
    "    This new loss function can be used in many other multi-modal architectures, with any mix of loss functions.\n",
    "    For example, this can be ammended to include the `is_impossible` task, as well as the start/end token tasks\n",
    "    in the SQUAD v2 dataset (or in any extractive question/answering task)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        # The loss function for each target\n",
    "        loss_classes: list[Callable] = [CrossEntropyLossFlat, CrossEntropyLossFlat],\n",
    "        # Any kwargs you want to pass to the loss functions above\n",
    "        loss_classes_kwargs: list[dict] = [{}, {}],\n",
    "        # The weights you want to apply to each loss (default: [1,1])\n",
    "        weights: list[float] | list[int] = [1, 1],\n",
    "        # The `reduction` parameter of the lass function (default: 'mean')\n",
    "        reduction: str = \"mean\",\n",
    "    ):\n",
    "        loss_funcs = [cls(reduction=reduction, **kwargs) for cls, kwargs in zip(loss_classes, loss_classes_kwargs)]\n",
    "        store_attr(self=self, names=\"loss_funcs, weights\")\n",
    "        self._reduction = reduction\n",
    "\n",
    "    # custom loss function must have either a reduction attribute or a reduction argument (like all fastai and\n",
    "    # PyTorch loss functions) so that the framework can change this as needed (e.g., when doing lear.get_preds\n",
    "    # it will set = 'none'). see this forum topic for more info: https://bit.ly/3br2Syz\n",
    "    @property\n",
    "    def reduction(self):\n",
    "        return self._reduction\n",
    "\n",
    "    @reduction.setter\n",
    "    def reduction(self, v):\n",
    "        self._reduction = v\n",
    "        for lf in self.loss_funcs:\n",
    "            lf.reduction = v\n",
    "\n",
    "    def forward(self, outputs, *targets):\n",
    "        loss = 0.0\n",
    "        for i, loss_func, weights, output, target in zip(range(len(outputs)), self.loss_funcs, self.weights, outputs, targets):\n",
    "            loss += weights * loss_func(output, target)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def activation(self, outs):\n",
    "        acts = [self.loss_funcs[i].activation(o) for i, o in enumerate(outs)]\n",
    "        return acts\n",
    "\n",
    "    def decodes(self, outs):\n",
    "        decodes = [self.loss_funcs[i].decodes(o) for i, o in enumerate(outs)]\n",
    "        return decodes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hugging Face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "def get_hf_objects(\n",
    "    pretrained_model_name_or_path: str | os.PathLike,\n",
    "    model_cls: PreTrainedModel,\n",
    "    config: PretrainedConfig | str | os.PathLike = None,\n",
    "    tokenizer_cls: PreTrainedTokenizerBase = None,\n",
    "    config_kwargs: dict = {},\n",
    "    tokenizer_kwargs: dict = {},\n",
    "    model_kwargs: dict = {},\n",
    "    cache_dir: str | os.PathLike = None,\n",
    ") -> tuple[str, PretrainedConfig, PreTrainedTokenizerBase, PreTrainedModel]:\n",
    "    \"\"\"\n",
    "    Given at minimum a `pretrained_model_name_or_path` and `model_cls (such as\n",
    "    `AutoModelForSequenceClassification\"), this method returns all the Hugging Face objects you need to train\n",
    "    a model using Blurr\n",
    "    \"\"\"\n",
    "    # config\n",
    "    if config is None:\n",
    "        config = AutoConfig.from_pretrained(pretrained_model_name_or_path, cache_dir=cache_dir, **config_kwargs)\n",
    "\n",
    "    # tokenizer (gpt2, roberta, bart (and maybe others) tokenizers require a prefix space)\n",
    "    if any(s in pretrained_model_name_or_path for s in [\"gpt2\", \"roberta\", \"bart\", \"longformer\"]):\n",
    "        tokenizer_kwargs = {**{\"add_prefix_space\": True}, **tokenizer_kwargs}\n",
    "\n",
    "    if tokenizer_cls is None:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path, cache_dir=cache_dir, **tokenizer_kwargs)\n",
    "    else:\n",
    "        tokenizer = tokenizer_cls.from_pretrained(pretrained_model_name_or_path, cache_dir=cache_dir, **tokenizer_kwargs)\n",
    "\n",
    "    # model\n",
    "    model = model_cls.from_pretrained(pretrained_model_name_or_path, config=config, cache_dir=cache_dir, **model_kwargs)\n",
    "\n",
    "    # arch\n",
    "    try:\n",
    "        arch = model.__module__.split(\".\")[2]\n",
    "    except:\n",
    "        arch = \"unknown\"\n",
    "\n",
    "    return (arch, config, tokenizer, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/ohmeow/blurr/blob/dev-3.0.0 #master/blurr/utils.py#L228){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "## get_hf_objects\n",
       "\n",
       ">      get_hf_objects (pretrained_model_name_or_path:str|os.PathLike,\n",
       ">                      model_cls:transformers.modeling_utils.PreTrainedModel, co\n",
       ">                      nfig:transformers.configuration_utils.PretrainedConfig|st\n",
       ">                      r|os.PathLike=None, tokenizer_cls:transformers.tokenizati\n",
       ">                      on_utils_base.PreTrainedTokenizerBase=None,\n",
       ">                      config_kwargs:dict={}, tokenizer_kwargs:dict={},\n",
       ">                      model_kwargs:dict={}, cache_dir:str|os.PathLike=None)\n",
       "\n",
       "Given at minimum a `pretrained_model_name_or_path` and `model_cls (such as\n",
       "`AutoModelForSequenceClassification\"), this method returns all the Hugging Face objects you need to train\n",
       "a model using Blurr"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/ohmeow/blurr/blob/dev-3.0.0 #master/blurr/utils.py#L228){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "## get_hf_objects\n",
       "\n",
       ">      get_hf_objects (pretrained_model_name_or_path:str|os.PathLike,\n",
       ">                      model_cls:transformers.modeling_utils.PreTrainedModel, co\n",
       ">                      nfig:transformers.configuration_utils.PretrainedConfig|st\n",
       ">                      r|os.PathLike=None, tokenizer_cls:transformers.tokenizati\n",
       ">                      on_utils_base.PreTrainedTokenizerBase=None,\n",
       ">                      config_kwargs:dict={}, tokenizer_kwargs:dict={},\n",
       ">                      model_kwargs:dict={}, cache_dir:str|os.PathLike=None)\n",
       "\n",
       "Given at minimum a `pretrained_model_name_or_path` and `model_cls (such as\n",
       "`AutoModelForSequenceClassification\"), this method returns all the Hugging Face objects you need to train\n",
       "a model using Blurr"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nbdev.show_doc(get_hf_objects, title_level=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert\n",
      "<class 'transformers.models.bert.configuration_bert.BertConfig'>\n",
      "<class 'transformers.models.bert.tokenization_bert_fast.BertTokenizerFast'>\n",
      "<class 'transformers.models.bert.modeling_bert.BertForMaskedLM'>\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForMaskedLM\n",
    "\n",
    "hf_logging.set_verbosity_error()\n",
    "\n",
    "arch, config, tokenizer, model = get_hf_objects(\"bert-base-cased-finetuned-mrpc\", model_cls=AutoModelForMaskedLM)\n",
    "\n",
    "test_eq(arch, \"bert\")\n",
    "test_eq(model.name_or_path, \"bert-base-cased-finetuned-mrpc\")\n",
    "test_eq(tokenizer.name_or_path, \"bert-base-cased-finetuned-mrpc\")\n",
    "test_eq(config._name_or_path, \"bert-base-cased-finetuned-mrpc\")\n",
    "\n",
    "print(arch)\n",
    "print(type(config))\n",
    "print(type(tokenizer))\n",
    "print(type(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "distilbert\n",
      "<class 'transformers.models.distilbert.configuration_distilbert.DistilBertConfig'>\n",
      "<class 'transformers.models.distilbert.tokenization_distilbert_fast.DistilBertTokenizerFast'>\n",
      "<class 'transformers.models.distilbert.modeling_distilbert.DistilBertForQuestionAnswering'>\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForQuestionAnswering\n",
    "\n",
    "hf_logging.set_verbosity_error()\n",
    "\n",
    "arch, config, tokenizer, model = get_hf_objects(\"distilbert-base-cased-distilled-squad\", model_cls=AutoModelForQuestionAnswering)\n",
    "\n",
    "test_eq(arch, \"distilbert\")\n",
    "test_eq(model.name_or_path, \"distilbert-base-cased-distilled-squad\")\n",
    "test_eq(tokenizer.name_or_path, \"distilbert-base-cased-distilled-squad\")\n",
    "test_eq(config._name_or_path, \"distilbert-base-cased-distilled-squad\")\n",
    "\n",
    "\n",
    "print(arch)\n",
    "print(type(config))\n",
    "print(type(tokenizer))\n",
    "print(type(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert\n",
      "<class 'transformers.models.bert.configuration_bert.BertConfig'>\n",
      "<class 'transformers.models.bert.tokenization_bert.BertTokenizer'>\n",
      "<class 'transformers.models.bert.modeling_bert.BertForNextSentencePrediction'>\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertForNextSentencePrediction\n",
    "\n",
    "hf_logging.set_verbosity_error()\n",
    "\n",
    "arch, config, tokenizer, model = get_hf_objects(\n",
    "    \"bert-base-cased-finetuned-mrpc\",\n",
    "    config=None,\n",
    "    tokenizer_cls=BertTokenizer,\n",
    "    model_cls=BertForNextSentencePrediction,\n",
    ")\n",
    "\n",
    "test_eq(arch, \"bert\")\n",
    "test_eq(model.name_or_path, \"bert-base-cased-finetuned-mrpc\")\n",
    "test_eq(tokenizer.name_or_path, \"bert-base-cased-finetuned-mrpc\")\n",
    "test_eq(config._name_or_path, \"bert-base-cased-finetuned-mrpc\")\n",
    "\n",
    "print(arch)\n",
    "print(type(config))\n",
    "print(type(tokenizer))\n",
    "print(type(model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "import nbdev\n",
    "\n",
    "nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
