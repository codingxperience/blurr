{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "skip_exec: true\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |hide\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Started\n",
    "\n",
    "> **BLURR** is a library designed for fastai developers who want to train and deploy Hugging Face transformers"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Named after the **fast**est **transformer** (well, at least of the Autobots), **BLURR** provides both a comprehensive and extensible framework for training and deploying 🤗 [huggingface](https://huggingface.co/transformers/) transformer models with [fastai](http://docs.fast.ai/) >= 2.0.\n",
    "\n",
    "Utilizing features like fastai's new `@typedispatch` and `@patch` decorators, along with a simple class hiearchy, **BLURR** provides fastai developers with the ability to train and deploy transformers on a variety of tasks. It includes a high, mid, and low-level API that will allow developers to use much of it out-of-the-box or customize it as needed.\n",
    "\n",
    "**Supported Text/NLP Tasks**:\n",
    "- Sequence Classification \n",
    "- Token Classification \n",
    "- Question Answering \n",
    "- Summarization \n",
    "- Tranlsation \n",
    "- Language Modeling (Causal and Masked)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can now pip install blurr via `pip install ohmeow-blurr`\n",
    "\n",
    "Or, even better as this library is under *very* active development, create an editable install like this:\n",
    "```\n",
    "git clone https://github.com/ohmeow/blurr.git\n",
    "cd blurr\n",
    "pip install -e \".[dev]\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to use"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please check the documentation for more thorough examples of how to use this package.\n",
    "\n",
    "The following two packages need to be installed for blurr to work: \n",
    "1. [fastai](http://docs.fast.ai/) \n",
    "2. [Hugging Face transformers](https://huggingface.co/transformers/installation.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wgilliam/mambaforge/envs/blurr-v3/lib/python3.10/site-packages/transformers/generation_utils.py:24: FutureWarning: Importing `GenerationMixin` from `src/transformers/generation_utils.py` is deprecated and will be removed in Transformers v5. Import as `from transformers import GenerationMixin` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# | output: false\n",
    "import os, warnings\n",
    "\n",
    "from datasets import load_dataset, concatenate_datasets\n",
    "import torch\n",
    "from transformers import *\n",
    "from transformers.utils import logging as hf_logging\n",
    "from fastai.text.all import *\n",
    "\n",
    "from blurr.data.core import *\n",
    "from blurr.training.core import *\n",
    "from blurr.utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.simplefilter(\"ignore\")\n",
    "hf_logging.set_verbosity_error()\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get your data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset imdb (/home/wgilliam/.cache/huggingface/datasets/imdb/plain_text/1.0.0/d613c88cf8fa3bab83b4ded3713f1f74830d1100e171db75bbddb80b3345c9c0)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76ffee182c2b4214819186cc6437804c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>is_valid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I grew up watching the original TV series in the sixties and one thing that I can tell you right away, there is NO comparison. This film was totally ridiculous with a flying suit that was alive. A martian that took different shapes. Special effects that looked like something that a little child would create. In contrast, in the original, characters were developed and the viewers developed a feeling for Tim and Uncle Martin. The only highlight in this film, yes, actually there was one, occurred when Ray Walston finally made an appearance at the end. He wore dark glasses and made references ...</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Damn, was that a lot to take in. I was pretty much mesmerised throughout. It was pretty perfect, though I would say the editing had a lot to do with that. I can't believe this guy stayed on good terms with the lot of them (Anton especially) to get all of this footage without any serious... beef. The Dandy's did come off well-together, middle-class kids who took advantage of their situation (and rightly so!). I felt bad for Jonestown and especially for Anton, which maybe wasn't what a lot of other people felt. Great piece of film-making and great choice of subject(s). I recommend this to an...</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I saw 'New York: I Love You' today and loved it! I was really looking forward to seeing this after watching 'Paris je t'aime' and overall I think I liked this one much better... Perhaps I need to watch 'Paris je t'aime' again I don't know... I read few of the reviews here about NY:ILY and yes, the movie is not without its faults. When you're paying tribute to a city like New York - it can get rather overwhelming and nothing seems fair enough to do the city due justice... so without elaborating on any of the film's shortcomings, I'll just write about what I liked.&lt;br /&gt;&lt;br /&gt;Unlike 'Paris j...</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>This was the worst MTV Movie Awards EVER!!! I barely laughed, none of the presenters were funny, the hosts really sucked, and the parodies weren't so great either. Why can't we go back to the good olden days when the show was a riot?</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Recap: It's business as usual at Louche's casino in Tanger. The casino is about to close and prepares for a big transaction the next day. The owner Louche and some staff leave for the night, leaving Modesty in charge. Suddenly a troop of armed gangsters storm the casino, shooting wildly. Unknown to Modesty, they have already killed Louche, and are now after the money hidden in the vault. But no one present, and still alive, at the casino knows the code to open the vault. The vault itself is heavily booby trapped with explosives so the assailants can't blow the door as planned. Suddenly Mod...</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      text  \\\n",
       "0  I grew up watching the original TV series in the sixties and one thing that I can tell you right away, there is NO comparison. This film was totally ridiculous with a flying suit that was alive. A martian that took different shapes. Special effects that looked like something that a little child would create. In contrast, in the original, characters were developed and the viewers developed a feeling for Tim and Uncle Martin. The only highlight in this film, yes, actually there was one, occurred when Ray Walston finally made an appearance at the end. He wore dark glasses and made references ...   \n",
       "1  Damn, was that a lot to take in. I was pretty much mesmerised throughout. It was pretty perfect, though I would say the editing had a lot to do with that. I can't believe this guy stayed on good terms with the lot of them (Anton especially) to get all of this footage without any serious... beef. The Dandy's did come off well-together, middle-class kids who took advantage of their situation (and rightly so!). I felt bad for Jonestown and especially for Anton, which maybe wasn't what a lot of other people felt. Great piece of film-making and great choice of subject(s). I recommend this to an...   \n",
       "2  I saw 'New York: I Love You' today and loved it! I was really looking forward to seeing this after watching 'Paris je t'aime' and overall I think I liked this one much better... Perhaps I need to watch 'Paris je t'aime' again I don't know... I read few of the reviews here about NY:ILY and yes, the movie is not without its faults. When you're paying tribute to a city like New York - it can get rather overwhelming and nothing seems fair enough to do the city due justice... so without elaborating on any of the film's shortcomings, I'll just write about what I liked.<br /><br />Unlike 'Paris j...   \n",
       "3                                                                                                                                                                                                                                                                                                                                                                                This was the worst MTV Movie Awards EVER!!! I barely laughed, none of the presenters were funny, the hosts really sucked, and the parodies weren't so great either. Why can't we go back to the good olden days when the show was a riot?   \n",
       "4  Recap: It's business as usual at Louche's casino in Tanger. The casino is about to close and prepares for a big transaction the next day. The owner Louche and some staff leave for the night, leaving Modesty in charge. Suddenly a troop of armed gangsters storm the casino, shooting wildly. Unknown to Modesty, they have already killed Louche, and are now after the money hidden in the vault. But no one present, and still alive, at the casino knows the code to open the vault. The vault itself is heavily booby trapped with explosives so the assailants can't blow the door as planned. Suddenly Mod...   \n",
       "\n",
       "   label  is_valid  \n",
       "0      0     False  \n",
       "1      1     False  \n",
       "2      1     False  \n",
       "3      0     False  \n",
       "4      1     False  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb_dsd = load_dataset(\"imdb\", split=[\"train\", \"test\"])\n",
    "\n",
    "# build HF `Dataset` objects\n",
    "train_ds = imdb_dsd[0].add_column(\"is_valid\", [False] * len(imdb_dsd[0])).shuffle().select(range(1000))\n",
    "valid_ds = imdb_dsd[1].add_column(\"is_valid\", [True] * len(imdb_dsd[1])).shuffle().select(range(200))\n",
    "imdb_ds = concatenate_datasets([train_ds, valid_ds])\n",
    "\n",
    "# build a `DataFrame` representation as well\n",
    "imdb_df = pd.DataFrame(imdb_ds)\n",
    "imdb_df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get `label_names` from data for config later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['neg', 'pos']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_names = imdb_dsd[0].features[\"label\"].names\n",
    "label_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get your 🤗 objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== config ===\n",
      "# of labels:\t2\n",
      "\n",
      "=== tokenizer ===\n",
      "Vocab size:\t\t30522\n",
      "Max # of tokens:\t512\n",
      "Attributes expected by model in forward pass:\t['input_ids', 'token_type_ids', 'attention_mask']\n"
     ]
    }
   ],
   "source": [
    "pretrained_model_name = \"bert-base-uncased\"\n",
    "\n",
    "hf_arch, hf_config, hf_tokenizer, hf_model = get_task_hf_objects(pretrained_model_name, label_names=label_names, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build your Data 🧱 and your DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# single input (note: we pass the label_names here because the labels in the dataset are already encoded as 0 or 1)\n",
    "blocks = (\n",
    "    TextBlock(\n",
    "        tokenize_tfm=BatchTokenizeTransform(hf_arch, hf_config, hf_tokenizer, hf_model), batch_decode_kwargs={\"label_names\": label_names}\n",
    "    ),\n",
    "    CategoryBlock,\n",
    ")\n",
    "\n",
    "dblock = DataBlock(\n",
    "    blocks=blocks,\n",
    "    get_x=ColReader(\"text\"),\n",
    "    get_y=ColReader(\"label\"),\n",
    "    splitter=ColSplitter(),\n",
    ")\n",
    "\n",
    "dls = dblock.dataloaders(imdb_df, bs=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>this movie was recently released on dvd in the us and i finally got the chance to see this hard - to - find gem. it even came with original theatrical previews of other italian horror classics like \" spasmo \" and \" beyond the darkness \". unfortunately, the previews were the best thing about this movie. &lt; br / &gt; &lt; br / &gt; \" zombi 3 \" in a bizarre way is actually linked to the infamous lucio fulci \" zombie \" franchise which began in 1979. similarly compared to \" zombie \", \" zombi 3 \" consists of a</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>i knew it wasn't gunna work out between me and d - wars from the moment we met. first its title was lazy. d war. like writing out dragon was too much for them. also... you really can't be that blatant with your title unless your blue monkey. blue monkey can do whatever the hell it wants. &lt; br / &gt; &lt; br / &gt; the second sign of a rocky relationship between us was the story's insane progression. here's the film, dreamy reporter guy reports on big snake tracks, flashes back to a time he and dad wander</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dls.show_batch(dataloaders=dls, max_n=2, trunc_at=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ... and 🚂"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |notest\n",
    "model = BaseModelWrapper(hf_model)\n",
    "\n",
    "learn = Learner(\n",
    "    dls,\n",
    "    hf_model,\n",
    "    opt_func=Adam,\n",
    "    loss_func=CrossEntropyLossFlat(),\n",
    "    metrics=[accuracy],\n",
    "    cbs=[BaseModelCallback],\n",
    "    splitter=blurr_splitter_on_head,\n",
    ")\n",
    "\n",
    "learn.create_opt()\n",
    "learn.freeze()\n",
    "\n",
    "learn.fit_one_cycle(3, lr_max=3e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>when it comes to those eerie and uncanny little crime films, the sorts that revolve around characters that are bordering on scum and inhabit equally scummy surroundings, and additionally carry that wavering and bleak feel thanks to some pretty grotty</td>\n",
       "      <td>pos</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>jean renoir's homage to the paris of the late 19th century is beautiful in many ways. not only does it appear to have been photographed by toulouse - lautrec and mucha, it portrays the geographic paris ; the streets accessible only by staircases, the</td>\n",
       "      <td>pos</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# |notest\n",
    "learn.show_results(learner=learn, max_n=2, trunc_at=250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |echo:false\n",
    "try:\n",
    "    del learn, hf_model\n",
    "except:\n",
    "    pass\n",
    "finally:\n",
    "    clean_memory()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the low-level blurr API"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BLURR now supports training with plain PyTorch Datasets/DataLoaders, Hugging Face Datasets, and/or fast.ai's low-level data API.  Here's an example of the later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== config ===\n",
      "# of labels:\t2\n",
      "\n",
      "=== tokenizer ===\n",
      "Vocab size:\t\t128000\n",
      "Max # of tokens:\t1000000000000000019884624838656\n",
      "Attributes expected by model in forward pass:\t['input_ids', 'token_type_ids', 'attention_mask']\n"
     ]
    }
   ],
   "source": [
    "hf_arch, hf_config, hf_tokenizer, hf_model = get_task_hf_objects(\"microsoft/deberta-v3-small\", label_names, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5470c2df2e274d1298b7127619b197a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# tokenize the dataset\n",
    "tokenize_func = partial(multiclass_tokenize_func, hf_tokenizer=hf_tokenizer)\n",
    "proc_imdb_ds = imdb_ds.map(tokenize_func, batched=True)\n",
    "\n",
    "# turn Arrow into DataFrame (`ColSplitter` only works with `DataFrame`s)\n",
    "train_df = pd.DataFrame(proc_imdb_ds)\n",
    "train_df.head()\n",
    "\n",
    "# define dataset splitter\n",
    "splitter = ColSplitter(\"is_valid\")\n",
    "splits = splitter(imdb_df)\n",
    "\n",
    "\n",
    "# define how we want to build our inputs and targets\n",
    "def _build_inputs(example):\n",
    "    return {fwd_arg_name: example[fwd_arg_name] for fwd_arg_name in hf_tokenizer.model_input_names if fwd_arg_name in list(example.keys())}\n",
    "\n",
    "\n",
    "def _build_targets(example):\n",
    "    return example[\"label\"]\n",
    "\n",
    "\n",
    "# create our fastai `Datasets` object\n",
    "dsets = Datasets(items=train_df, splits=splits, tfms=[[_build_inputs], _build_targets], n_inp=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = TextCollatorWithPadding(hf_tokenizer)\n",
    "sort_func = partial(sorted_dl_func, hf_tokenizer=hf_tokenizer)\n",
    "batch_decode_tfm = BatchDecodeTransform(hf_tokenizer, hf_arch, hf_config, hf_model, label_names=label_names)\n",
    "\n",
    "dls = dsets.dataloaders(\n",
    "    batch_size=4,\n",
    "    create_batch=data_collator,\n",
    "    after_batch=batch_decode_tfm,\n",
    "    dl_type=partial(SortedDL, sort_func=sort_func),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I grew up watching the original TV series in the sixties and one thing that I can tell you right away, there is NO comparison. This film was totally ridiculous with a flying suit that was alive. A martian that took different shapes. Special effects that looked like something that a little child would create. In contrast, in the original, characters were developed and the viewers developed a feeling for Tim and Uncle Martin. The only highlight in this film, yes, actually there was one, occurred w</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Poor Michael Madsen; he must be kicking himself to know folks have found out about this horrible flick. I really can't think of anything worse I have ever seen, except amateur porn. It's that bad, and all here; wooden acting, bad script, crappy moral ending, you hate it and it is in this movie.&lt;br /&gt;&lt;br /&gt;My question is: \"Who the Hell put $$$ into this piece of doggy doo? At least we could have seen Michael's sister Virginia nude in a scene, but I don't think even that would save this stinker...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I knew it wasn't gunna work out between me and D-wars from the moment we met. First its title was lazy. D war. Like writing out Dragon was too much for them. Also... you really can't be that blatant with your title unless your Blue Monkey. Blue Monkey can do whatever the hell it wants. &lt;br /&gt;&lt;br /&gt;The second sign of a rocky relationship between us was the story's insane progression. Here's the film, dreamy reporter guy reports on big snake tracks, flashes back to a time he and dad wandered into</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Minor Spoilers&lt;br /&gt;&lt;br /&gt;Alison Parker (Cristina Raines) is a successful top model, living with the lawyer Michael Lerman (Chris Sarandon) in his apartment. She tried to commit suicide twice in the past: the first time, when she was a teenager and saw her father cheating her mother with two women in her home, and then when Michael's wife died. Since then, she left Christ and the Catholic Church behind. Alison wants to live alone in her own apartment and with the help of the real state agent Mis</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dls.show_batch(dataloaders=dls, max_n=8, trunc_at=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |notest\n",
    "set_seed()\n",
    "\n",
    "model = BaseModelWrapper(hf_model)\n",
    "\n",
    "learn = Learner(\n",
    "    dls,\n",
    "    model,\n",
    "    opt_func=Adam,\n",
    "    loss_func=CrossEntropyLossFlat(),\n",
    "    metrics=[accuracy],\n",
    "    cbs=[BaseModelCallback],\n",
    "    splitter=blurr_splitter_on_head,\n",
    ")\n",
    "\n",
    "learn.create_opt()\n",
    "learn.freeze()\n",
    "\n",
    "learn = learn.to_fp16()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "# |notest\n",
    "print(len(learn.opt.param_groups))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaseModelWrapper (Input shape: 4 x 1793)\n",
       "============================================================================\n",
       "Layer (type)         Output Shape         Param #    Trainable \n",
       "============================================================================\n",
       "                     4 x 1793 x 768      \n",
       "Embedding                                 98380800   False     \n",
       "LayerNorm                                 1536       True      \n",
       "StableDropout                                                  \n",
       "Linear                                    590592     False     \n",
       "Linear                                    590592     False     \n",
       "Linear                                    590592     False     \n",
       "StableDropout                                                  \n",
       "StableDropout                                                  \n",
       "Linear                                    590592     False     \n",
       "LayerNorm                                 1536       True      \n",
       "StableDropout                                                  \n",
       "____________________________________________________________________________\n",
       "                     4 x 1793 x 3072     \n",
       "Linear                                    2362368    False     \n",
       "GELUActivation                                                 \n",
       "____________________________________________________________________________\n",
       "                     4 x 1793 x 768      \n",
       "Linear                                    2360064    False     \n",
       "LayerNorm                                 1536       True      \n",
       "StableDropout                                                  \n",
       "Linear                                    590592     False     \n",
       "Linear                                    590592     False     \n",
       "Linear                                    590592     False     \n",
       "StableDropout                                                  \n",
       "StableDropout                                                  \n",
       "Linear                                    590592     False     \n",
       "LayerNorm                                 1536       True      \n",
       "StableDropout                                                  \n",
       "____________________________________________________________________________\n",
       "                     4 x 1793 x 3072     \n",
       "Linear                                    2362368    False     \n",
       "GELUActivation                                                 \n",
       "____________________________________________________________________________\n",
       "                     4 x 1793 x 768      \n",
       "Linear                                    2360064    False     \n",
       "LayerNorm                                 1536       True      \n",
       "StableDropout                                                  \n",
       "Linear                                    590592     False     \n",
       "Linear                                    590592     False     \n",
       "Linear                                    590592     False     \n",
       "StableDropout                                                  \n",
       "StableDropout                                                  \n",
       "Linear                                    590592     False     \n",
       "LayerNorm                                 1536       True      \n",
       "StableDropout                                                  \n",
       "____________________________________________________________________________\n",
       "                     4 x 1793 x 3072     \n",
       "Linear                                    2362368    False     \n",
       "GELUActivation                                                 \n",
       "____________________________________________________________________________\n",
       "                     4 x 1793 x 768      \n",
       "Linear                                    2360064    False     \n",
       "LayerNorm                                 1536       True      \n",
       "StableDropout                                                  \n",
       "Linear                                    590592     False     \n",
       "Linear                                    590592     False     \n",
       "Linear                                    590592     False     \n",
       "StableDropout                                                  \n",
       "StableDropout                                                  \n",
       "Linear                                    590592     False     \n",
       "LayerNorm                                 1536       True      \n",
       "StableDropout                                                  \n",
       "____________________________________________________________________________\n",
       "                     4 x 1793 x 3072     \n",
       "Linear                                    2362368    False     \n",
       "GELUActivation                                                 \n",
       "____________________________________________________________________________\n",
       "                     4 x 1793 x 768      \n",
       "Linear                                    2360064    False     \n",
       "LayerNorm                                 1536       True      \n",
       "StableDropout                                                  \n",
       "Linear                                    590592     False     \n",
       "Linear                                    590592     False     \n",
       "Linear                                    590592     False     \n",
       "StableDropout                                                  \n",
       "StableDropout                                                  \n",
       "Linear                                    590592     False     \n",
       "LayerNorm                                 1536       True      \n",
       "StableDropout                                                  \n",
       "____________________________________________________________________________\n",
       "                     4 x 1793 x 3072     \n",
       "Linear                                    2362368    False     \n",
       "GELUActivation                                                 \n",
       "____________________________________________________________________________\n",
       "                     4 x 1793 x 768      \n",
       "Linear                                    2360064    False     \n",
       "LayerNorm                                 1536       True      \n",
       "StableDropout                                                  \n",
       "Linear                                    590592     False     \n",
       "Linear                                    590592     False     \n",
       "Linear                                    590592     False     \n",
       "StableDropout                                                  \n",
       "StableDropout                                                  \n",
       "Linear                                    590592     False     \n",
       "LayerNorm                                 1536       True      \n",
       "StableDropout                                                  \n",
       "____________________________________________________________________________\n",
       "                     4 x 1793 x 3072     \n",
       "Linear                                    2362368    False     \n",
       "GELUActivation                                                 \n",
       "____________________________________________________________________________\n",
       "                     4 x 1793 x 768      \n",
       "Linear                                    2360064    False     \n",
       "LayerNorm                                 1536       True      \n",
       "StableDropout                                                  \n",
       "LayerNorm                                 1536       True      \n",
       "Linear                                    590592     True      \n",
       "StableDropout                                                  \n",
       "____________________________________________________________________________\n",
       "                     4 x 2               \n",
       "Linear                                    1538       True      \n",
       "StableDropout                                                  \n",
       "____________________________________________________________________________\n",
       "\n",
       "Total params: 141,503,234\n",
       "Total trainable params: 613,634\n",
       "Total non-trainable params: 140,889,600\n",
       "\n",
       "Optimizer used: <function Adam>\n",
       "Loss function: FlattenedLoss of CrossEntropyLoss()\n",
       "\n",
       "Model frozen up to parameter group #1\n",
       "\n",
       "Callbacks:\n",
       "  - TrainEvalCallback\n",
       "  - BaseModelCallback\n",
       "  - CastToTensor\n",
       "  - MixedPrecision\n",
       "  - Recorder\n",
       "  - ProgressCallback"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# |notest\n",
    "learn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |notest\n",
    "learn.fit_one_cycle(3, lr_max=3e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>this film is what happens when people see like in this particular one blair witch project and say hell people running around with cameras, acting slash documentary themed no problemo i can do it and start out with a lame idea make up a terrible scrip</td>\n",
       "      <td>neg</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The film itself is only a compilation of scenes which have no inherent meaning to someone living outside of Russia. I won't deny that some of the images and techniques were quite revolutionary at the time (filmed 1928) but the problem with the film i</td>\n",
       "      <td>neg</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# |notest\n",
    "learn.show_results(learner=learn, max_n=2, trunc_at=250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |echo:false\n",
    "try:\n",
    "    del learn, hf_model\n",
    "except:\n",
    "    pass\n",
    "finally:\n",
    "    clean_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ⭐ Props\n",
    "\n",
    "A word of gratitude to the following individuals, repos, and articles upon which much of this work is inspired from:\n",
    "\n",
    "- The wonderful community that is the [fastai forum](https://forums.fast.ai/) and especially the tireless work of both Jeremy and Sylvain in building this amazing framework and place to learn deep learning.\n",
    "- All the great tokenizers, transformers, docs, examples, and people over at [huggingface](https://huggingface.co/)\n",
    "- [FastHugs](https://github.com/morganmcg1/fasthugs)\n",
    "- [Fastai with 🤗Transformers (BERT, RoBERTa, XLNet, XLM, DistilBERT)](https://towardsdatascience.com/fastai-with-transformers-bert-roberta-xlnet-xlm-distilbert-4f41ee18ecb2)\n",
    "- [Fastai integration with BERT: Multi-label text classification identifying toxicity in texts](https://medium.com/@abhikjha/fastai-integration-with-bert-a0a66b1cecbe)\n",
    "- [fastinference](https://muellerzr.github.io/fastinference/)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
