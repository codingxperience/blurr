{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text: Core\n",
    "\n",
    "> The `text.core` module contains the core bits required to use fast.ai's low-level and/or mid-level APIs to define `Datasets`, build `DataLoaders`, and train text/NLP models with fast.ai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |default_exp text.core\n",
    "# |default_cls_lvl 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "from __future__ import annotations\n",
    "\n",
    "import gc, importlib, sys, traceback\n",
    "\n",
    "from accelerate.logging import get_logger\n",
    "from dataclasses import dataclass\n",
    "from datasets import concatenate_datasets, load_dataset\n",
    "from dotenv import load_dotenv\n",
    "from fastai.callback.all import *\n",
    "from fastai.imports import *\n",
    "from fastai.learner import *\n",
    "from fastai.losses import BaseLoss, BCEWithLogitsLossFlat, CrossEntropyLossFlat\n",
    "from fastai.data.transforms import DataLoaders, Datasets, ColSplitter, ItemTransform, TfmdDL\n",
    "from fastai.optimizer import Adam, OptimWrapper, params\n",
    "from fastai.metrics import accuracy, F1Score, accuracy_multi, F1ScoreMulti\n",
    "from fastai.test_utils import show_install\n",
    "from fastai.torch_core import *\n",
    "from fastai.torch_imports import *\n",
    "from transformers import AutoConfig, AutoTokenizer, PretrainedConfig, PreTrainedTokenizerBase, PreTrainedModel\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers import logging as hf_logging\n",
    "from transformers.data.data_collator import DataCollatorWithPadding\n",
    "\n",
    "from blurr.utils import clean_memory, get_hf_objects, set_seed, PreCalculatedLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |hide\n",
    "import pdb\n",
    "\n",
    "from datasets import Value\n",
    "from fastai.data.block import CategoryBlock, ColReader, ColSplitter, DataBlock, ItemGetter, RandomSplitter, FuncSplitter\n",
    "from fastai.text.data import SortedDL\n",
    "from fastcore.test import *\n",
    "import nbdev\n",
    "\n",
    "from blurr.utils import print_versions, PreCalculatedCrossEntropyLoss, PreCalculatedBCELoss, PreCalculatedMSELoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# |export\n",
    "# silence all the HF warnings and load environment variables\n",
    "warnings.simplefilter(\"ignore\")\n",
    "hf_logging.set_verbosity_error()\n",
    "logger = get_logger(__name__)\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU #0: NVIDIA GeForce RTX 3090\n"
     ]
    }
   ],
   "source": [
    "# |hide\n",
    "# |notest\n",
    "torch.cuda.set_device(0)\n",
    "print(f\"Using GPU #{torch.cuda.current_device()}: {torch.cuda.get_device_name()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What we're running with at the time this documentation was generated:\n",
      "torch: 1.13.1\n",
      "fastai: 2.7.11\n",
      "transformers: 4.26.1\n"
     ]
    }
   ],
   "source": [
    "# | echo: false\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "print(\"What we're running with at the time this documentation was generated:\")\n",
    "print_versions(\"torch fastai transformers\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "We'll use a subset of `imdb` to demonstrate how to configure your BLURR for sequence classification tasks. **BLURR** is designed to work with Hugging Face `Dataset` and/or pandas `DataFrame` objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_new_hf_objects(labels):\n",
    "    model_cls = AutoModelForSequenceClassification\n",
    "    hf_logging.set_verbosity_error()\n",
    "\n",
    "    pretrained_model_name = \"microsoft/deberta-v3-small\"\n",
    "    n_labels = len(labels)\n",
    "\n",
    "    hf_arch, hf_config, hf_tokenizer, hf_model = get_hf_objects(\n",
    "        pretrained_model_name, model_cls=model_cls, config_kwargs={\"num_labels\": n_labels}\n",
    "    )\n",
    "\n",
    "    hf_arch, type(hf_config), type(hf_tokenizer), type(hf_model)\n",
    "\n",
    "    print(\"=== config ===\")\n",
    "    print(f\"# of labels:\\t{hf_config.num_labels}\")\n",
    "    print(\"\")\n",
    "    print(\"=== tokenizer ===\")\n",
    "    print(f\"Vocab size:\\t\\t{hf_tokenizer.vocab_size}\")\n",
    "    print(f\"Max # of tokens:\\t{hf_tokenizer.model_max_length}\")\n",
    "    print(f\"Attributes expected by model in forward pass:\\t{hf_tokenizer.model_input_names}\")\n",
    "\n",
    "    return hf_arch, hf_config, hf_tokenizer, hf_model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset imdb (/home/wgilliam/.cache/huggingface/datasets/imdb/plain_text/1.0.0/d613c88cf8fa3bab83b4ded3713f1f74830d1100e171db75bbddb80b3345c9c0)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89c0d610871d4a20a37755d1377bd8a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 200\n",
      "1000 200\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>is_valid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>At the end of the film I just asked myself :\"is it the worse movie I have ever seen or is it the worse movie I have ever seen ?\". And the answer is... Actually, after having seen this movie and thought a bit about the meaning of it, you just can't find any meaning and you can only remember the two rape scenes, which are unbelievably brutal and useless. It seems to me as if the director tried to push this question into the crowd's head : \"what are such crimes compared to horror of war and extermination ?\" because i noticed that the two awful scenes where directly connected to war and it's h...</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What i like about you is one of those series you need to see but aren't sure you would see, the beginning is cool and its sucks you into the series just for fun, the second part i season 2-3 which are more stale, they come and go in what you want. what happens with many series is that they don't end with something special because the second part of the series always goes down into the drain, this one also somewhat did, the third part is the one to spoil and ruin the whole series, usually, but it doesn't, this ending is perfect for the series, it fits perfect, actually i was pretty angry ab...</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Mulva is put in a sugercoma by Teen Ape. When she awakens she's considerably hotter (the parts played by Debbie Rochon in this sequel), and out for revenge on those that did her wrong. As the sub-title and box art implies this is indeed a take off on the \"Kill Bill\" films, but this being a Chris Seaver's film, it's a wildly incompetent satire (and I use that last term extremely loosely) I'd love to say this is better than the first film, but truth be told I was so impossibly drunk off my ass when I saw the previous film that I can't even hope to compare the two at this point in time. But I...</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A funny comedy from beginning to end! There are several hilarious scenes but it's also loaded with many subtle comedic moments which is what made the movie for me. Creative story line with a very talented cast. I thoroughly enjoyed it!</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I won't mention any of the plot, because, although it would be highly predictable anyway, the one notable plot twist is given away everywhere, in the movie comments, in the plot summary here, and even in the synopsis on my Netflix envelope. I might have enjoyed it more if I hadn't known that. Maybe. This film has a deceptively good cast, most of whom did creditable acting with the rather limited material at hand, including Donald Sutherland, Lesley Ann Warren, and Tia Carrere and Rosemary Dunsmore in smaller parts. It was impossible to like William McNamara, but that was clearly by design....</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      text  \\\n",
       "0  At the end of the film I just asked myself :\"is it the worse movie I have ever seen or is it the worse movie I have ever seen ?\". And the answer is... Actually, after having seen this movie and thought a bit about the meaning of it, you just can't find any meaning and you can only remember the two rape scenes, which are unbelievably brutal and useless. It seems to me as if the director tried to push this question into the crowd's head : \"what are such crimes compared to horror of war and extermination ?\" because i noticed that the two awful scenes where directly connected to war and it's h...   \n",
       "1  What i like about you is one of those series you need to see but aren't sure you would see, the beginning is cool and its sucks you into the series just for fun, the second part i season 2-3 which are more stale, they come and go in what you want. what happens with many series is that they don't end with something special because the second part of the series always goes down into the drain, this one also somewhat did, the third part is the one to spoil and ruin the whole series, usually, but it doesn't, this ending is perfect for the series, it fits perfect, actually i was pretty angry ab...   \n",
       "2  Mulva is put in a sugercoma by Teen Ape. When she awakens she's considerably hotter (the parts played by Debbie Rochon in this sequel), and out for revenge on those that did her wrong. As the sub-title and box art implies this is indeed a take off on the \"Kill Bill\" films, but this being a Chris Seaver's film, it's a wildly incompetent satire (and I use that last term extremely loosely) I'd love to say this is better than the first film, but truth be told I was so impossibly drunk off my ass when I saw the previous film that I can't even hope to compare the two at this point in time. But I...   \n",
       "3                                                                                                                                                                                                                                                                                                                                                                              A funny comedy from beginning to end! There are several hilarious scenes but it's also loaded with many subtle comedic moments which is what made the movie for me. Creative story line with a very talented cast. I thoroughly enjoyed it!   \n",
       "4  I won't mention any of the plot, because, although it would be highly predictable anyway, the one notable plot twist is given away everywhere, in the movie comments, in the plot summary here, and even in the synopsis on my Netflix envelope. I might have enjoyed it more if I hadn't known that. Maybe. This film has a deceptively good cast, most of whom did creditable acting with the rather limited material at hand, including Donald Sutherland, Lesley Ann Warren, and Tia Carrere and Rosemary Dunsmore in smaller parts. It was impossible to like William McNamara, but that was clearly by design....   \n",
       "\n",
       "   label  is_valid  \n",
       "0      0     False  \n",
       "1      1     False  \n",
       "2      0     False  \n",
       "3      1     False  \n",
       "4      0     False  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb_dsd = load_dataset(\"imdb\", split=[\"train\", \"test\"])\n",
    "\n",
    "# build HF `Dataset` objects\n",
    "train_ds = imdb_dsd[0].add_column(\"is_valid\", [False] * len(imdb_dsd[0])).shuffle().select(range(1000))\n",
    "valid_ds = imdb_dsd[1].add_column(\"is_valid\", [True] * len(imdb_dsd[1])).shuffle().select(range(200))\n",
    "imdb_ds = concatenate_datasets([train_ds, valid_ds])\n",
    "\n",
    "# build a `DataFrame` representation as well\n",
    "imdb_df = pd.DataFrame(imdb_ds)\n",
    "\n",
    "print(len(train_ds), len(valid_ds))\n",
    "print(len(imdb_df[imdb_df[\"is_valid\"] == False]), len(imdb_df[imdb_df[\"is_valid\"] == True]))\n",
    "imdb_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['neg', 'pos']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = imdb_dsd[0].features[\"label\"].names\n",
    "labels"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multilabel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset civil_comments (/home/wgilliam/.cache/huggingface/datasets/civil_comments/default/0.9.0/e7a3aacd2ab7d135fa958e7209d10b1fa03807d44c486e3c34897aa08ea8ffab)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2b238bd9d9d4f009d9bf92dcd90a1f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc1d1a797c5442f5a6b78bdbeea7c239",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc8709e4009d43d89b7a1e9ac52a85c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Casting the dataset:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d20058746e4e47feabc64822b2fc5bb8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9009e96be30a4e84bed99ae1854e0755",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Casting the dataset:   0%|          | 0/200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 200\n",
      "1000 200\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>toxicity</th>\n",
       "      <th>severe_toxicity</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_attack</th>\n",
       "      <th>sexual_explicit</th>\n",
       "      <th>is_valid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The next four years will be the biggest pay to play operation this country has ever seen. I predict a multi-billion dollar enterprise by the time Trump is stopped.</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Fact: Ontario (and most of Canada) is a COLD province/country. We need heat to survive. I don't heat my house above 18 degrees Celsius (I am way too hot all the time) and i do not live in the GTA with it's abundance of public transport. Therefore, as a taxpayer i must drive to my job (no public transport where i am). I do drive a very fuel efficient car and do not waste heat or gas. I completely disagree with the Cap and Trade system. It is sheer lunacy for punishing Ontarians for living in a cold climate.</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Cage match…winner take all.</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ha! I would dearly love to see that column!</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>This from the friendly and liberal-loved New York Times.\\n\\nhttp://www.nytimes.com/2016/08/21/us/politics/hillary-clinton-presidential-campaign-charity.html?_r=0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              text  \\\n",
       "0                                                                                                                                                                                                                                                                                                                                                              The next four years will be the biggest pay to play operation this country has ever seen. I predict a multi-billion dollar enterprise by the time Trump is stopped.   \n",
       "1  Fact: Ontario (and most of Canada) is a COLD province/country. We need heat to survive. I don't heat my house above 18 degrees Celsius (I am way too hot all the time) and i do not live in the GTA with it's abundance of public transport. Therefore, as a taxpayer i must drive to my job (no public transport where i am). I do drive a very fuel efficient car and do not waste heat or gas. I completely disagree with the Cap and Trade system. It is sheer lunacy for punishing Ontarians for living in a cold climate.   \n",
       "2                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      Cage match…winner take all.   \n",
       "3                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      Ha! I would dearly love to see that column!   \n",
       "4                                                                                                                                                                                                                                                                                                                                                                This from the friendly and liberal-loved New York Times.\\n\\nhttp://www.nytimes.com/2016/08/21/us/politics/hillary-clinton-presidential-campaign-charity.html?_r=0   \n",
       "\n",
       "   toxicity  severe_toxicity  obscene  threat  insult  identity_attack  \\\n",
       "0         0                0        0       0       0                0   \n",
       "1         0                0        0       0       0                0   \n",
       "2         0                0        0       0       0                0   \n",
       "3         0                0        0       0       0                0   \n",
       "4         0                0        0       0       0                0   \n",
       "\n",
       "   sexual_explicit  is_valid  \n",
       "0                0     False  \n",
       "1                0     False  \n",
       "2                0     False  \n",
       "3                0     False  \n",
       "4                0     False  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "civil_dsd = load_dataset(\"civil_comments\", split=[\"train\", \"validation\"])\n",
    "\n",
    "# round the floats\n",
    "civil_labels = [\"toxicity\", \"severe_toxicity\", \"obscene\", \"threat\", \"insult\", \"identity_attack\", \"sexual_explicit\"]\n",
    "\n",
    "\n",
    "def round_targs(example):\n",
    "    for lbl in civil_labels:\n",
    "        example[lbl] = np.round(example[lbl])\n",
    "    return example\n",
    "\n",
    "\n",
    "# convert floats to ints\n",
    "def fix_dtypes(ds):\n",
    "    new_features = ds.features.copy()\n",
    "    for lbl in civil_labels:\n",
    "        new_features[lbl] = Value(\"int32\")\n",
    "    return ds.cast(new_features)\n",
    "\n",
    "\n",
    "# build HF `Dataset` objects\n",
    "civil_train_ds = civil_dsd[0].add_column(\"is_valid\", [False] * len(civil_dsd[0])).shuffle().select(range(1000))\n",
    "civil_train_ds = civil_train_ds.map(round_targs)\n",
    "civil_train_ds = fix_dtypes(civil_train_ds)\n",
    "\n",
    "civil_valid_ds = civil_dsd[1].add_column(\"is_valid\", [True] * len(civil_dsd[1])).shuffle().select(range(200))\n",
    "civil_valid_ds = civil_valid_ds.map(round_targs)\n",
    "civil_valid_ds = fix_dtypes(civil_valid_ds)\n",
    "\n",
    "civil_ds = concatenate_datasets([civil_train_ds, civil_valid_ds])\n",
    "\n",
    "# build a `DataFrame` representation as well\n",
    "civil_df = pd.DataFrame(civil_ds)\n",
    "\n",
    "print(len(civil_train_ds), len(civil_valid_ds))\n",
    "print(len(civil_df[civil_df[\"is_valid\"] == False]), len(civil_df[civil_df[\"is_valid\"] == True]))\n",
    "civil_df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Core: Data\n",
    "\n",
    "A base collation function that works with a variety of input formats and pads inputs on-the-fly at batch time"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `TextCollatorWithPadding` -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "@dataclass\n",
    "class TextCollatorWithPadding:\n",
    "    def __init__(\n",
    "        self,\n",
    "        # A Hugging Face tokenizer\n",
    "        hf_tokenizer: PreTrainedTokenizerBase,\n",
    "        # The abbreviation/name of your Hugging Face transformer architecture (e.b., bert, bart, etc..)\n",
    "        hf_arch: str = None,\n",
    "        # A specific configuration instance you want to use\n",
    "        hf_config: PretrainedConfig = None,\n",
    "        # A Hugging Face model\n",
    "        hf_model: PreTrainedModel = None,\n",
    "        # The number of inputs expected by your model\n",
    "        n_inp: int = 1,\n",
    "        # Defaults to use Hugging Face's DataCollatorWithPadding(tokenizer=hf_tokenizer)\n",
    "        data_collator_cls: type = DataCollatorWithPadding,\n",
    "        # kwyargs specific for the instantiation of the `data_collator`\n",
    "        data_collator_kwargs: dict = {},\n",
    "    ):\n",
    "        store_attr()\n",
    "        self.hf_tokenizer = data_collator_kwargs.pop(\"tokenizer\", self.hf_tokenizer)\n",
    "        self.data_collator = data_collator_cls(tokenizer=self.hf_tokenizer, **data_collator_kwargs)\n",
    "\n",
    "    def __call__(self, features):\n",
    "        features = L(features)\n",
    "        inputs, labels, targs = [], [], []\n",
    "\n",
    "        # features contain dictionaries\n",
    "        if isinstance(features[0], dict):\n",
    "            feature_keys = list(features[0].keys())\n",
    "            inputs = [self._build_inputs_d(features, feature_keys)]\n",
    "\n",
    "            input_labels = self._build_input_labels(features, feature_keys)\n",
    "            if input_labels is not None:\n",
    "                labels, targs = [input_labels], [input_labels.clone()]\n",
    "        # features contains tuples, each of which can contain multiple inputs and/or targets\n",
    "        elif isinstance(features[0], tuple):\n",
    "            for f_idx in range(self.n_inp):\n",
    "                feature_keys = list(features[0][f_idx].keys())\n",
    "                inputs.append(self._build_inputs_d(features.itemgot(f_idx), feature_keys))\n",
    "\n",
    "                input_labels = self._build_input_labels(features.itemgot(f_idx), feature_keys)\n",
    "                labels.append(input_labels if input_labels is not None else [])\n",
    "\n",
    "            targs = [self._proc_targets(list(features.itemgot(f_idx))) for f_idx in range(self.n_inp, len(features[0]))]\n",
    "\n",
    "        return self._build_batch(inputs, labels, targs)\n",
    "\n",
    "    # ----- utility methods -----\n",
    "\n",
    "    # to build the inputs dictionary\n",
    "    def _build_inputs_d(self, features, feature_keys):\n",
    "        return {fwd_arg: list(features.attrgot(fwd_arg)) for fwd_arg in self.hf_tokenizer.model_input_names if fwd_arg in feature_keys}\n",
    "\n",
    "    # to build the input \"labels\"\n",
    "    def _build_input_labels(self, features, feature_keys):\n",
    "        if \"label\" in feature_keys:\n",
    "            labels = list(features.attrgot(\"label\"))\n",
    "            return self._proc_targets(labels)\n",
    "        return None\n",
    "\n",
    "    # used to give the labels/targets the right shape\n",
    "    def _proc_targets(self, targs):\n",
    "        if is_listy(targs[0]):\n",
    "            targs = torch.stack([tensor(lbls) for lbls in targs])\n",
    "        elif isinstance(targs[0], torch.Tensor) and len(targs[0].size()) > 0:\n",
    "            targs = torch.stack(targs)\n",
    "        else:\n",
    "            targs = torch.tensor(targs)\n",
    "\n",
    "        return targs\n",
    "\n",
    "    # will properly assemble are batch given a list of inputs, labels, and targets\n",
    "    def _build_batch(self, inputs, labels, targs):\n",
    "        batch = []\n",
    "\n",
    "        for input, input_labels in zip(inputs, labels):\n",
    "            input_d = dict(self.data_collator(input))\n",
    "            if len(input_labels) > 0:\n",
    "                input_d[\"labels\"] = input_labels\n",
    "            batch.append(input_d)\n",
    "\n",
    "        for targ in targs:\n",
    "            batch.append(targ)\n",
    "\n",
    "        return batch"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Core: Training\n",
    "\n",
    "Base plitters for defining paramater groups, model wrapper, and model callback"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `blurr_splitter`s -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "def blurr_params(modules: Module | list[Module]):\n",
    "    \"Like fast.ai's `params()` method, this method returns all parameters of `m` but also works with lists of modules\"\n",
    "    if not is_listy(modules):\n",
    "        modules = [modules]\n",
    "    return [p for m in modules for p in m.parameters()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/ohmeow/blurr/blob/dev-3.0.0 #master/blurr/text/core.py#L130){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### blurr_params\n",
       "\n",
       ">      blurr_params\n",
       ">                    (modules:fastai.torch_core.Module|list[fastai.torch_core.Mo\n",
       ">                    dule])\n",
       "\n",
       "Like fast.ai's `params()` method, this method returns all parameters of `m` but also works with lists of modules"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/ohmeow/blurr/blob/dev-3.0.0 #master/blurr/text/core.py#L130){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### blurr_params\n",
       "\n",
       ">      blurr_params\n",
       ">                    (modules:fastai.torch_core.Module|list[fastai.torch_core.Mo\n",
       ">                    dule])\n",
       "\n",
       "Like fast.ai's `params()` method, this method returns all parameters of `m` but also works with lists of modules"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nbdev.show_doc(blurr_params, title_level=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "def blurr_splitter(m: Module):\n",
    "    \"\"\"Splits the Hugging Face model based on various model architecture conventions\"\"\"\n",
    "    model = m.hf_model if (hasattr(m, \"hf_model\")) else m\n",
    "    root_modules = list(model.named_children())\n",
    "    top_module_name, top_module = root_modules[0]\n",
    "\n",
    "    groups = L([m for m_name, m in list(top_module.named_children())])\n",
    "    groups += L([m for m_name, m in root_modules[1:]])\n",
    "\n",
    "    return groups.map(params).filter(lambda el: len(el) > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/ohmeow/blurr/blob/dev-3.0.0 #master/blurr/text/core.py#L137){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### blurr_splitter\n",
       "\n",
       ">      blurr_splitter (m:fastai.torch_core.Module)\n",
       "\n",
       "Splits the Hugging Face model based on various model architecture conventions"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/ohmeow/blurr/blob/dev-3.0.0 #master/blurr/text/core.py#L137){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### blurr_splitter\n",
       "\n",
       ">      blurr_splitter (m:fastai.torch_core.Module)\n",
       "\n",
       "Splits the Hugging Face model based on various model architecture conventions"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nbdev.show_doc(blurr_splitter, title_level=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def blurr_splitter_on_head(m: Module):\n",
    "    \"\"\"Creates two layers groups: One for the backbone and one for the pooler/classification head\"\"\"\n",
    "    model = m.hf_model if (hasattr(m, \"hf_model\")) else m\n",
    "    root_modules = list(model.named_children())\n",
    "    backbone_module_name, backbone_module = root_modules[0]\n",
    "\n",
    "    groups = L(backbone_module)\n",
    "    groups.append(L([m for m_name, m in root_modules[1:]]))\n",
    "\n",
    "    return groups.map(blurr_params).filter(lambda el: len(el) > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "### blurr_splitter_on_head\n",
       "\n",
       ">      blurr_splitter_on_head (m:fastai.torch_core.Module)\n",
       "\n",
       "Creates two layers groups: One for the backbone and one for the pooler/classification head"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "### blurr_splitter_on_head\n",
       "\n",
       ">      blurr_splitter_on_head (m:fastai.torch_core.Module)\n",
       "\n",
       "Creates two layers groups: One for the backbone and one for the pooler/classification head"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nbdev.show_doc(blurr_splitter_on_head, title_level=3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `BaseModelWrapper` -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "class BaseModelWrapper(Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        # Your Hugging Face model\n",
    "        hf_model: PreTrainedModel,\n",
    "        # If True, hidden_states will be returned and accessed from Learner\n",
    "        output_hidden_states: bool = False,\n",
    "        # If True, attentions will be returned and accessed from Learner\n",
    "        output_attentions: bool = False,\n",
    "        # Any additional keyword arguments you want passed into your models forward method\n",
    "        hf_model_kwargs={},\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        store_attr()\n",
    "        self.hf_model = hf_model.cuda() if torch.cuda.is_available() else hf_model\n",
    "        self.hf_model_fwd_args = list(inspect.signature(self.hf_model.forward).parameters.keys())\n",
    "\n",
    "    def forward(self, x):\n",
    "        for k in list(x):\n",
    "            if k not in self.hf_model_fwd_args:\n",
    "                del x[k]\n",
    "\n",
    "        return self.hf_model(\n",
    "            **x,\n",
    "            output_hidden_states=self.output_hidden_states,\n",
    "            output_attentions=self.output_attentions,\n",
    "            return_dict=True,\n",
    "            **self.hf_model_kwargs,\n",
    "        )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that `BaseModelWrapper` includes some nifty code for just passing in the things your model needs, as not all transformer architectures require/use the same information."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `BaseModelCallback` -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "class BaseModelCallback(Callback):\n",
    "    def __init__(\n",
    "        self,\n",
    "        # Additional keyword arguments passed to `BaseModelWrapper`\n",
    "        base_model_wrapper_kwargs: dict = {},\n",
    "    ):\n",
    "        self.base_model_wrapper_kwargs = base_model_wrapper_kwargs\n",
    "\n",
    "    def after_create(self):\n",
    "        if isinstance(self.learn.model, PreTrainedModel):\n",
    "            self.learn.model = BaseModelWrapper(self.learn.model, **self.base_model_wrapper_kwargs)\n",
    "\n",
    "    def before_batch(self):\n",
    "        self.hf_loss = None\n",
    "\n",
    "    def after_pred(self):\n",
    "        model_outputs = self.pred\n",
    "        self.learn.blurr_model_outputs = {}\n",
    "\n",
    "        for k, v in model_outputs.items():\n",
    "            # if the \"labels\" are included, we are training with target labels in which case the loss is returned\n",
    "            if k == \"loss\" and isinstance(self.learn.loss_func, PreCalculatedLoss):\n",
    "                self.hf_loss = to_float(v)\n",
    "            # the logits represent the prediction\n",
    "            elif k == \"logits\":\n",
    "                self.learn.pred = v\n",
    "            # add any other things included in model_outputs as blurr_{model_output_key}\n",
    "            else:\n",
    "                self.learn.blurr_model_outputs[k] = v\n",
    "\n",
    "    def after_loss(self):\n",
    "        # if we already have the loss from the model, update the Learner's loss to be it\n",
    "        if self.hf_loss is not None:\n",
    "            self.learn.loss_grad = self.hf_loss\n",
    "            self.learn.loss = self.learn.loss_grad.clone()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use a `Callback` for handling the [`ModelOutput`](https://huggingface.co/transformers/main_classes/output.html#transformers.file_utils.ModelOutput) returned by Hugging Face transformers. It allows us to associate anything we want from that object to our `Learner`.\n",
    "\n",
    "**Note** that your `Learner`'s loss will be set for you only if the Hugging Face model returns one *and* you are using the `PreCalculatedLoss` loss function.  \n",
    "\n",
    "Also note that anything else you asked the model to return (for example, last hidden state, etc..) will be available for you via the `blurr_model_outputs` property attached to your `Learner`. For example, assuming you are using BERT for a classification task ... if you have told your `BaseModelWrapper` instance to return attentions, you'd be able to access them via `learn.blurr_model_outputs['attentions']`."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Core: Examples\n",
    "\n",
    "This section demonstrates how you can use standard `Dataset` objects (PyTorch and Hugging Face) to build PyTorch `DataLoader` iterators and train your model using the fast.ai `Learner`. \n",
    "\n",
    "**Note** that most fast.ai specific features such as `DataLoaders.one_batch`, `Learner.show_summary`, `DataLoader.show_batch`, `Learner.show_results` are not available when using PyTorch."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multiclass"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 1: HF objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== config ===\n",
      "# of labels:\t2\n",
      "\n",
      "=== tokenizer ===\n",
      "Vocab size:\t\t128000\n",
      "Max # of tokens:\t1000000000000000019884624838656\n",
      "Attributes expected by model in forward pass:\t['input_ids', 'token_type_ids', 'attention_mask']\n"
     ]
    }
   ],
   "source": [
    "hf_arch, hf_config, hf_tokenizer, hf_model = get_new_hf_objects(labels)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 2: `torch.utils.data.Dataset`s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train|Validation examples:  1000 200\n",
      "{'text': ['At the end of the film I just asked myself :\"is it the worse movie I have ever seen or is it the worse movie I have ever seen ?\". And the answer is... Actually, after having seen this movie and thought a bit about the meaning of it, you just can\\'t find any meaning and you can only remember the two rape scenes, which are unbelievably brutal and useless. It seems to me as if the director tried to push this question into the crowd\\'s head : \"what are such crimes compared to horror of war and extermination ?\" because i noticed that the two awful scenes where directly connected to war and it\\'s horrors (during the first scene you can here the girl that is being raped screaming and in the same time you hear one of president Bush\\'s speeches about the necessity of starting a war with Iraq and in the second scene, the pictures of the three criminals sticking a sword in a woman\\'s vagina, are directly followed by archive pictures of World war II. But as a matter of facts, i really could not think about the relative gravity of theses two different kinds of human horror\\'s expression, being done i was too shocked by what i had just seen and felt. (sorry for bad English)', \"What i like about you is one of those series you need to see but aren't sure you would see, the beginning is cool and its sucks you into the series just for fun, the second part i season 2-3 which are more stale, they come and go in what you want. what happens with many series is that they don't end with something special because the second part of the series always goes down into the drain, this one also somewhat did, the third part is the one to spoil and ruin the whole series, usually, but it doesn't, this ending is perfect for the series, it fits perfect, actually i was pretty angry about all these guys in Val's life, actually i wanted to end with Jeff in the end, but later on it changed, they chose to take Vic into the series after almost 3 seasons without him, and that was the biggest surprise and also what made the series go on top.<br /><br />see it many times the series is actually very cool just don't expect the second part to be that good it isn't but the third part does what was needed and made the series to one that was worth the whole thing, i am happy to say i was glad and happy about the series and now i will go over to see two guys a girl and a pizza place, when i have seen the whole series i will be back...\"], 'label': [0, 1], 'is_valid': [False, False]}\n",
      "\n",
      "['At the end of the film I just asked myself :\"is it the worse movie I have ever seen or is it the worse movie I have ever seen ?\". And the answer is... Actually, after having seen this movie and thought a bit about the meaning of it, you just can\\'t find any meaning and you can only remember the two rape scenes, which are unbelievably brutal and useless. It seems to me as if the director tried to push this question into the crowd\\'s head : \"what are such crimes compared to horror of war and extermination ?\" because i noticed that the two awful scenes where directly connected to war and it\\'s horrors (during the first scene you can here the girl that is being raped screaming and in the same time you hear one of president Bush\\'s speeches about the necessity of starting a war with Iraq and in the second scene, the pictures of the three criminals sticking a sword in a woman\\'s vagina, are directly followed by archive pictures of World war II. But as a matter of facts, i really could not think about the relative gravity of theses two different kinds of human horror\\'s expression, being done i was too shocked by what i had just seen and felt. (sorry for bad English)', \"What i like about you is one of those series you need to see but aren't sure you would see, the beginning is cool and its sucks you into the series just for fun, the second part i season 2-3 which are more stale, they come and go in what you want. what happens with many series is that they don't end with something special because the second part of the series always goes down into the drain, this one also somewhat did, the third part is the one to spoil and ruin the whole series, usually, but it doesn't, this ending is perfect for the series, it fits perfect, actually i was pretty angry about all these guys in Val's life, actually i wanted to end with Jeff in the end, but later on it changed, they chose to take Vic into the series after almost 3 seasons without him, and that was the biggest surprise and also what made the series go on top.<br /><br />see it many times the series is actually very cool just don't expect the second part to be that good it isn't but the third part does what was needed and made the series to one that was worth the whole thing, i am happy to say i was glad and happy about the series and now i will go over to see two guys a girl and a pizza place, when i have seen the whole series i will be back...\"]\n"
     ]
    }
   ],
   "source": [
    "print(\"Train|Validation examples: \", len(train_ds), len(valid_ds))\n",
    "\n",
    "print(train_ds[:2])\n",
    "print(\"\")\n",
    "print(train_ds[\"text\"][:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7369407d5ca42489b27a9025d482882",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32c6ee0515f84fe9820263b97f647c0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# tokenize the dataset\n",
    "def tokenize_func(example):\n",
    "    return hf_tokenizer(example[\"text\"], truncation=True)\n",
    "\n",
    "\n",
    "proc_train_ds = train_ds.map(tokenize_func, batched=True)\n",
    "proc_train_ds.set_format(\"torch\", columns=hf_tokenizer.model_input_names + [\"label\"])\n",
    "\n",
    "proc_valid_ds = valid_ds.map(tokenize_func, batched=True)\n",
    "proc_valid_ds.set_format(\"torch\", columns=hf_tokenizer.model_input_names + [\"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define our PyTorch Dataset class\n",
    "class HFTextClassificationDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, hf_dataset, hf_tokenizer):\n",
    "        self.hf_dataset = hf_dataset\n",
    "        self.hf_tokenizer = hf_tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.hf_dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.hf_dataset[idx]\n",
    "        return item\n",
    "\n",
    "\n",
    "# build our PyTorch training and validation Datasets\n",
    "pt_proc_train_ds = HFTextClassificationDataset(proc_train_ds, hf_tokenizer=hf_tokenizer)\n",
    "pt_proc_valid_ds = HFTextClassificationDataset(proc_valid_ds, hf_tokenizer=hf_tokenizer)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 3: `DataLoaders`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build your fastai `DataLoaders` from Pytorch `DataLoader` objects\n",
    "batch_size = 4\n",
    "data_collator = TextCollatorWithPadding(hf_tokenizer)\n",
    "train_dl = torch.utils.data.DataLoader(pt_proc_train_ds, batch_size=batch_size, shuffle=True, collate_fn=data_collator)\n",
    "valid_dl = torch.utils.data.DataLoader(pt_proc_valid_ds, batch_size=batch_size * 2, shuffle=False, collate_fn=data_collator)\n",
    "\n",
    "dls = DataLoaders(train_dl, valid_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250 25\n",
      "2\n",
      "\n",
      "[CLS] Although I'm grateful this obscure gem of 70's Italian exploitation cinema features in the recently released \"Grindhouse Experience\" box set, and although it's also available on disc under the misleading and stupid alternate title \"Escape from Death Row\", I honestly think it deserves a proper and luxurious DVD edition, completely in its originally spoken languages with subtitle options (the dubbing is truly horrible), restored picture quality and a truckload of special bonus features! Heck, I don't even need the restored picture quality and bonus features if only we could watch the film in its original language. \"Mean Frank and Crazy Tony\" is a cheerfully fast-paced mafia/crime flick with a lot of violence, comedy (which, admittedly, doesn't always work), feminine beauty and two witty main characters. Tony Lo Bianco is terrific as the small thug pretending to be the city's biggest Don. When the real crime lord Frankie Dio (Lee Van Cleef) arrives in town, he\n",
      "\n",
      "tensor([1, 1, 0, 1])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[({'input_ids': tensor([[   1, 1589,  273,  ...,    0,    0,    0],\n",
       "           [   1,  790,  261,  ...,    0,    0,    0],\n",
       "           [   1,  643, 1077,  ...,    0,    0,    0],\n",
       "           [   1,  329,  284,  ..., 1012,  260,    2]]),\n",
       "   'token_type_ids': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
       "           [0, 0, 0,  ..., 0, 0, 0],\n",
       "           [0, 0, 0,  ..., 0, 0, 0],\n",
       "           [0, 0, 0,  ..., 0, 0, 0]]),\n",
       "   'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "           [1, 1, 1,  ..., 0, 0, 0],\n",
       "           [1, 1, 1,  ..., 0, 0, 0],\n",
       "           [1, 1, 1,  ..., 1, 1, 1]]),\n",
       "   'labels': tensor([1, 1, 0, 1])},),\n",
       " tensor([1, 1, 0, 1])]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(train_dl), len(valid_dl))\n",
    "\n",
    "b = next(iter(train_dl))\n",
    "print(len(b))\n",
    "print(\"\")\n",
    "print(hf_tokenizer.decode(b[0][\"input_ids\"][0][:200]))\n",
    "print(\"\")\n",
    "print(b[1])\n",
    "\n",
    "# b"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 4: Learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed()\n",
    "\n",
    "model = BaseModelWrapper(hf_model)\n",
    "\n",
    "learn = Learner(\n",
    "    dls,\n",
    "    model,\n",
    "    opt_func=partial(OptimWrapper, opt=torch.optim.Adam),\n",
    "    loss_func=PreCalculatedCrossEntropyLoss(),  # CrossEntropyLossFlat(),\n",
    "    metrics=[accuracy],\n",
    "    cbs=[BaseModelCallback],\n",
    "    splitter=blurr_splitter_on_head,\n",
    ")\n",
    "\n",
    "learn.create_opt()\n",
    "learn.freeze()\n",
    "\n",
    "learn = learn.to_fp16()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "print(len(learn.opt.param_groups))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOPE: Won't work with PyTorch DataLoaders\n",
    "# AttributeError: 'DataLoader' object has no attribute 'show_batch'\n",
    "# learn.dls.valid.show_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOPE: Won't work with PyTorch DataLoaders\n",
    "# AttributeError: 'DataLoader' object has no attribute 'one_batch'\n",
    "# learn.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 5: Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      <progress value='0' class='' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      0.00% [0/1 00:00&lt;?]\n",
       "    </div>\n",
       "    \n",
       "\n",
       "\n",
       "    <div>\n",
       "      <progress value='0' class='' max='250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      0.00% [0/250 00:00&lt;?]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "TypeError",
     "evalue": "'tuple' object does not support item deletion",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m learn\u001b[39m.\u001b[39munfreeze()\n\u001b[0;32m----> 2\u001b[0m learn\u001b[39m.\u001b[39;49mlr_find(suggest_funcs\u001b[39m=\u001b[39;49m[minimum, steep, valley, slide])\n",
      "File \u001b[0;32m~/mambaforge/envs/blurr-v3/lib/python3.10/site-packages/fastai/callback/schedule.py:293\u001b[0m, in \u001b[0;36mlr_find\u001b[0;34m(self, start_lr, end_lr, num_it, stop_div, show_plot, suggest_funcs)\u001b[0m\n\u001b[1;32m    291\u001b[0m n_epoch \u001b[39m=\u001b[39m num_it\u001b[39m/\u001b[39m\u001b[39m/\u001b[39m\u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdls\u001b[39m.\u001b[39mtrain) \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    292\u001b[0m cb\u001b[39m=\u001b[39mLRFinder(start_lr\u001b[39m=\u001b[39mstart_lr, end_lr\u001b[39m=\u001b[39mend_lr, num_it\u001b[39m=\u001b[39mnum_it, stop_div\u001b[39m=\u001b[39mstop_div)\n\u001b[0;32m--> 293\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mno_logging(): \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfit(n_epoch, cbs\u001b[39m=\u001b[39;49mcb)\n\u001b[1;32m    294\u001b[0m \u001b[39mif\u001b[39;00m suggest_funcs \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    295\u001b[0m     lrs, losses \u001b[39m=\u001b[39m tensor(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrecorder\u001b[39m.\u001b[39mlrs[num_it\u001b[39m/\u001b[39m\u001b[39m/\u001b[39m\u001b[39m10\u001b[39m:\u001b[39m-\u001b[39m\u001b[39m5\u001b[39m]), tensor(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrecorder\u001b[39m.\u001b[39mlosses[num_it\u001b[39m/\u001b[39m\u001b[39m/\u001b[39m\u001b[39m10\u001b[39m:\u001b[39m-\u001b[39m\u001b[39m5\u001b[39m])\n",
      "File \u001b[0;32m~/mambaforge/envs/blurr-v3/lib/python3.10/site-packages/fastai/learner.py:264\u001b[0m, in \u001b[0;36mLearner.fit\u001b[0;34m(self, n_epoch, lr, wd, cbs, reset_opt, start_epoch)\u001b[0m\n\u001b[1;32m    262\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mopt\u001b[39m.\u001b[39mset_hypers(lr\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlr \u001b[39mif\u001b[39;00m lr \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m lr)\n\u001b[1;32m    263\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_epoch \u001b[39m=\u001b[39m n_epoch\n\u001b[0;32m--> 264\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_with_events(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_do_fit, \u001b[39m'\u001b[39;49m\u001b[39mfit\u001b[39;49m\u001b[39m'\u001b[39;49m, CancelFitException, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_end_cleanup)\n",
      "File \u001b[0;32m~/mambaforge/envs/blurr-v3/lib/python3.10/site-packages/fastai/learner.py:199\u001b[0m, in \u001b[0;36mLearner._with_events\u001b[0;34m(self, f, event_type, ex, final)\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_with_events\u001b[39m(\u001b[39mself\u001b[39m, f, event_type, ex, final\u001b[39m=\u001b[39mnoop):\n\u001b[0;32m--> 199\u001b[0m     \u001b[39mtry\u001b[39;00m: \u001b[39mself\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mbefore_\u001b[39m\u001b[39m{\u001b[39;00mevent_type\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m);  f()\n\u001b[1;32m    200\u001b[0m     \u001b[39mexcept\u001b[39;00m ex: \u001b[39mself\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mafter_cancel_\u001b[39m\u001b[39m{\u001b[39;00mevent_type\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[1;32m    201\u001b[0m     \u001b[39mself\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mafter_\u001b[39m\u001b[39m{\u001b[39;00mevent_type\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m);  final()\n",
      "File \u001b[0;32m~/mambaforge/envs/blurr-v3/lib/python3.10/site-packages/fastai/learner.py:253\u001b[0m, in \u001b[0;36mLearner._do_fit\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    251\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_epoch):\n\u001b[1;32m    252\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mepoch\u001b[39m=\u001b[39mepoch\n\u001b[0;32m--> 253\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_with_events(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_do_epoch, \u001b[39m'\u001b[39;49m\u001b[39mepoch\u001b[39;49m\u001b[39m'\u001b[39;49m, CancelEpochException)\n",
      "File \u001b[0;32m~/mambaforge/envs/blurr-v3/lib/python3.10/site-packages/fastai/learner.py:199\u001b[0m, in \u001b[0;36mLearner._with_events\u001b[0;34m(self, f, event_type, ex, final)\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_with_events\u001b[39m(\u001b[39mself\u001b[39m, f, event_type, ex, final\u001b[39m=\u001b[39mnoop):\n\u001b[0;32m--> 199\u001b[0m     \u001b[39mtry\u001b[39;00m: \u001b[39mself\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mbefore_\u001b[39m\u001b[39m{\u001b[39;00mevent_type\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m);  f()\n\u001b[1;32m    200\u001b[0m     \u001b[39mexcept\u001b[39;00m ex: \u001b[39mself\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mafter_cancel_\u001b[39m\u001b[39m{\u001b[39;00mevent_type\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[1;32m    201\u001b[0m     \u001b[39mself\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mafter_\u001b[39m\u001b[39m{\u001b[39;00mevent_type\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m);  final()\n",
      "File \u001b[0;32m~/mambaforge/envs/blurr-v3/lib/python3.10/site-packages/fastai/learner.py:247\u001b[0m, in \u001b[0;36mLearner._do_epoch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    246\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_do_epoch\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m--> 247\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_do_epoch_train()\n\u001b[1;32m    248\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_do_epoch_validate()\n",
      "File \u001b[0;32m~/mambaforge/envs/blurr-v3/lib/python3.10/site-packages/fastai/learner.py:239\u001b[0m, in \u001b[0;36mLearner._do_epoch_train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_do_epoch_train\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    238\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdl \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdls\u001b[39m.\u001b[39mtrain\n\u001b[0;32m--> 239\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_with_events(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mall_batches, \u001b[39m'\u001b[39;49m\u001b[39mtrain\u001b[39;49m\u001b[39m'\u001b[39;49m, CancelTrainException)\n",
      "File \u001b[0;32m~/mambaforge/envs/blurr-v3/lib/python3.10/site-packages/fastai/learner.py:199\u001b[0m, in \u001b[0;36mLearner._with_events\u001b[0;34m(self, f, event_type, ex, final)\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_with_events\u001b[39m(\u001b[39mself\u001b[39m, f, event_type, ex, final\u001b[39m=\u001b[39mnoop):\n\u001b[0;32m--> 199\u001b[0m     \u001b[39mtry\u001b[39;00m: \u001b[39mself\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mbefore_\u001b[39m\u001b[39m{\u001b[39;00mevent_type\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m);  f()\n\u001b[1;32m    200\u001b[0m     \u001b[39mexcept\u001b[39;00m ex: \u001b[39mself\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mafter_cancel_\u001b[39m\u001b[39m{\u001b[39;00mevent_type\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[1;32m    201\u001b[0m     \u001b[39mself\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mafter_\u001b[39m\u001b[39m{\u001b[39;00mevent_type\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m);  final()\n",
      "File \u001b[0;32m~/mambaforge/envs/blurr-v3/lib/python3.10/site-packages/fastai/learner.py:205\u001b[0m, in \u001b[0;36mLearner.all_batches\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mall_batches\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    204\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_iter \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdl)\n\u001b[0;32m--> 205\u001b[0m     \u001b[39mfor\u001b[39;00m o \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdl): \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mone_batch(\u001b[39m*\u001b[39;49mo)\n",
      "File \u001b[0;32m~/mambaforge/envs/blurr-v3/lib/python3.10/site-packages/fastai/learner.py:235\u001b[0m, in \u001b[0;36mLearner.one_batch\u001b[0;34m(self, i, b)\u001b[0m\n\u001b[1;32m    233\u001b[0m b \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_set_device(b)\n\u001b[1;32m    234\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_split(b)\n\u001b[0;32m--> 235\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_with_events(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_do_one_batch, \u001b[39m'\u001b[39;49m\u001b[39mbatch\u001b[39;49m\u001b[39m'\u001b[39;49m, CancelBatchException)\n",
      "File \u001b[0;32m~/mambaforge/envs/blurr-v3/lib/python3.10/site-packages/fastai/learner.py:199\u001b[0m, in \u001b[0;36mLearner._with_events\u001b[0;34m(self, f, event_type, ex, final)\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_with_events\u001b[39m(\u001b[39mself\u001b[39m, f, event_type, ex, final\u001b[39m=\u001b[39mnoop):\n\u001b[0;32m--> 199\u001b[0m     \u001b[39mtry\u001b[39;00m: \u001b[39mself\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mbefore_\u001b[39m\u001b[39m{\u001b[39;00mevent_type\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m);  f()\n\u001b[1;32m    200\u001b[0m     \u001b[39mexcept\u001b[39;00m ex: \u001b[39mself\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mafter_cancel_\u001b[39m\u001b[39m{\u001b[39;00mevent_type\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[1;32m    201\u001b[0m     \u001b[39mself\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mafter_\u001b[39m\u001b[39m{\u001b[39;00mevent_type\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m);  final()\n",
      "File \u001b[0;32m~/mambaforge/envs/blurr-v3/lib/python3.10/site-packages/fastai/learner.py:216\u001b[0m, in \u001b[0;36mLearner._do_one_batch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_do_one_batch\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m--> 216\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpred \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel(\u001b[39m*\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mxb)\n\u001b[1;32m    217\u001b[0m     \u001b[39mself\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mafter_pred\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m    218\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39myb):\n",
      "File \u001b[0;32m~/mambaforge/envs/blurr-v3/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[19], line 23\u001b[0m, in \u001b[0;36mBaseModelWrapper.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[39mfor\u001b[39;00m k \u001b[39min\u001b[39;00m \u001b[39mlist\u001b[39m(x):\n\u001b[1;32m     22\u001b[0m     \u001b[39mif\u001b[39;00m k \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhf_model_fwd_args:\n\u001b[0;32m---> 23\u001b[0m         \u001b[39mdel\u001b[39;00m x[k]\n\u001b[1;32m     25\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhf_model(\n\u001b[1;32m     26\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mx,\n\u001b[1;32m     27\u001b[0m     output_hidden_states\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput_hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhf_model_kwargs,\n\u001b[1;32m     31\u001b[0m )\n",
      "\u001b[0;31mTypeError\u001b[0m: 'tuple' object does not support item deletion"
     ]
    }
   ],
   "source": [
    "learn.unfreeze()\n",
    "learn.lr_find(suggest_funcs=[minimum, steep, valley, slide])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit_one_cycle(3, slice(3e-6, 3e-5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for m_name, m_val in zip(learn.recorder.metric_names[1:-1], learn.recorder.final_record):\n",
    "    print(m_name, m_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.recorder.plot_loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOPE: Won't work with PyTorch DataLoader\n",
    "# AttributeError: 'DataLoader' object has no attribute 'new'\n",
    "# learn.show_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOPE: Won't work with PyTorch DataLoader\n",
    "# AttributeError: 'DataLoader' object has no attribute 'new'\n",
    "# learn.export(\"../models/test_topic_segmentation.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |echo:false\n",
    "try:\n",
    "    del learn, hf_model\n",
    "except:\n",
    "    pass\n",
    "finally:\n",
    "    clean_memory()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multilabel"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 1: HF objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_arch, hf_config, hf_tokenizer, hf_model = get_new_hf_objects(civil_labels)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 2: `torch.utils.data.Dataset`s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Train|Validation examples: \", len(civil_train_ds), len(civil_valid_ds))\n",
    "\n",
    "print(civil_train_ds[:2])\n",
    "print(\"\")\n",
    "print(civil_train_ds[\"text\"][:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize the dataset\n",
    "def tokenize_func(example):\n",
    "    return hf_tokenizer(example[\"text\"], truncation=True)\n",
    "\n",
    "\n",
    "proc_civil_train_ds = civil_train_ds.map(tokenize_func, batched=True)\n",
    "proc_civil_train_ds.set_format(\"torch\", columns=hf_tokenizer.model_input_names + civil_labels)\n",
    "\n",
    "proc_civil_valid_ds = civil_valid_ds.map(tokenize_func, batched=True)\n",
    "proc_civil_valid_ds.set_format(\"torch\", columns=hf_tokenizer.model_input_names + civil_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define our PyTorch Dataset class\n",
    "class HFTextMultilabelClassificationDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, hf_dataset, hf_tokenizer, labels):\n",
    "        self.hf_dataset = hf_dataset\n",
    "        self.hf_tokenizer = hf_tokenizer\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.hf_dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.hf_dataset[idx]\n",
    "        item[\"label\"] = [item[lbl] for lbl in self.labels]\n",
    "        return item\n",
    "\n",
    "\n",
    "# build our PyTorch training and validation Datasets\n",
    "pt_proc_civil_train_ds = HFTextMultilabelClassificationDataset(proc_civil_train_ds, hf_tokenizer=hf_tokenizer, labels=civil_labels)\n",
    "pt_proc_civil_valid_ds = HFTextMultilabelClassificationDataset(proc_civil_valid_ds, hf_tokenizer=hf_tokenizer, labels=civil_labels)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 3: `DataLoaders`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build your fastai `DataLoaders` from Pytorch `DataLoader` objects\n",
    "batch_size = 4\n",
    "data_collator = TextCollatorWithPadding(hf_tokenizer)\n",
    "train_dl = torch.utils.data.DataLoader(pt_proc_civil_train_ds, batch_size=batch_size, shuffle=True, collate_fn=data_collator)\n",
    "valid_dl = torch.utils.data.DataLoader(pt_proc_civil_valid_ds, batch_size=batch_size * 2, shuffle=False, collate_fn=data_collator)\n",
    "\n",
    "dls = DataLoaders(train_dl, valid_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(train_dl), len(valid_dl))\n",
    "\n",
    "b = next(iter(train_dl))\n",
    "print(len(b))\n",
    "print(\"\")\n",
    "print(hf_tokenizer.decode(b[0][\"input_ids\"][0][:200]))\n",
    "print(\"\")\n",
    "print(b[1])\n",
    "\n",
    "# b"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 4: Learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed()\n",
    "\n",
    "model = BaseModelWrapper(hf_model)\n",
    "\n",
    "learn = Learner(\n",
    "    dls,\n",
    "    model,\n",
    "    opt_func=partial(OptimWrapper, opt=torch.optim.Adam),\n",
    "    loss_func=BCEWithLogitsLossFlat(),\n",
    "    metrics=[partial(accuracy_multi, thresh=0.2)],\n",
    "    cbs=[BaseModelCallback],\n",
    "    splitter=blurr_splitter_on_head,\n",
    ")\n",
    "\n",
    "learn.loss_func.thresh = 0.15\n",
    "learn.create_opt()\n",
    "learn.freeze()\n",
    "\n",
    "learn = learn.to_fp16()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(learn.opt.param_groups))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOPE: Won't work with PyTorch DataLoaders\n",
    "# AttributeError: 'DataLoader' object has no attribute 'show_batch'\n",
    "# learn.dls.valid.show_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOPE: Won't work with PyTorch DataLoaders\n",
    "# AttributeError: 'DataLoader' object has no attribute 'one_batch'\n",
    "# learn.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 5: Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.unfreeze()\n",
    "learn.lr_find(suggest_funcs=[minimum, steep, valley, slide])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit_one_cycle(3, slice(3e-6, 3e-5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for m_name, m_val in zip(learn.recorder.metric_names[1:-1], learn.recorder.final_record):\n",
    "    print(m_name, m_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.recorder.plot_loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOPE: Won't work with PyTorch DataLoader\n",
    "# AttributeError: 'DataLoader' object has no attribute 'new'\n",
    "# learn.show_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOPE: Won't work with PyTorch DataLoader\n",
    "# AttributeError: 'DataLoader' object has no attribute 'new'\n",
    "# learn.export(\"../models/test_topic_segmentation.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |echo:false\n",
    "try:\n",
    "    del learn, hf_model\n",
    "except:\n",
    "    pass\n",
    "finally:\n",
    "    clean_memory()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hugging Face"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multiclass"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 1: HF objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_arch, hf_config, hf_tokenizer, hf_model = get_new_hf_objects(labels)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 2: `Datasets`\n",
    "\n",
    "We'll use the Hugging Face `Dataset` objects created in *Setup*, but these could just as well be instances of `torch.utils.data.Dataset`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Train|Validation examples: \", len(train_ds), len(valid_ds))\n",
    "\n",
    "print(train_ds[:2])\n",
    "print(\"\")\n",
    "print(train_ds[\"text\"][:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize the dataset\n",
    "def tokenize_func(example):\n",
    "    return hf_tokenizer(example[\"text\"], truncation=True)\n",
    "\n",
    "\n",
    "proc_train_ds = train_ds.map(tokenize_func, batched=True)\n",
    "proc_train_ds.set_format(\"torch\", columns=hf_tokenizer.model_input_names + [\"label\"])\n",
    "\n",
    "proc_valid_ds = valid_ds.map(tokenize_func, batched=True)\n",
    "proc_valid_ds.set_format(\"torch\", columns=hf_tokenizer.model_input_names + [\"label\"])\n",
    "\n",
    "print(proc_train_ds)\n",
    "print(proc_valid_ds)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 3: `DataLoaders`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build your fastai `DataLoaders` from Pytorch `DataLoader` objects\n",
    "batch_size = 4\n",
    "data_collator = TextCollatorWithPadding(hf_tokenizer)\n",
    "train_dl = torch.utils.data.DataLoader(proc_train_ds, batch_size=batch_size, shuffle=True, collate_fn=data_collator)\n",
    "valid_dl = torch.utils.data.DataLoader(proc_valid_ds, batch_size=batch_size * 2, shuffle=False, collate_fn=data_collator)\n",
    "\n",
    "dls = DataLoaders(train_dl, valid_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(train_dl), len(valid_dl))\n",
    "\n",
    "b = next(iter(train_dl))\n",
    "print(len(b))\n",
    "print(\"\")\n",
    "print(hf_tokenizer.decode(b[0][\"input_ids\"][0][:200]))\n",
    "print(\"\")\n",
    "print(b[1])\n",
    "\n",
    "# b"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 4: Learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed()\n",
    "\n",
    "model = BaseModelWrapper(hf_model)\n",
    "\n",
    "learn = Learner(\n",
    "    dls,\n",
    "    model,\n",
    "    opt_func=partial(OptimWrapper, opt=torch.optim.Adam),\n",
    "    loss_func=PreCalculatedCrossEntropyLoss(),  # CrossEntropyLossFlat(),\n",
    "    metrics=[accuracy],\n",
    "    cbs=[BaseModelCallback],\n",
    "    splitter=blurr_splitter_on_head,\n",
    ")\n",
    "\n",
    "learn.create_opt()\n",
    "learn.freeze()\n",
    "\n",
    "learn = learn.to_fp16()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(learn.opt.param_groups))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOPE: Won't work with PyTorch DataLoaders\n",
    "# AttributeError: 'DataLoader' object has no attribute 'show_batch'\n",
    "# learn.dls.valid.show_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOPE: Won't work with PyTorch DataLoaders\n",
    "# AttributeError: 'DataLoader' object has no attribute 'one_batch'\n",
    "# learn.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 5: Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.unfreeze()\n",
    "learn.lr_find(suggest_funcs=[minimum, steep, valley, slide])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit_one_cycle(3, slice(3e-6, 3e-5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for m_name, m_val in zip(learn.recorder.metric_names[1:-1], learn.recorder.final_record):\n",
    "    print(m_name, m_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.recorder.plot_loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOPE: Won't work with PyTorch DataLoader\n",
    "# AttributeError: 'DataLoader' object has no attribute 'new'\n",
    "# learn.show_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOPE: Won't work with PyTorch DataLoader\n",
    "# AttributeError: 'DataLoader' object has no attribute 'new'\n",
    "# learn.export(\"../models/test_topic_segmentation.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |echo:false\n",
    "try:\n",
    "    del learn, hf_model\n",
    "except:\n",
    "    pass\n",
    "finally:\n",
    "    clean_memory()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multilabel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 1: HF objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_arch, hf_config, hf_tokenizer, hf_model = get_new_hf_objects(civil_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 2: `Datasets`\n",
    "\n",
    "We'll use the Hugging Face `Dataset` objects created in *Setup*, but these could just as well be instances of `torch.utils.data.Dataset`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Train|Validation examples: \", len(civil_train_ds), len(civil_valid_ds))\n",
    "\n",
    "print(civil_train_ds[:2])\n",
    "print(\"\")\n",
    "print(civil_train_ds[\"text\"][:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize the dataset\n",
    "def tokenize_func(example):\n",
    "    updated_example = dict(hf_tokenizer(example[\"text\"], truncation=True))\n",
    "    labels = torch.stack([tensor(example[lbl]) for lbl in civil_labels], dim=-1)\n",
    "    updated_example[\"label\"] = labels\n",
    "\n",
    "    return updated_example\n",
    "\n",
    "\n",
    "proc_civil_train_ds = civil_train_ds.map(tokenize_func, batched=True)\n",
    "# proc_civil_train_ds.set_format(\"torch\", columns=hf_tokenizer.model_input_names + [\"label\"])\n",
    "\n",
    "proc_civil_valid_ds = civil_valid_ds.map(tokenize_func, batched=True, batch_size=4)\n",
    "# proc_civil_valid_ds.set_format(\"torch\", columns=hf_tokenizer.model_input_names + [\"label\"])\n",
    "\n",
    "print(proc_civil_train_ds)\n",
    "print(proc_civil_valid_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 3: `DataLoaders`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build your fastai `DataLoaders` from Pytorch `DataLoader` objects\n",
    "batch_size = 4\n",
    "data_collator = TextCollatorWithPadding(hf_tokenizer)\n",
    "train_dl = torch.utils.data.DataLoader(proc_civil_train_ds, batch_size=batch_size, shuffle=True, collate_fn=data_collator)\n",
    "valid_dl = torch.utils.data.DataLoader(proc_civil_valid_ds, batch_size=batch_size * 2, shuffle=False, collate_fn=data_collator)\n",
    "\n",
    "dls = DataLoaders(train_dl, valid_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(train_dl), len(valid_dl))\n",
    "\n",
    "b = next(iter(train_dl))\n",
    "print(len(b))\n",
    "print(\"\")\n",
    "print(hf_tokenizer.decode(b[0][\"input_ids\"][0][:200]))\n",
    "print(\"\")\n",
    "print(b[1])\n",
    "\n",
    "# b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b[1].dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 4: Learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed()\n",
    "\n",
    "model = BaseModelWrapper(hf_model)\n",
    "\n",
    "learn = Learner(\n",
    "    dls,\n",
    "    model,\n",
    "    opt_func=partial(OptimWrapper, opt=torch.optim.Adam),\n",
    "    loss_func=BCEWithLogitsLossFlat(),\n",
    "    metrics=[partial(accuracy_multi, thresh=0.2)],\n",
    "    cbs=[BaseModelCallback],\n",
    "    splitter=blurr_splitter_on_head,\n",
    ")\n",
    "\n",
    "learn.loss_func.thresh = 0.15\n",
    "learn.create_opt()\n",
    "learn.freeze()\n",
    "\n",
    "learn = learn.to_fp16()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(learn.opt.param_groups))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOPE: Won't work with PyTorch DataLoaders\n",
    "# AttributeError: 'DataLoader' object has no attribute 'show_batch'\n",
    "# learn.dls.valid.show_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOPE: Won't work with PyTorch DataLoaders\n",
    "# AttributeError: 'DataLoader' object has no attribute 'one_batch'\n",
    "# learn.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 5: Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.unfreeze()\n",
    "learn.lr_find(suggest_funcs=[minimum, steep, valley, slide])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit_one_cycle(3, slice(3e-6, 3e-5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for m_name, m_val in zip(learn.recorder.metric_names[1:-1], learn.recorder.final_record):\n",
    "    print(m_name, m_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.recorder.plot_loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOPE: Won't work with PyTorch DataLoader\n",
    "# AttributeError: 'DataLoader' object has no attribute 'new'\n",
    "# learn.show_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOPE: Won't work with PyTorch DataLoader\n",
    "# AttributeError: 'DataLoader' object has no attribute 'new'\n",
    "# learn.export(\"../models/test_topic_segmentation.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |echo:false\n",
    "try:\n",
    "    del learn, hf_model\n",
    "except:\n",
    "    pass\n",
    "finally:\n",
    "    clean_memory()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Low-Level API: Data\n",
    "\n",
    "This section demonstrates how you can migrate from using PyTorch/Hugging Face to fast.ai `Datasets` and `DataLoaders` to recapture much of the fast.ai specific features unavailable when using basic PyTorch."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `TextInput` -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "class TextInput(TensorBase):\n",
    "    \"\"\"The base represenation of your inputs; used by the various fastai `show` methods\"\"\"\n",
    "\n",
    "    pass"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A `TextInput` object is returned from the decodes method of `BatchDecodeTransform` as a means to customize `@typedispatch`ed functions like `DataLoaders.show_batch` and `Learner.show_results`. The value will the your \"input_ids\"."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `BatchDecodeTransform` -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "class BatchDecodeTransform(Transform):\n",
    "    \"\"\"A class used to cast your inputs as `input_return_type` for fastai `show` methods\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        # A Hugging Face tokenizer (not required if passing in an instance of `BatchTokenizeTransform` to `before_batch_tfm`)\n",
    "        hf_tokenizer: PreTrainedTokenizerBase,\n",
    "        # The abbreviation/name of your Hugging Face transformer architecture (not required if passing in an instance of `BatchTokenizeTransform` to `before_batch_tfm`)\n",
    "        hf_arch: str = None,\n",
    "        # A Hugging Face configuration object (not required if passing in an instance of `BatchTokenizeTransform` to `before_batch_tfm`)\n",
    "        hf_config: PretrainedConfig = None,\n",
    "        # A Hugging Face model (not required if passing in an instance of `BatchTokenizeTransform` to `before_batch_tfm`)\n",
    "        hf_model: PreTrainedModel = None,\n",
    "        # Used by typedispatched show methods\n",
    "        input_return_type: type = TextInput,\n",
    "        # Any other keyword arguments\n",
    "        **kwargs,\n",
    "    ):\n",
    "        store_attr()\n",
    "        self.kwargs = kwargs\n",
    "\n",
    "    def decodes(self, items):\n",
    "        \"\"\"Returns the proper object and data for show related fastai methods\"\"\"\n",
    "        inps = self.input_return_type(items[0][\"input_ids\"])\n",
    "        # inps = self.input_return_type(items[0][0])\n",
    "        if len(items) > 1:\n",
    "            return inps, *items[1:]\n",
    "        else:\n",
    "            labels = items[0].get(\"labels\", [None] * items[0][\"input_ids\"])\n",
    "            return inps, labels"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As of fastai 2.1.5, before batch transforms no longer have a `decodes` method ... and so, I've introduced a standard batch transform here, `BatchDecodeTransform`, (one that occurs \"after\" the batch has been created) that will do the decoding for us."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility classes and methods \n",
    "\n",
    "These methods are use internally for getting blurr transforms associated to your `DataLoaders`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "def get_blurr_tfm(\n",
    "    # A list of transforms (e.g., dls.after_batch, dls.before_batch, etc...)\n",
    "    tfms_list: Pipeline,\n",
    "    # The transform to find\n",
    "    tfm_class: Transform = BatchDecodeTransform,\n",
    "):\n",
    "    \"\"\"\n",
    "    Given a fastai DataLoaders batch transforms, this method can be used to get at a transform\n",
    "    instance used in your Blurr DataBlock\n",
    "    \"\"\"\n",
    "    return next(filter(lambda el: issubclass(type(el), tfm_class), tfms_list), None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbdev.show_doc(get_blurr_tfm, title_level=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "def first_blurr_tfm(\n",
    "    # Your fast.ai `DataLoaders\n",
    "    dls: DataLoaders,\n",
    "    # The Blurr transforms to look for in order\n",
    "    tfms: list[Transform] = [BatchDecodeTransform],\n",
    "):\n",
    "    \"\"\"\n",
    "    This convenience method will find the first Blurr transform required for methods such as\n",
    "    `show_batch` and `show_results`. The returned transform should have everything you need to properly\n",
    "    decode and 'show' your Hugging Face inputs/targets\n",
    "    \"\"\"\n",
    "    for tfm in tfms:\n",
    "        found_tfm = get_blurr_tfm(dls.before_batch, tfm_class=tfm)\n",
    "        if found_tfm:\n",
    "            return found_tfm\n",
    "\n",
    "        found_tfm = get_blurr_tfm(dls.after_batch, tfm_class=tfm)\n",
    "        if found_tfm:\n",
    "            return found_tfm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbdev.show_doc(first_blurr_tfm, title_level=3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `show_batch` -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "@typedispatch\n",
    "def show_batch(\n",
    "    # This typedispatched `show_batch` will be called for `TextInput` typed inputs\n",
    "    x: TextInput,\n",
    "    # Your targets\n",
    "    y,\n",
    "    # Your raw inputs/targets\n",
    "    samples,\n",
    "    # Your `DataLoaders`. This is required so as to get at the Hugging Face objects for\n",
    "    # decoding them into something understandable\n",
    "    dataloaders,\n",
    "    # Your `show_batch` context\n",
    "    ctxs=None,\n",
    "    # The maximum number of items to show\n",
    "    max_n=6,\n",
    "    # Any truncation your want applied to your decoded inputs\n",
    "    trunc_at=None,\n",
    "    # Any other keyword arguments you want applied to `show_batch`\n",
    "    **kwargs,\n",
    "):\n",
    "    # grab our tokenizer\n",
    "    tfm = first_blurr_tfm(dataloaders)\n",
    "    hf_tokenizer = tfm.hf_tokenizer\n",
    "\n",
    "    # if we've included our labels list, we'll use it to look up the value of our target(s)\n",
    "    trg_labels = tfm.kwargs[\"labels\"] if (\"labels\" in tfm.kwargs) else None\n",
    "\n",
    "    res = L()\n",
    "    n_inp = dataloaders.n_inp\n",
    "\n",
    "    n_samples = min(max_n, dataloaders.bs)\n",
    "    for idx in range(n_samples):\n",
    "        input_ids = x[idx]\n",
    "        label = y[idx] if y is not None else None\n",
    "        sample = samples[idx] if samples is not None else None\n",
    "\n",
    "        rets = [hf_tokenizer.decode(input_ids, skip_special_tokens=True)[:trunc_at]]\n",
    "        for item in sample[n_inp:]:\n",
    "            if not torch.is_tensor(item):\n",
    "                trg = trg_labels[int(item)] if trg_labels else item\n",
    "            elif is_listy(item.tolist()):\n",
    "                trg = [trg_labels[idx] for idx, val in enumerate(label.numpy().tolist()) if (val == 1)] if (trg_labels) else label.numpy()\n",
    "            else:\n",
    "                trg = trg_labels[label.item()] if (trg_labels) else label.item()\n",
    "\n",
    "            rets.append(trg)\n",
    "        res.append(tuplify(rets))\n",
    "\n",
    "    cols = [\"text\"] + [\"target\" if (i == 0) else f\"target_{i}\" for i in range(len(res[0]) - n_inp)]\n",
    "    display_df(pd.DataFrame(res, columns=cols)[:max_n])\n",
    "    return ctxs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `TextDataLoader` -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "@delegates()\n",
    "class TextDataLoader(TfmdDL):\n",
    "    \"\"\"\n",
    "    A transformed `DataLoader` that works with Blurr.\n",
    "    From the fastai docs: A `TfmDL` is described as \"a DataLoader that creates Pipeline from a list of Transforms\n",
    "    for the callbacks `after_item`, `before_batch` and `after_batch`. As a result, it can decode or show a processed batch.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        # A standard PyTorch Dataset\n",
    "        dataset: torch.utils.data.dataset.Dataset | Datasets,\n",
    "        # A Hugging Face tokenizer (not required if passing in an instance of `BatchTokenizeTransform` to `before_batch_tfm`)\n",
    "        hf_tokenizer: PreTrainedTokenizerBase,\n",
    "        # The abbreviation/name of your Hugging Face transformer architecture (not required if passing in an \\\n",
    "        # instance of `BatchTokenizeTransform` to `before_batch_tfm`)\n",
    "        hf_arch: str = None,\n",
    "        # A Hugging Face configuration object (not required if passing in an  \\\n",
    "        # instance of `BatchTokenizeTransform` to `before_batch_tfm`)\n",
    "        hf_config: PretrainedConfig = None,\n",
    "        # A Hugging Face model (not required if passing in an instance of `BatchTokenizeTransform` to `before_batch_tfm`)\n",
    "        hf_model: PreTrainedModel = None,\n",
    "        # An instance of `TextCollatorWithPadding` or equivalent (defaults to `BlurrBatchCreator`)\n",
    "        text_collator: TextCollatorWithPadding = None,\n",
    "        # The batch_tfm used to decode Blurr batches (defaults to `BatchDecodeTransform`)\n",
    "        batch_decode_tfm: BatchDecodeTransform = None,\n",
    "        # Used by typedispatched show methods\n",
    "        input_return_type: type = TextInput,\n",
    "        # Keyword arguments to be applied to your `batch_decode_tfm`\n",
    "        batch_decode_kwargs: dict = {},\n",
    "        # Keyword arguments to be applied to `BlurrDataLoader`\n",
    "        **kwargs,\n",
    "    ):\n",
    "        # define what happens when a batch is created (e.g., this is where collation happens)\n",
    "        if \"create_batch\" in kwargs:\n",
    "            kwargs.pop(\"create_batch\")\n",
    "        if not text_collator:\n",
    "            text_collator = TextCollatorWithPadding(hf_tokenizer, hf_arch, hf_config, hf_model)\n",
    "\n",
    "        # define the transform applied after the batch is created (used of show methods)\n",
    "        if \"after_batch\" in kwargs:\n",
    "            kwargs.pop(\"after_batch\")\n",
    "        if not batch_decode_tfm:\n",
    "            batch_decode_tfm = BatchDecodeTransform(\n",
    "                hf_tokenizer,\n",
    "                hf_arch,\n",
    "                hf_config,\n",
    "                hf_model,\n",
    "                input_return_type,\n",
    "                **batch_decode_kwargs.copy(),\n",
    "            )\n",
    "\n",
    "        super().__init__(\n",
    "            dataset=dataset,\n",
    "            create_batch=text_collator,\n",
    "            after_batch=batch_decode_tfm,\n",
    "            **kwargs,\n",
    "        )\n",
    "        store_attr()\n",
    "\n",
    "    def new(\n",
    "        self,\n",
    "        # A standard PyTorch and fastai dataset\n",
    "        dataset: Union[torch.utils.data.dataset.Dataset, Datasets] = None,\n",
    "        # The class you want to create an instance of (will be \"self\" if None)\n",
    "        cls: type = None,\n",
    "        #  Any additional keyword arguments you want to pass to the __init__ method of `cls`\n",
    "        **kwargs,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        We have to override the new method in order to add back the Hugging Face objects in this factory\n",
    "        method (called for example in places like `show_results`). With the exception of the additions to the kwargs\n",
    "        dictionary, the code below is pulled from the `DataLoaders.new` method as is.\n",
    "        \"\"\"\n",
    "        # we need to add these arguments back in (these, after_batch, and create_batch will go in as kwargs)\n",
    "        kwargs[\"hf_arch\"] = self.hf_arch\n",
    "        kwargs[\"hf_config\"] = self.hf_config\n",
    "        kwargs[\"hf_tokenizer\"] = self.hf_tokenizer\n",
    "        kwargs[\"hf_model\"] = self.hf_model\n",
    "\n",
    "        kwargs[\"text_collator\"] = self.text_collator\n",
    "        kwargs[\"batch_decode_tfm\"] = self.batch_decode_tfm\n",
    "        kwargs[\"batch_decode_kwargs\"] = self.batch_decode_kwargs\n",
    "\n",
    "        return super().new(dataset, cls, **kwargs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Low-Level API: Training\n",
    "\n",
    "In this section, we include a custom `show_results()` method that will work with fast.ai's low-level and mid-level (think `DataBlock`) APIs."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `show_results` -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "@typedispatch\n",
    "def show_results(\n",
    "    # This typedispatched `show_results` will be called for `TextInput` typed inputs\n",
    "    x: TextInput,\n",
    "    # Your targets\n",
    "    y,\n",
    "    # Your raw inputs/targets\n",
    "    samples,\n",
    "    # The model's predictions\n",
    "    outs,\n",
    "    # Your `Learner`. This is required so as to get at the Hugging Face objects for decoding them into\n",
    "    # something understandable\n",
    "    learner,\n",
    "    # Your `show_results` context\n",
    "    ctxs=None,\n",
    "    # The maximum number of items to show\n",
    "    max_n=6,\n",
    "    # Any truncation your want applied to your decoded inputs\n",
    "    trunc_at=None,\n",
    "    # Any other keyword arguments you want applied to `show_results`\n",
    "    **kwargs,\n",
    "):\n",
    "    # grab our tokenizer\n",
    "    tfm = first_blurr_tfm(learner.dls)\n",
    "    hf_tokenizer = tfm.hf_tokenizer\n",
    "\n",
    "    # if we've included our labels list, we'll use it to look up the value of our target(s)\n",
    "    trg_labels = tfm.kwargs[\"labels\"] if (\"labels\" in tfm.kwargs) else None\n",
    "\n",
    "    res = L()\n",
    "    n_inp = learner.dls.n_inp\n",
    "\n",
    "    n_samples = min(max_n, learner.dls.bs)\n",
    "    for idx in range(n_samples):\n",
    "        input_ids = x[idx]\n",
    "        label = y[idx] if y is not None else None\n",
    "        pred = outs[idx]\n",
    "        sample = samples[idx] if samples is not None else None\n",
    "        # add in the input text\n",
    "        rets = [hf_tokenizer.decode(input_ids, skip_special_tokens=True)[:trunc_at]]\n",
    "        # add in the targets\n",
    "        for item in sample[n_inp:]:\n",
    "            if not torch.is_tensor(item):\n",
    "                trg = trg_labels[int(item)] if trg_labels else item\n",
    "            elif is_listy(item.tolist()):\n",
    "                trg = [trg_labels[idx] for idx, val in enumerate(label.numpy().tolist()) if (val == 1)] if (trg_labels) else label.numpy()\n",
    "            else:\n",
    "                trg = trg_labels[label.item()] if (trg_labels) else label.item()\n",
    "\n",
    "            rets.append(trg)\n",
    "        # add in the predictions\n",
    "\n",
    "        res.append(tuplify(rets))\n",
    "        for item in pred:\n",
    "            if not torch.is_tensor(item):\n",
    "                p = trg_labels[int(item)] if trg_labels else item\n",
    "            elif is_listy(item.tolist()):\n",
    "                p = [trg_labels[idx] for idx, val in enumerate(label.numpy().tolist()) if (val == 1)] if (trg_labels) else label.numpy()\n",
    "            else:\n",
    "                p = trg_labels[label.item()] if (trg_labels) else label.item()\n",
    "\n",
    "            rets.append(p)\n",
    "\n",
    "        res.append(tuplify(rets))\n",
    "\n",
    "    cols = [\"text\"] + [\"target\" if (i == 0) else f\"target_{i}\" for i in range(len(res[0]) - n_inp * 2)]\n",
    "    cols += [\"prediction\" if (i == 0) else f\"prediction_{i}\" for i in range(len(res[0]) - n_inp * 2)]\n",
    "    display_df(pd.DataFrame(res, columns=cols)[:max_n])\n",
    "    return ctxs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Low-Level API: Examples"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using fast.ai `Datasets` and `DataLoaders`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multiclass"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 1: HF objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_arch, hf_config, hf_tokenizer, hf_model = get_new_hf_objects(labels)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 2: `Datasets`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize the dataset\n",
    "def tokenize_func(example):\n",
    "    return hf_tokenizer(example[\"text\"], truncation=True)\n",
    "\n",
    "\n",
    "proc_imdb_ds = imdb_ds.map(tokenize_func, batched=True)\n",
    "proc_imdb_ds.set_format(\"torch\", columns=hf_tokenizer.model_input_names + [\"label\"])\n",
    "\n",
    "# turn Arrow into DataFrame (`ColSplitter` only works with `DataFrame`s)\n",
    "train_df = pd.DataFrame(proc_imdb_ds)\n",
    "train_df.head()\n",
    "\n",
    "# define dataset splitter\n",
    "splitter = ColSplitter(\"is_valid\")\n",
    "splits = splitter(imdb_df)\n",
    "\n",
    "\n",
    "# define how we want to build our inputs and targets\n",
    "def _build_inputs(example):\n",
    "    return {fwd_arg_name: example[fwd_arg_name] for fwd_arg_name in hf_tokenizer.model_input_names if fwd_arg_name in list(example.keys())}\n",
    "\n",
    "\n",
    "def _build_targets(example):\n",
    "    return example[\"label\"]\n",
    "\n",
    "\n",
    "# create our fastai `Datasets` object\n",
    "dsets = Datasets(items=train_df, splits=splits, tfms=[[_build_inputs], _build_targets], n_inp=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Items in train|validation datasets: \", len(dsets.train), len(dsets.valid))\n",
    "\n",
    "example = dsets.valid[0]\n",
    "# example\n",
    "\n",
    "print(f\"Items in each example: {len(example)}\")\n",
    "print(f\"Example inputs: {list(example[0].keys())}\")\n",
    "print(f\"Example target(s): {example[1]}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 3: `DataLoaders`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sorted_dl_func(item):\n",
    "    return len(item[0][\"input_ids\"])\n",
    "\n",
    "\n",
    "data_collator = TextCollatorWithPadding(hf_tokenizer)\n",
    "dls = dsets.dataloaders(\n",
    "    batch_size=4,\n",
    "    create_batch=data_collator,\n",
    "    dl_type=partial(SortedDL, sort_func=sorted_dl_func),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(dls.train), len(dls.valid))\n",
    "\n",
    "b = dls.valid.one_batch()\n",
    "print(len(b))\n",
    "print(\"\")\n",
    "print(hf_tokenizer.decode(b[0][\"input_ids\"][0][:200]))\n",
    "print(\"\")\n",
    "print(b[1])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 4: Learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed()\n",
    "\n",
    "model = BaseModelWrapper(hf_model)\n",
    "\n",
    "learn = Learner(\n",
    "    dls,\n",
    "    model,\n",
    "    opt_func=partial(OptimWrapper, opt=torch.optim.Adam),\n",
    "    loss_func=CrossEntropyLossFlat(),\n",
    "    metrics=[accuracy],\n",
    "    cbs=[BaseModelCallback],\n",
    "    splitter=blurr_splitter_on_head,\n",
    ")\n",
    "\n",
    "learn.create_opt()\n",
    "learn.freeze()\n",
    "\n",
    "learn = learn.to_fp16()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(learn.opt.param_groups))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOPE: Will work assuming we define a suitable `show_batch()`` method for our examples\n",
    "# AttributeError: 'DataLoader' object has no attribute 'show_batch'\n",
    "# learn.dls.valid.show_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YES: Since we're using fastai `DataLoaders`, we get `one_batch()` back\n",
    "learn.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 5: Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.unfreeze()\n",
    "learn.lr_find(suggest_funcs=[minimum, steep, valley, slide])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit_one_cycle(3, slice(3e-6, 3e-5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for m_name, m_val in zip(learn.recorder.metric_names[1:-1], learn.recorder.final_record):\n",
    "    print(m_name, m_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.recorder.plot_loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOPE: Won't work with PyTorch DataLoader\n",
    "# AttributeError: 'DataLoader' object has no attribute 'new'\n",
    "# learn.show_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YES: Will work now that we're using fastai `Datasets`, `DataLoaders`, and transforms\n",
    "# AttributeError: 'DataLoader' object has no attribute 'new'\n",
    "learn.export(\"../models/test_text_core_low_level.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |echo:false\n",
    "try:\n",
    "    del learn, hf_model\n",
    "except:\n",
    "    pass\n",
    "finally:\n",
    "    clean_memory()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multilabel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 1: HF objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_arch, hf_config, hf_tokenizer, hf_model = get_new_hf_objects(civil_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 2: `Datasets`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize the dataset\n",
    "def tokenize_func(example):\n",
    "    updated_example = dict(hf_tokenizer(example[\"text\"], truncation=True))\n",
    "    labels = torch.stack([tensor(example[lbl]) for lbl in civil_labels], dim=-1)\n",
    "    updated_example[\"label\"] = labels\n",
    "\n",
    "    return updated_example\n",
    "\n",
    "\n",
    "proc_civil_ds = civil_ds.map(tokenize_func, batched=True)\n",
    "proc_civil_ds.set_format(\"torch\", columns=hf_tokenizer.model_input_names + [\"label\"])\n",
    "\n",
    "# turn Arrow into DataFrame (`ColSplitter` only works with `DataFrame`s)\n",
    "train_df = pd.DataFrame(proc_civil_ds)\n",
    "train_df.head()\n",
    "\n",
    "# define dataset splitter\n",
    "splitter = ColSplitter(\"is_valid\")\n",
    "splits = splitter(civil_df)\n",
    "\n",
    "\n",
    "# define how we want to build our inputs and targets\n",
    "def _build_inputs(example):\n",
    "    return {fwd_arg_name: example[fwd_arg_name] for fwd_arg_name in hf_tokenizer.model_input_names if fwd_arg_name in list(example.keys())}\n",
    "\n",
    "\n",
    "def _build_targets(example):\n",
    "    return example[\"label\"]\n",
    "\n",
    "\n",
    "# create our fastai `Datasets` object\n",
    "dsets = Datasets(items=train_df, splits=splits, tfms=[[_build_inputs], _build_targets], n_inp=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Items in train|validation datasets: \", len(dsets.train), len(dsets.valid))\n",
    "\n",
    "example = dsets.valid[0]\n",
    "# example\n",
    "\n",
    "print(f\"Items in each example: {len(example)}\")\n",
    "print(f\"Example inputs: {list(example[0].keys())}\")\n",
    "print(f\"Example target(s): {example[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 3: `DataLoaders`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sorted_dl_func(item):\n",
    "    return len(item[0][\"input_ids\"])\n",
    "\n",
    "\n",
    "data_collator = TextCollatorWithPadding(hf_tokenizer)\n",
    "dls = dsets.dataloaders(\n",
    "    batch_size=4,\n",
    "    create_batch=data_collator,\n",
    "    dl_type=partial(SortedDL, sort_func=sorted_dl_func),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(dls.train), len(dls.valid))\n",
    "\n",
    "b = dls.valid.one_batch()\n",
    "print(len(b))\n",
    "print(\"\")\n",
    "print(hf_tokenizer.decode(b[0][\"input_ids\"][0][:200]))\n",
    "print(\"\")\n",
    "print(b[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 4: Learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed()\n",
    "\n",
    "model = BaseModelWrapper(hf_model)\n",
    "\n",
    "learn = Learner(\n",
    "    dls,\n",
    "    model,\n",
    "    opt_func=partial(OptimWrapper, opt=torch.optim.Adam),\n",
    "    loss_func=BCEWithLogitsLossFlat(),\n",
    "    metrics=[partial(accuracy_multi, thresh=0.2)],\n",
    "    cbs=[BaseModelCallback],\n",
    "    splitter=blurr_splitter_on_head,\n",
    ")\n",
    "\n",
    "learn.loss_func.thresh = 0.15\n",
    "learn.create_opt()\n",
    "learn.freeze()\n",
    "\n",
    "learn = learn.to_fp16()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(learn.opt.param_groups))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOPE: Will work assuming we define a suitable `show_batch()`` method for our examples\n",
    "# AttributeError: 'DataLoader' object has no attribute 'show_batch'\n",
    "# learn.dls.valid.show_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YES: Since we're using fastai `DataLoaders`, we get `one_batch()` back\n",
    "learn.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 5: Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.unfreeze()\n",
    "learn.lr_find(suggest_funcs=[minimum, steep, valley, slide])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit_one_cycle(3, slice(3e-6, 3e-5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for m_name, m_val in zip(learn.recorder.metric_names[1:-1], learn.recorder.final_record):\n",
    "    print(m_name, m_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.recorder.plot_loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOPE: Won't work with PyTorch DataLoader\n",
    "# AttributeError: 'DataLoader' object has no attribute 'new'\n",
    "# learn.show_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YES: Will work now that we're using fastai `Datasets`, `DataLoaders`, and transforms\n",
    "# AttributeError: 'DataLoader' object has no attribute 'new'\n",
    "learn.export(\"../models/test_text_core_low_level.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |echo:false\n",
    "try:\n",
    "    del learn, hf_model\n",
    "except:\n",
    "    pass\n",
    "finally:\n",
    "    clean_memory()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using `BatchDecodeTransform` and `TextDataLoader`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multiclass"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 1: HF objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_arch, hf_config, hf_tokenizer, hf_model = get_new_hf_objects(labels)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 2: `Dataset`s\n",
    "\n",
    "We'll use the Hugging Face `Dataset` objects created in *Setup*, but these could just as well be instances of `torch.utils.data.Dataset`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize the dataset\n",
    "def tokenize_func(example):\n",
    "    return hf_tokenizer(example[\"text\"], truncation=True)\n",
    "\n",
    "\n",
    "proc_imdb_ds = imdb_ds.map(tokenize_func, batched=True)\n",
    "# proc_imdb_ds.set_format(\"torch\", columns=hf_tokenizer.model_input_names + [\"label\"])\n",
    "\n",
    "\n",
    "# define dataset splitter\n",
    "def _split_func(example):\n",
    "    return example[\"is_valid\"] == True\n",
    "\n",
    "\n",
    "splitter = FuncSplitter(_split_func)\n",
    "splits = splitter(proc_imdb_ds)\n",
    "\n",
    "\n",
    "# define how we want to build our inputs and targets\n",
    "def _build_inputs(example):\n",
    "    return {fwd_arg_name: example[fwd_arg_name] for fwd_arg_name in hf_tokenizer.model_input_names if fwd_arg_name in list(example.keys())}\n",
    "\n",
    "\n",
    "def _build_targets(example):\n",
    "    return example[\"label\"]\n",
    "\n",
    "\n",
    "# create our fastai `Datasets` object\n",
    "dsets = Datasets(items=proc_imdb_ds, splits=splits, tfms=[[_build_inputs], _build_targets], n_inp=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 3: `DataLoaders`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_names = train_ds.features[\"label\"].names\n",
    "label_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "\n",
    "trn_dl = TextDataLoader(\n",
    "    dsets.train,\n",
    "    hf_tokenizer,\n",
    "    batch_decode_kwargs={\"labels\": label_names},\n",
    "    shuffle=True,\n",
    "    batch_size=batch_size,\n",
    ")\n",
    "\n",
    "val_dl = TextDataLoader(\n",
    "    dsets.valid,\n",
    "    hf_tokenizer,\n",
    "    batch_decode_kwargs={\"labels\": label_names},\n",
    "    batch_size=batch_size * 2,\n",
    ")\n",
    "\n",
    "dls = DataLoaders(trn_dl, val_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(dls.train), len(dls.valid))\n",
    "\n",
    "b = dls.valid.one_batch()\n",
    "print(len(b))\n",
    "print(\"\")\n",
    "print(hf_tokenizer.decode(b[0][\"input_ids\"][0][:200]))\n",
    "print(\"\")\n",
    "print(b[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dls.show_batch(dataloaders=dls, max_n=2, trunc_at=800)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 4: Learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed()\n",
    "\n",
    "model = BaseModelWrapper(hf_model)\n",
    "\n",
    "learn = Learner(\n",
    "    dls,\n",
    "    model,\n",
    "    opt_func=partial(OptimWrapper, opt=torch.optim.Adam),\n",
    "    loss_func=CrossEntropyLossFlat(),\n",
    "    metrics=[accuracy],\n",
    "    cbs=[BaseModelCallback],\n",
    "    splitter=blurr_splitter_on_head,\n",
    ")\n",
    "\n",
    "learn.create_opt()\n",
    "learn.freeze()\n",
    "\n",
    "learn = learn.to_fp16()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(learn.opt.param_groups))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YES: Since we're using fastai `TfmDL` and including a transform that knows how to show our inputs,\n",
    "# we get `show_batch()` back\n",
    "learn.dls.valid.show_batch(dataloaders=learn.dls, max_n=2, trunc_at=250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YES: Since we're using fastai `DataLoaders`, we get `learn.summary()` back\n",
    "learn.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 5: Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.unfreeze()\n",
    "learn.lr_find(suggest_funcs=[minimum, steep, valley, slide])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit_one_cycle(3, slice(3e-6, 3e-5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for m_name, m_val in zip(learn.recorder.metric_names[1:-1], learn.recorder.final_record):\n",
    "    print(m_name, m_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.recorder.plot_loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@patch\n",
    "def show_results(\n",
    "    self: TfmdDL,\n",
    "    b,  # Batch to show results for\n",
    "    out,  # Predicted output from model for the batch\n",
    "    max_n: int = 9,  # Maximum number of items to show\n",
    "    ctxs=None,  # List of `ctx` objects to show data. Could be matplotlib axis, DataFrame etc\n",
    "    show: bool = True,  # Whether to display data\n",
    "    **kwargs,\n",
    "):\n",
    "    \"Show `max_n` results with input(s), target(s) and prediction(s).\"\n",
    "    x, y, its = self.show_batch(b, max_n=max_n, show=False)\n",
    "    b_out = type(b)(b[: self.n_inp] + (tuple(out) if is_listy(out) else (out,)))\n",
    "    x1, y1, outs = self.show_batch(b_out, max_n=max_n, show=False)\n",
    "    res = (x, x1, None, None) if its is None else (x, y, its, outs.itemgot(slice(self.n_inp, None)))\n",
    "    if not show:\n",
    "        return res\n",
    "    show_results(*res, ctxs=ctxs, max_n=max_n, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = (1, 2, 31)\n",
    "b[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YES: Will work now that we're using fastai `Datasets`, `DataLoaders`, and transforms\n",
    "learn.show_results(learner=learn, max_n=2, trunc_at=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YES: Will work now that we're using fastai `Datasets`, `DataLoaders`, and transforms\n",
    "learn.export(\"../models/test_text_core_low_level.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |echo:false\n",
    "try:\n",
    "    del learn, hf_model\n",
    "except:\n",
    "    pass\n",
    "finally:\n",
    "    clean_memory()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multilabel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 1: HF objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_arch, hf_config, hf_tokenizer, hf_model = get_new_hf_objects(civil_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 2: `Dataset`s\n",
    "\n",
    "We'll use the Hugging Face `Dataset` objects created in *Setup*, but these could just as well be instances of `torch.utils.data.Dataset`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize the dataset\n",
    "def tokenize_func(example):\n",
    "    updated_example = dict(hf_tokenizer(example[\"text\"], truncation=True))\n",
    "    labels = torch.stack([tensor(example[lbl]) for lbl in civil_labels], dim=-1)\n",
    "    updated_example[\"label\"] = labels\n",
    "\n",
    "    return updated_example\n",
    "\n",
    "\n",
    "proc_civil_ds = civil_ds.map(tokenize_func, batched=True)\n",
    "# proc_imdb_ds.set_format(\"torch\", columns=hf_tokenizer.model_input_names + [\"label\"])\n",
    "\n",
    "\n",
    "# define dataset splitter\n",
    "def _split_func(example):\n",
    "    return example[\"is_valid\"] == True\n",
    "\n",
    "\n",
    "splitter = FuncSplitter(_split_func)\n",
    "splits = splitter(proc_civil_ds)\n",
    "\n",
    "\n",
    "# define how we want to build our inputs and targets\n",
    "def _build_inputs(example):\n",
    "    return {fwd_arg_name: example[fwd_arg_name] for fwd_arg_name in hf_tokenizer.model_input_names if fwd_arg_name in list(example.keys())}\n",
    "\n",
    "\n",
    "def _build_targets(example):\n",
    "    return example[\"label\"]\n",
    "\n",
    "\n",
    "# create our fastai `Datasets` object\n",
    "dsets = Datasets(items=proc_civil_ds, splits=splits, tfms=[[_build_inputs], _build_targets], n_inp=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 3: `DataLoaders`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "\n",
    "trn_dl = TextDataLoader(\n",
    "    dsets.train,\n",
    "    hf_tokenizer,\n",
    "    batch_decode_kwargs={\"labels\": civil_labels},\n",
    "    shuffle=True,\n",
    "    batch_size=batch_size,\n",
    ")\n",
    "\n",
    "val_dl = TextDataLoader(\n",
    "    dsets.valid,\n",
    "    hf_tokenizer,\n",
    "    batch_decode_kwargs={\"labels\": civil_labels},\n",
    "    batch_size=batch_size * 2,\n",
    ")\n",
    "\n",
    "dls = DataLoaders(trn_dl, val_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(dls.train), len(dls.valid))\n",
    "\n",
    "b = dls.valid.one_batch()\n",
    "print(len(b))\n",
    "print(\"\")\n",
    "print(hf_tokenizer.decode(b[0][\"input_ids\"][0][:200]))\n",
    "print(\"\")\n",
    "print(b[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dls.show_batch(dataloaders=dls, max_n=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 4: Learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed()\n",
    "\n",
    "model = BaseModelWrapper(hf_model)\n",
    "\n",
    "learn = Learner(\n",
    "    dls,\n",
    "    model,\n",
    "    opt_func=partial(OptimWrapper, opt=torch.optim.Adam),\n",
    "    loss_func=BCEWithLogitsLossFlat(),\n",
    "    metrics=[partial(accuracy_multi, thresh=0.2)],\n",
    "    cbs=[BaseModelCallback],\n",
    "    splitter=blurr_splitter_on_head,\n",
    ")\n",
    "\n",
    "learn.loss_func.thresh = 0.15\n",
    "learn.create_opt()\n",
    "learn.freeze()\n",
    "\n",
    "learn = learn.to_fp16()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(learn.opt.param_groups))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YES: Since we're using fastai `TfmDL` and including a transform that knows how to show our inputs,\n",
    "# we get `show_batch()` back\n",
    "learn.dls.valid.show_batch(dataloaders=learn.dls, max_n=2, trunc_at=250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YES: Since we're using fastai `DataLoaders`, we get `learn.summary()` back\n",
    "learn.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 5: Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.unfreeze()\n",
    "learn.lr_find(suggest_funcs=[minimum, steep, valley, slide])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit_one_cycle(3, slice(3e-6, 3e-5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for m_name, m_val in zip(learn.recorder.metric_names[1:-1], learn.recorder.final_record):\n",
    "    print(m_name, m_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.recorder.plot_loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YES: Will work now that we're using fastai `Datasets`, `DataLoaders`, and transforms\n",
    "learn.show_results(learner=learn, max_n=2, trunc_at=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YES: Will work now that we're using fastai `Datasets`, `DataLoaders`, and transforms\n",
    "learn.export(\"../models/test_text_core_low_level_multilabel.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |echo:false\n",
    "try:\n",
    "    del learn, hf_model\n",
    "except:\n",
    "    pass\n",
    "finally:\n",
    "    clean_memory()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Low-Level API: Predictions & Text Generation\n",
    "\n",
    "**Prediction**\n",
    "\n",
    "We need to replace fastai's `Learner.predict` method with the one above which is able to work with inputs that are represented by multiple tensors included in a dictionary.\n",
    "\n",
    "**Text Generation**\n",
    "\n",
    "With regards to **text generation**, though not useful in sequence classification, we will also add a `blurr_generate` method to `Learner` that uses Hugging Face's `PreTrainedModel.generate` for text generation tasks.  \n",
    "\n",
    "For the full list of arguments you can pass in see [here](https://huggingface.co/transformers/main_classes/model.html#transformers.PreTrainedModel.generate). You can also check out their [\"How To Generate\"](https://github.com/huggingface/blog/blob/master/notebooks/02_how_to_generate.ipynb) notebook for more information about how it all works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "@patch\n",
    "def blurr_predict(self: Learner, items, rm_type_tfms=None, tok_is_split_into_words=False):\n",
    "    # grab our blurr tfm with the bits to properly decode/show our inputs/targets\n",
    "    tfm = first_blurr_tfm(self.dls)\n",
    "    hf_tokenizer = tfm.hf_tokenizer\n",
    "    trg_labels = tfm.kwargs[\"labels\"] if (\"labels\" in tfm.kwargs) else None\n",
    "\n",
    "    is_split_into_words = getattr(tfm, \"is_split_into_words\", tok_is_split_into_words)\n",
    "    is_split_str = is_split_into_words == True and isinstance(items[0], str)\n",
    "    is_df = isinstance(items, pd.DataFrame)\n",
    "\n",
    "    if not is_df and (is_split_str or not is_listy(items)):\n",
    "        items = [items]\n",
    "\n",
    "    inputs_d = dict(\n",
    "        hf_tokenizer(items, is_split_into_words=is_split_into_words, padding=True, max_length=True, truncation=True, return_tensors=\"pt\")\n",
    "    )\n",
    "    encoded_items = [{k: inputs_d[k][idx] for k in inputs_d.keys()} for idx in range(len(inputs_d[\"input_ids\"]))]\n",
    "    dl = self.dls.test_dl(encoded_items, rm_type_tfms=rm_type_tfms, num_workers=0)\n",
    "\n",
    "    with self.no_bar():\n",
    "        probs, _, decoded_preds = self.get_preds(dl=dl, with_input=False, with_decoded=True)\n",
    "\n",
    "    trg_tfms = self.dls.tfms[self.dls.n_inp :]\n",
    "\n",
    "    outs = []\n",
    "    is_multilabel = isinstance(self.loss_func, BCEWithLogitsLossFlat)\n",
    "    probs, decoded_preds = L(probs), L(decoded_preds)\n",
    "    for i in range(len(items)):\n",
    "        item_probs = probs.itemgot(i)\n",
    "        item_dec_preds = decoded_preds.itemgot(i)\n",
    "        item_dec_labels = tuplify([tfm.decode(item_dec_preds[tfm_idx]) for tfm_idx, tfm in enumerate(trg_tfms)])[0]\n",
    "\n",
    "        if trg_labels:\n",
    "            # handle multiclass output\n",
    "            if len(item_dec_labels.size()) == 0:\n",
    "                item_dec_labels = [item_dec_labels.item()]\n",
    "            # handle multilabel output\n",
    "            else:\n",
    "                item_dec_labels = [trg_labels[lbl_idx] for lbl_idx, lbl in enumerate(item_dec_labels) if lbl == True]\n",
    "\n",
    "        res = {}\n",
    "        if is_multilabel:\n",
    "            res[\"labels\"] = list(item_dec_labels)\n",
    "            msk = item_dec_preds[0]\n",
    "            res[\"scores\"] = item_probs[0][msk].tolist()\n",
    "            res[\"class_indices\"] = [int(val) for val in item_dec_preds[0]]\n",
    "        else:\n",
    "            res[\"label\"] = item_dec_labels[0]\n",
    "            res[\"score\"] = item_probs[0].tolist()[item_dec_preds[0]]\n",
    "            res[\"class_index\"] = item_dec_preds[0].item()\n",
    "\n",
    "        if trg_labels is not None or hasattr(self.dls, \"vocab\"):\n",
    "            res[\"class_labels\"] = trg_labels if trg_labels else self.dls.vocab\n",
    "        else:\n",
    "            res[\"class_labels\"] = None\n",
    "\n",
    "        res[\"probs\"] = item_probs[0].tolist()\n",
    "\n",
    "        outs.append(res)\n",
    "    return outs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbdev.show_doc(Learner.blurr_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inf_learn = load_learner(\"../models/test_text_core_low_level.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inf_learn.blurr_predict(\"This is perhaps the best movie I have ever seen!!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inf_learn.blurr_predict(\"Acting was so bad it was almost funny.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inf_learn.blurr_predict([\"I really liked the movie\", \"Worse movie I ever saw\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inf_learn = load_learner(\"../models/test_text_core_low_level_multilabel.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inf_learn.blurr_predict(\"I will tell everyone what you did if you tell you jerk!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "@patch\n",
    "def blurr_generate(self: Learner, items, key=\"generated_texts\", **kwargs):\n",
    "    \"\"\"Uses the built-in `generate` method to generate the text\n",
    "    (see [here](https://huggingface.co/transformers/main_classes/model.html#transformers.PreTrainedModel.generate)\n",
    "    for a list of arguments you can pass in)\n",
    "    \"\"\"\n",
    "    if not is_listy(items):\n",
    "        items = [items]\n",
    "\n",
    "    # grab our blurr tfm with the bits to properly decode/show our inputs/targets\n",
    "    tfm = first_blurr_tfm(self.dls)\n",
    "\n",
    "    # grab the Hugging Face tokenizer from the learner's dls.tfms\n",
    "    hf_tokenizer = tfm.hf_tokenizer\n",
    "    tok_kwargs = tfm.tok_kwargs\n",
    "\n",
    "    # grab the text generation kwargs\n",
    "    text_gen_kwargs = tfm.text_gen_kwargs if (len(kwargs) == 0) else kwargs\n",
    "\n",
    "    results = []\n",
    "    for idx, inp in enumerate(items):\n",
    "        if isinstance(inp, str):\n",
    "            input_ids = hf_tokenizer.encode(inp, padding=True, truncation=True, return_tensors=\"pt\", **tok_kwargs)\n",
    "        else:\n",
    "            # note (10/30/2020): as of pytorch 1.7, this has to be a plain ol tensor (not a subclass of TensorBase)\n",
    "            input_ids = inp.as_subclass(Tensor)\n",
    "\n",
    "        input_ids = input_ids.to(self.model.hf_model.device)\n",
    "\n",
    "        gen_texts = self.model.hf_model.generate(input_ids, **text_gen_kwargs)\n",
    "        outputs = [hf_tokenizer.decode(txt, skip_special_tokens=True, clean_up_tokenization_spaces=False) for txt in gen_texts]\n",
    "\n",
    "        if tfm.hf_arch == \"pegasus\":\n",
    "            outputs = [o.replace(\"<n>\", \" \") for o in outputs]\n",
    "\n",
    "        results.append({key: outputs[0] if len(outputs) == 1 else outputs})\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbdev.show_doc(Learner.blurr_generate)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mid-Level API: Data\n",
    "\n",
    "BLURR's mid-level API provides a way to build your `DataLoaders` using fast.ai's mid-level `DataBlock` API.  \n",
    "\n",
    "BLURR supports three ways of doing this in the mid-level API: Using pre-tokenized data (the traditional approach), batch-time tokenization (the default approach in previous versions of blurr), and item-time tokenization (e.g., to apply tokenization on individual items as they are pulled from their respective `Dataset`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tests\n",
    "\n",
    "The tests below to ensure the core DataBlock code above works for **all** pretrained sequence classification models available in Hugging Face.  These tests are excluded from the CI workflow because of how long they would take to run and the amount of data that would be required to download.\n",
    "\n",
    "**Note**: Feel free to modify the code below to test whatever pretrained classification models you are working with ... and if any of your pretrained sequence classification models fail, please submit a github issue *(or a PR if you'd like to fix it yourself)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "import nbdev\n",
    "\n",
    "nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
