{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Token Classification: Data\n",
    "\n",
    "> The `data.token_classification` module contains the core bits required to use fast.ai's low-level and/or mid-level APIs to define `Datasets`, build `DataLoaders` for training transformers on token classification tasks (e.g., Named entity recognition (NER), Part-of-speech tagging (POS), etc...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |default_exp data.token_classification\n",
    "# |default_cls_lvl 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "from __future__ import annotations\n",
    "\n",
    "import gc, importlib, sys, traceback\n",
    "\n",
    "from accelerate.logging import get_logger\n",
    "from dataclasses import dataclass\n",
    "from dotenv import load_dotenv\n",
    "from fastai.callback.all import *\n",
    "from fastai.imports import *\n",
    "from fastai.learner import *\n",
    "from fastai.losses import CrossEntropyLossFlat\n",
    "from fastai.data.block import TransformBlock, Category, CategoryMap\n",
    "from fastai.data.transforms import TfmdDL\n",
    "from fastai.text.data import SortedDL\n",
    "from fastai.torch_core import *\n",
    "from fastai.torch_imports import *\n",
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoTokenizer,\n",
    "    PretrainedConfig,\n",
    "    PreTrainedTokenizerBase,\n",
    "    PreTrainedModel,\n",
    "    AutoModelForTokenClassification,\n",
    ")\n",
    "from transformers import logging as hf_logging\n",
    "from transformers.data.data_collator import DataCollatorForTokenClassification, DataCollatorWithPadding\n",
    "\n",
    "from blurr.text.data.core import (\n",
    "    ItemTokenizeTransform,\n",
    "    TextInput,\n",
    "    TextCollatorWithPadding,\n",
    "    BatchTokenizeTransform,\n",
    "    first_blurr_tfm,\n",
    "    sorted_dl_func,\n",
    "    TextDataLoader,\n",
    ")\n",
    "from blurr.utils import clean_memory, get_hf_objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |hide\n",
    "import pdb\n",
    "from fastai.data.transforms import DataLoader, DataLoaders, Datasets, ItemTransform\n",
    "from fastai.losses import BaseLoss, BCEWithLogitsLossFlat\n",
    "from datasets import concatenate_datasets, load_dataset, Value\n",
    "from fastai.data.block import CategoryBlock, ColReader, ColSplitter, DataBlock, FuncSplitter, MultiCategoryBlock\n",
    "from fastcore.test import *\n",
    "import nbdev\n",
    "\n",
    "from blurr.text.data.core import TextBlock\n",
    "from blurr.utils import print_versions, set_seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# |export\n",
    "# silence all the HF warnings and load environment variables\n",
    "warnings.simplefilter(\"ignore\")\n",
    "hf_logging.set_verbosity_error()\n",
    "logger = get_logger(__name__)\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU #0: NVIDIA GeForce RTX 3090\n"
     ]
    }
   ],
   "source": [
    "# |hide\n",
    "# |notest\n",
    "torch.cuda.set_device(0)\n",
    "print(f\"Using GPU #{torch.cuda.current_device()}: {torch.cuda.get_device_name()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What we're running with at the time this documentation was generated:\n",
      "torch: 1.13.1\n",
      "fastai: 2.7.11\n",
      "transformers: 4.26.1\n"
     ]
    }
   ],
   "source": [
    "# | echo: false\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "print(\"What we're running with at the time this documentation was generated:\")\n",
    "print_versions(\"torch fastai transformers\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "We'll use a subset of `conll2003` to demonstrate how to configure your blurr code for token classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset conll2003 (/home/wgilliam/.cache/huggingface/datasets/conll2003/conll2003/1.0.0/9a4d16a94f8674ba3466315300359b0acd891b68b6c8743ddf60b9c702adce98)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40facdd545ad44e4887e3f2078797a56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n",
       "        num_rows: 14041\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n",
       "        num_rows: 3250\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n",
       "        num_rows: 3453\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conll2003_dsd = load_dataset(\"conll2003\")\n",
    "conll2003_dsd"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to get a list of the distinct entities we want to predict. If they are represented as list in their raw/readable form in another attribute/column in our dataset, we could use something like this to build a sorted list of distinct values as such: \n",
    "\n",
    "`label_names = sorted(list(set([lbls for sublist in germ_eval_df.labels.tolist() for lbls in sublist])))`\n",
    "\n",
    "Fortunately, the `conll2003` dataset allows us to get at this list directly using the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['O', 'B-ADJP', 'I-ADJP', 'B-ADVP', 'I-ADVP', 'B-CONJP', 'I-CONJP', 'B-INTJ', 'I-INTJ', 'B-LST', 'I-LST', 'B-NP', 'I-NP', 'B-PP', 'I-PP', 'B-PRT', 'I-PRT', 'B-SBAR', 'I-SBAR', 'B-UCP']\n",
      "['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC', 'B-MISC', 'I-MISC']\n",
      "['\"', \"''\", '#', '$', '(', ')', ',', '.', ':', '``', 'CC', 'CD', 'DT', 'EX', 'FW', 'IN', 'JJ', 'JJR', 'JJS', 'LS']\n"
     ]
    }
   ],
   "source": [
    "print(conll2003_dsd[\"train\"].features[\"chunk_tags\"].feature.names[:20])\n",
    "print(conll2003_dsd[\"train\"].features[\"ner_tags\"].feature.names[:20])\n",
    "print(conll2003_dsd[\"train\"].features[\"pos_tags\"].feature.names[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['EU', 'rejects', 'German', 'call', 'to', 'boycott', 'British', 'lamb', '.']\n",
      "[3, 0, 7, 0, 0, 0, 7, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "print(conll2003_dsd[\"train\"][0][\"tokens\"])\n",
    "print(conll2003_dsd[\"train\"][0][\"ner_tags\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC', 'B-MISC', 'I-MISC']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_names = conll2003_dsd[\"train\"].features[\"ner_tags\"].feature.names\n",
    "label_names"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's prepare both a Hugging Face `Dataset`s and `DataFrame`s for illustrating how each can be used in BLURR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 200\n",
      "1000 200\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tokens</th>\n",
       "      <th>pos_tags</th>\n",
       "      <th>chunk_tags</th>\n",
       "      <th>ner_tags</th>\n",
       "      <th>is_valid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11503</td>\n",
       "      <td>[Chile, has, proposed, a, resolution, ,, still, under, discussion, ,, that, would, impose, an, immediate, arms, embargo, on, Burundi, and, call, for, negotiations, .]</td>\n",
       "      <td>[22, 42, 40, 12, 21, 6, 30, 15, 21, 6, 43, 20, 37, 12, 16, 24, 21, 15, 22, 10, 37, 15, 24, 7]</td>\n",
       "      <td>[11, 21, 22, 11, 12, 0, 3, 13, 11, 0, 11, 21, 22, 11, 12, 12, 12, 13, 11, 0, 21, 13, 11, 0]</td>\n",
       "      <td>[5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0]</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>598</td>\n",
       "      <td>[(, Hungary, 1, (, 0-0, )]</td>\n",
       "      <td>[4, 22, 11, 4, 11, 5]</td>\n",
       "      <td>[0, 11, 12, 0, 11, 0]</td>\n",
       "      <td>[0, 5, 0, 0, 0, 0]</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10323</td>\n",
       "      <td>[Amr, Shabana, (, Egypt, ), beat, John, White, (, Australia, ), 10-15, 15-9]</td>\n",
       "      <td>[22, 22, 4, 22, 5, 37, 22, 22, 4, 22, 5, 11, 16]</td>\n",
       "      <td>[11, 12, 0, 11, 0, 21, 11, 12, 0, 11, 0, 11, 12]</td>\n",
       "      <td>[1, 2, 0, 5, 0, 0, 1, 2, 0, 5, 0, 0, 0]</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>122</td>\n",
       "      <td>[TUNIS, 1996-08-22]</td>\n",
       "      <td>[24, 11]</td>\n",
       "      <td>[11, 12]</td>\n",
       "      <td>[5, 0]</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3001</td>\n",
       "      <td>[The, PKK, often, uses, bases, in, northern, Iraq, in, its, fight, for, autonomy, or, independence, in, southeast, Turkey, .]</td>\n",
       "      <td>[12, 16, 30, 42, 24, 15, 16, 22, 15, 29, 21, 15, 21, 10, 21, 15, 16, 22, 7]</td>\n",
       "      <td>[11, 12, 3, 21, 11, 13, 11, 12, 13, 11, 12, 13, 11, 0, 11, 13, 11, 12, 0]</td>\n",
       "      <td>[0, 3, 0, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0]</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id  \\\n",
       "0  11503   \n",
       "1    598   \n",
       "2  10323   \n",
       "3    122   \n",
       "4   3001   \n",
       "\n",
       "                                                                                                                                                                   tokens  \\\n",
       "0  [Chile, has, proposed, a, resolution, ,, still, under, discussion, ,, that, would, impose, an, immediate, arms, embargo, on, Burundi, and, call, for, negotiations, .]   \n",
       "1                                                                                                                                              [(, Hungary, 1, (, 0-0, )]   \n",
       "2                                                                                            [Amr, Shabana, (, Egypt, ), beat, John, White, (, Australia, ), 10-15, 15-9]   \n",
       "3                                                                                                                                                     [TUNIS, 1996-08-22]   \n",
       "4                                           [The, PKK, often, uses, bases, in, northern, Iraq, in, its, fight, for, autonomy, or, independence, in, southeast, Turkey, .]   \n",
       "\n",
       "                                                                                        pos_tags  \\\n",
       "0  [22, 42, 40, 12, 21, 6, 30, 15, 21, 6, 43, 20, 37, 12, 16, 24, 21, 15, 22, 10, 37, 15, 24, 7]   \n",
       "1                                                                          [4, 22, 11, 4, 11, 5]   \n",
       "2                                               [22, 22, 4, 22, 5, 37, 22, 22, 4, 22, 5, 11, 16]   \n",
       "3                                                                                       [24, 11]   \n",
       "4                    [12, 16, 30, 42, 24, 15, 16, 22, 15, 29, 21, 15, 21, 10, 21, 15, 16, 22, 7]   \n",
       "\n",
       "                                                                                    chunk_tags  \\\n",
       "0  [11, 21, 22, 11, 12, 0, 3, 13, 11, 0, 11, 21, 22, 11, 12, 12, 12, 13, 11, 0, 21, 13, 11, 0]   \n",
       "1                                                                        [0, 11, 12, 0, 11, 0]   \n",
       "2                                             [11, 12, 0, 11, 0, 21, 11, 12, 0, 11, 0, 11, 12]   \n",
       "3                                                                                     [11, 12]   \n",
       "4                    [11, 12, 3, 21, 11, 13, 11, 12, 13, 11, 12, 13, 11, 0, 11, 13, 11, 12, 0]   \n",
       "\n",
       "                                                                   ner_tags  \\\n",
       "0  [5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0]   \n",
       "1                                                        [0, 5, 0, 0, 0, 0]   \n",
       "2                                   [1, 2, 0, 5, 0, 0, 1, 2, 0, 5, 0, 0, 0]   \n",
       "3                                                                    [5, 0]   \n",
       "4                 [0, 3, 0, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0]   \n",
       "\n",
       "   is_valid  \n",
       "0     False  \n",
       "1     False  \n",
       "2     False  \n",
       "3     False  \n",
       "4     False  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# build HF `Dataset` objects\n",
    "train_ds = conll2003_dsd[\"train\"].add_column(\"is_valid\", [False] * len(conll2003_dsd[\"train\"])).shuffle().select(range(1000))\n",
    "valid_ds = conll2003_dsd[\"validation\"].add_column(\"is_valid\", [True] * len(conll2003_dsd[\"validation\"])).shuffle().select(range(200))\n",
    "conll2003_ds = concatenate_datasets([train_ds, valid_ds])\n",
    "\n",
    "# build a `DataFrame` representation as well\n",
    "conll2003_df = pd.DataFrame(conll2003_ds)\n",
    "\n",
    "print(len(train_ds), len(valid_ds))\n",
    "print(len(conll2003_df[conll2003_df[\"is_valid\"] == False]), len(conll2003_df[conll2003_df[\"is_valid\"] == True]))\n",
    "conll2003_df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Base API\n",
    "\n",
    "A base collation function that works with a variety of input formats and pads inputs on-the-fly at batch time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "def get_task_hf_objects(\n",
    "    pretrained_model_name: str,\n",
    "    label_names: list = [\"O\", \"B-PER\", \"I-PER\", \"B-ORG\", \"I-ORG\", \"B-LOC\", \"I-LOC\", \"B-MISC\", \"I-MISC\"],\n",
    "    verbose: bool = False,\n",
    "):\n",
    "    model_cls = AutoModelForTokenClassification\n",
    "    n_labels = len(label_names)\n",
    "\n",
    "    hf_arch, hf_config, hf_tokenizer, hf_model = get_hf_objects(\n",
    "        pretrained_model_name, model_cls=model_cls, config_kwargs={\"num_labels\": n_labels}\n",
    "    )\n",
    "\n",
    "    if verbose:\n",
    "        hf_arch, type(hf_config), type(hf_tokenizer), type(hf_model)\n",
    "\n",
    "        print(\"=== config ===\")\n",
    "        print(f\"# of labels:\\t{hf_config.num_labels}\")\n",
    "        print(\"\")\n",
    "        print(\"=== tokenizer ===\")\n",
    "        print(f\"Vocab size:\\t\\t{hf_tokenizer.vocab_size}\")\n",
    "        print(f\"Max # of tokens:\\t{hf_tokenizer.model_max_length}\")\n",
    "        print(f\"Attributes expected by model in forward pass:\\t{hf_tokenizer.model_input_names}\")\n",
    "\n",
    "    return hf_arch, hf_config, hf_tokenizer, hf_model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Labeling Strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "class BaseLabelingStrategy:\n",
    "    def __init__(\n",
    "        self,\n",
    "        hf_tokenizer: PreTrainedTokenizerBase,\n",
    "        label_names: Optional[List[str]],\n",
    "        non_entity_label: str = \"O\",\n",
    "        ignore_token_id: int = CrossEntropyLossFlat().ignore_index,\n",
    "    ) -> None:\n",
    "        self.hf_tokenizer = hf_tokenizer\n",
    "        self.ignore_token_id = ignore_token_id\n",
    "        self.label_names = label_names\n",
    "        self.non_entity_label = non_entity_label\n",
    "\n",
    "    def align_labels_with_tokens(self, word_ids, word_labels):\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we include a `BaseLabelingStrategy` abstract class and several different strategies for assigning labels to your tokenized inputs. The \"only first token\" and \"B/I\" labeling strategies are discussed in the [\"Token Classification\"](https://huggingface.co/course/chapter7/2?fw=pt) section in part 7 of the Hugging Face's Transformers course."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "class OnlyFirstTokenLabelingStrategy(BaseLabelingStrategy):\n",
    "    \"\"\"\n",
    "    Only the first token of word is associated with the label (all other subtokens with the `ignore_index_id`). Works where labels\n",
    "    are Ids or strings (in the later case we'll use the `label_names` to look up it's Id)\n",
    "    \"\"\"\n",
    "\n",
    "    def align_labels_with_tokens(self, word_ids, word_labels):\n",
    "        new_labels = []\n",
    "        current_word = None\n",
    "        for word_id in word_ids:\n",
    "            if word_id != current_word:\n",
    "                # start of a new word\n",
    "                current_word = word_id\n",
    "                label = self.ignore_token_id if word_id is None else word_labels[word_id]\n",
    "                new_labels.append(label if isinstance(label, int) else self.label_names.index(label))\n",
    "            else:\n",
    "                # special token or another subtoken of current word\n",
    "                new_labels.append(self.ignore_token_id)\n",
    "\n",
    "        return new_labels\n",
    "\n",
    "\n",
    "class SameLabelLabelingStrategy(BaseLabelingStrategy):\n",
    "    \"\"\"\n",
    "    Every token associated with a given word is associated with the word's label. Works where labels\n",
    "    are Ids or strings (in the later case we'll use the `label_names` to look up it's Id)\n",
    "    \"\"\"\n",
    "\n",
    "    def align_labels_with_tokens(self, word_ids, word_labels):\n",
    "        new_labels = []\n",
    "        for word_id in word_ids:\n",
    "            if word_id == None:\n",
    "                new_labels.append(self.ignore_token_id)\n",
    "            else:\n",
    "                label = word_labels[word_id]\n",
    "                new_labels.append(label if isinstance(label, int) else self.label_names.index(label))\n",
    "\n",
    "        return new_labels\n",
    "\n",
    "\n",
    "class BILabelingStrategy(BaseLabelingStrategy):\n",
    "    \"\"\"\n",
    "    If using B/I labels, the first token assoicated to a given word gets the \"B\" label while all other tokens related\n",
    "    to that same word get \"I\" labels.  If \"I\" labels don't exist, this strategy behaves like the `OnlyFirstTokenLabelingStrategy`.\n",
    "    Works where labels are Ids or strings (in the later case we'll use the `label_names` to look up it's Id)\n",
    "    \"\"\"\n",
    "\n",
    "    def align_labels_with_tokens(self, word_ids, word_labels):\n",
    "        new_labels = []\n",
    "        current_word = None\n",
    "        for word_id in word_ids:\n",
    "            if word_id != current_word:\n",
    "                # start of a new word\n",
    "                current_word = word_id\n",
    "                label = self.ignore_token_id if word_id is None else word_labels[word_id]\n",
    "                new_labels.append(label if isinstance(label, int) else self.label_names.index(label))\n",
    "            elif word_id is None:\n",
    "                # special token\n",
    "                new_labels.append(self.ignore_token_id)\n",
    "            else:\n",
    "                # we're in the same word\n",
    "                label = word_labels[word_id]\n",
    "                label_name = self.label_names[label] if isinstance(label, int) else label\n",
    "\n",
    "                # append the I-{ENTITY} if it exists in `labels`, else default to the `same_label` strategy\n",
    "                iLabel = f\"I-{label_name[2:]}\"\n",
    "                new_labels.append(\n",
    "                    self.label_names.index(iLabel) if iLabel in self.label_names else self.label_names.index(self.non_entity_label)\n",
    "                )\n",
    "\n",
    "        return new_labels"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reconstructing inputs/labels\n",
    "\n",
    "The utility methods below allow blurr users to reconstruct the original word/label associations from the input_ids/label associations. For example, these are used in our token classification `show_batch` method below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "def get_token_labels_from_input_ids(\n",
    "    # A Hugging Face tokenizer\n",
    "    hf_tokenizer: PreTrainedTokenizerBase,\n",
    "    # List of input_ids for the tokens in a single piece of processed text\n",
    "    input_ids: List[int],\n",
    "    # List of label indexs for each token\n",
    "    token_label_ids: List[int],\n",
    "    # List of label names from witch the `label` indicies can be used to find the name of the label\n",
    "    vocab: List[str],\n",
    "    # The token ID that should be ignored when calculating the loss\n",
    "    ignore_token_id: int = CrossEntropyLossFlat().ignore_index,\n",
    "    # The token used to identifiy ignored tokens (default: [xIGNx])\n",
    "    ignore_token: str = \"[xIGNx]\",\n",
    ") -> List[Tuple[str, str]]:\n",
    "    \"\"\"\n",
    "    Given a list of input IDs, the label ID associated to each, and the labels vocab, this method will return a list of tuples whereby\n",
    "    each tuple defines the \"token\" and its label name. For example:\n",
    "    [('ĠWay', B-PER), ('de', B-PER), ('ĠGill', I-PER), ('iam', I-PER), ('Ġloves'), ('ĠHug', B-ORG), ('ging', B-ORG), ('ĠFace', I-ORG)]\n",
    "    \"\"\"\n",
    "    # convert ids to tokens\n",
    "    toks = hf_tokenizer.convert_ids_to_tokens(input_ids)\n",
    "    # align \"tokens\" with labels\n",
    "    tok_labels = [\n",
    "        (tok, ignore_token if label_id == ignore_token_id else vocab[label_id])\n",
    "        for tok_id, tok, label_id in zip(input_ids, toks, token_label_ids)\n",
    "        if tok_id not in hf_tokenizer.all_special_ids\n",
    "    ]\n",
    "    return tok_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== config ===\n",
      "# of labels:\t9\n",
      "\n",
      "=== tokenizer ===\n",
      "Vocab size:\t\t128000\n",
      "Max # of tokens:\t1000000000000000019884624838656\n",
      "Attributes expected by model in forward pass:\t['input_ids', 'token_type_ids', 'attention_mask']\n"
     ]
    }
   ],
   "source": [
    "hf_arch, hf_config, hf_tokenizer, hf_model = get_task_hf_objects(\"microsoft/deberta-v3-small\", label_names, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TESTS for align_labels_with_tokens()\n",
    "\n",
    "for idx in range(3):\n",
    "    raw_word_list = conll2003_df.iloc[idx][\"tokens\"]\n",
    "    raw_label_list = conll2003_df.iloc[idx][\"ner_tags\"]\n",
    "\n",
    "    be = hf_tokenizer(raw_word_list, is_split_into_words=True)\n",
    "    input_ids = be[\"input_ids\"]\n",
    "    targ_ids = [-100 if (word_id == None) else raw_label_list[word_id] for word_id in be.word_ids()]\n",
    "\n",
    "    tok_labels = get_token_labels_from_input_ids(hf_tokenizer, input_ids, targ_ids, label_names)\n",
    "\n",
    "    for tok_label, targ_id in zip(tok_labels, [label_id for label_id in targ_ids if label_id != -100]):\n",
    "        test_eq(tok_label[1], label_names[targ_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/ohmeow/blurr/blob/dev-3.0.0 #master/blurr/text/data/token_classification.py#L173){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### get_token_labels_from_input_ids\n",
       "\n",
       ">      get_token_labels_from_input_ids (hf_tokenizer:transformers.tokenization_u\n",
       ">                                       tils_base.PreTrainedTokenizerBase,\n",
       ">                                       input_ids:List[int],\n",
       ">                                       token_label_ids:List[int],\n",
       ">                                       vocab:List[str],\n",
       ">                                       ignore_token_id:int=-100,\n",
       ">                                       ignore_token:str='[xIGNx]')\n",
       "\n",
       "Given a list of input IDs, the label ID associated to each, and the labels vocab, this method will return a list of tuples whereby\n",
       "each tuple defines the \"token\" and its label name. For example:\n",
       "[('ĠWay', B-PER), ('de', B-PER), ('ĠGill', I-PER), ('iam', I-PER), ('Ġloves'), ('ĠHug', B-ORG), ('ging', B-ORG), ('ĠFace', I-ORG)]\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| hf_tokenizer | PreTrainedTokenizerBase |  | A Hugging Face tokenizer |\n",
       "| input_ids | List[int] |  | List of input_ids for the tokens in a single piece of processed text |\n",
       "| token_label_ids | List[int] |  | List of label indexs for each token |\n",
       "| vocab | List[str] |  | List of label names from witch the `label` indicies can be used to find the name of the label |\n",
       "| ignore_token_id | int | -100 | The token ID that should be ignored when calculating the loss |\n",
       "| ignore_token | str | [xIGNx] | The token used to identifiy ignored tokens (default: [xIGNx]) |\n",
       "| **Returns** | **List[Tuple[str, str]]** |  |  |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/ohmeow/blurr/blob/dev-3.0.0 #master/blurr/text/data/token_classification.py#L173){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### get_token_labels_from_input_ids\n",
       "\n",
       ">      get_token_labels_from_input_ids (hf_tokenizer:transformers.tokenization_u\n",
       ">                                       tils_base.PreTrainedTokenizerBase,\n",
       ">                                       input_ids:List[int],\n",
       ">                                       token_label_ids:List[int],\n",
       ">                                       vocab:List[str],\n",
       ">                                       ignore_token_id:int=-100,\n",
       ">                                       ignore_token:str='[xIGNx]')\n",
       "\n",
       "Given a list of input IDs, the label ID associated to each, and the labels vocab, this method will return a list of tuples whereby\n",
       "each tuple defines the \"token\" and its label name. For example:\n",
       "[('ĠWay', B-PER), ('de', B-PER), ('ĠGill', I-PER), ('iam', I-PER), ('Ġloves'), ('ĠHug', B-ORG), ('ging', B-ORG), ('ĠFace', I-ORG)]\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| hf_tokenizer | PreTrainedTokenizerBase |  | A Hugging Face tokenizer |\n",
       "| input_ids | List[int] |  | List of input_ids for the tokens in a single piece of processed text |\n",
       "| token_label_ids | List[int] |  | List of label indexs for each token |\n",
       "| vocab | List[str] |  | List of label names from witch the `label` indicies can be used to find the name of the label |\n",
       "| ignore_token_id | int | -100 | The token ID that should be ignored when calculating the loss |\n",
       "| ignore_token | str | [xIGNx] | The token used to identifiy ignored tokens (default: [xIGNx]) |\n",
       "| **Returns** | **List[Tuple[str, str]]** |  |  |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nbdev.show_doc(get_token_labels_from_input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "def get_word_labels_from_token_labels(\n",
    "    hf_arch: str,\n",
    "    # A Hugging Face tokenizer\n",
    "    hf_tokenizer: PreTrainedTokenizerBase,\n",
    "    # A list of tuples, where each represents a token and its label (e.g., [('ĠHug', B-ORG), ('ging', B-ORG), ('ĠFace', I-ORG), ...])\n",
    "    tok_labels,\n",
    ") -> List[Tuple[str, str]]:\n",
    "    \"\"\"\n",
    "    Given a list of tuples where each tuple defines a token and its label, return a list of tuples whereby each tuple defines the\n",
    "    \"word\" and its label. Method assumes that model inputs are a list of words, and in conjunction with the `align_labels_with_tokens` method,\n",
    "    allows the user to reconstruct the orginal raw inputs and labels.\n",
    "    \"\"\"\n",
    "    # recreate raw words list (we assume for token classification that the input is a list of words)\n",
    "    words = hf_tokenizer.convert_tokens_to_string([tok_label[0] for tok_label in tok_labels]).split()\n",
    "\n",
    "    if hf_arch == \"canine\":\n",
    "        word_list = [f\"{word} \" for word in words]\n",
    "    else:\n",
    "        word_list = [word for word in words]\n",
    "\n",
    "    # align \"words\" with labels\n",
    "    word_labels, idx = [], 0\n",
    "    for word in word_list:\n",
    "        word_labels.append((word, tok_labels[idx][1]))\n",
    "        idx += len(hf_tokenizer.tokenize(word))\n",
    "\n",
    "    return word_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== config ===\n",
      "# of labels:\t9\n",
      "\n",
      "=== tokenizer ===\n",
      "Vocab size:\t\t128000\n",
      "Max # of tokens:\t1000000000000000019884624838656\n",
      "Attributes expected by model in forward pass:\t['input_ids', 'token_type_ids', 'attention_mask']\n"
     ]
    }
   ],
   "source": [
    "hf_arch, hf_config, hf_tokenizer, hf_model = get_task_hf_objects(\"microsoft/deberta-v3-small\", label_names, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TESTS for align_labels_with_words()\n",
    "for idx in range(5):\n",
    "    raw_word_list = conll2003_df.iloc[idx][\"tokens\"]\n",
    "    raw_label_list = conll2003_df.iloc[idx][\"ner_tags\"]\n",
    "\n",
    "    be = hf_tokenizer(raw_word_list, is_split_into_words=True)\n",
    "    input_ids = be[\"input_ids\"]\n",
    "    targ_ids = [-100 if (word_id == None) else raw_label_list[word_id] for word_id in be.word_ids()]\n",
    "\n",
    "    tok_labels = get_token_labels_from_input_ids(hf_tokenizer, input_ids, targ_ids, label_names)\n",
    "    word_labels = get_word_labels_from_token_labels(hf_arch, hf_tokenizer, tok_labels)\n",
    "\n",
    "    for word_label, raw_word, raw_label_id in zip(word_labels, raw_word_list, raw_label_list):\n",
    "        test_eq(word_label[0], raw_word)\n",
    "        test_eq(word_label[1], label_names[raw_label_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/ohmeow/blurr/blob/dev-3.0.0 #master/blurr/text/data/token_classification.py#L203){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### get_word_labels_from_token_labels\n",
       "\n",
       ">      get_word_labels_from_token_labels (hf_arch:str, hf_tokenizer:transformers\n",
       ">                                         .tokenization_utils_base.PreTrainedTok\n",
       ">                                         enizerBase, tok_labels)\n",
       "\n",
       "Given a list of tuples where each tuple defines a token and its label, return a list of tuples whereby each tuple defines the\n",
       "\"word\" and its label. Method assumes that model inputs are a list of words, and in conjunction with the `align_labels_with_tokens` method,\n",
       "allows the user to reconstruct the orginal raw inputs and labels.\n",
       "\n",
       "|    | **Type** | **Details** |\n",
       "| -- | -------- | ----------- |\n",
       "| hf_arch | str |  |\n",
       "| hf_tokenizer | PreTrainedTokenizerBase | A Hugging Face tokenizer |\n",
       "| tok_labels |  | A list of tuples, where each represents a token and its label (e.g., [('ĠHug', B-ORG), ('ging', B-ORG), ('ĠFace', I-ORG), ...]) |\n",
       "| **Returns** | **List[Tuple[str, str]]** |  |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/ohmeow/blurr/blob/dev-3.0.0 #master/blurr/text/data/token_classification.py#L203){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### get_word_labels_from_token_labels\n",
       "\n",
       ">      get_word_labels_from_token_labels (hf_arch:str, hf_tokenizer:transformers\n",
       ">                                         .tokenization_utils_base.PreTrainedTok\n",
       ">                                         enizerBase, tok_labels)\n",
       "\n",
       "Given a list of tuples where each tuple defines a token and its label, return a list of tuples whereby each tuple defines the\n",
       "\"word\" and its label. Method assumes that model inputs are a list of words, and in conjunction with the `align_labels_with_tokens` method,\n",
       "allows the user to reconstruct the orginal raw inputs and labels.\n",
       "\n",
       "|    | **Type** | **Details** |\n",
       "| -- | -------- | ----------- |\n",
       "| hf_arch | str |  |\n",
       "| hf_tokenizer | PreTrainedTokenizerBase | A Hugging Face tokenizer |\n",
       "| tok_labels |  | A list of tuples, where each represents a token and its label (e.g., [('ĠHug', B-ORG), ('ging', B-ORG), ('ĠFace', I-ORG), ...]) |\n",
       "| **Returns** | **List[Tuple[str, str]]** |  |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nbdev.show_doc(get_word_labels_from_token_labels)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `TokenClassTextCollatorWithPadding` -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "@dataclass\n",
    "class TokenClassTextCollatorWithPadding(TextCollatorWithPadding):\n",
    "    def __init__(\n",
    "        self,\n",
    "        # A Hugging Face tokenizer\n",
    "        hf_tokenizer: PreTrainedTokenizerBase,\n",
    "        # The abbreviation/name of your Hugging Face transformer architecture (e.b., bert, bart, etc..)\n",
    "        hf_arch: str = None,\n",
    "        # A specific configuration instance you want to use\n",
    "        hf_config: PretrainedConfig = None,\n",
    "        # A Hugging Face model\n",
    "        hf_model: PreTrainedModel = None,\n",
    "        # The number of inputs expected by your model\n",
    "        n_inp: int = 1,\n",
    "        # The token ID that should be ignored when calculating the loss\n",
    "        ignore_token_id: int = CrossEntropyLossFlat().ignore_index,\n",
    "        # Defaults to use Hugging Face's DataCollatorWithPadding(tokenizer=hf_tokenizer)\n",
    "        data_collator_cls: type = DataCollatorWithPadding,\n",
    "        # kwyargs specific for the instantiation of the `data_collator`\n",
    "        data_collator_kwargs: dict = {},\n",
    "    ):\n",
    "        self.ignore_token_id = ignore_token_id\n",
    "\n",
    "        super().__init__(\n",
    "            hf_tokenizer=hf_tokenizer,\n",
    "            hf_arch=hf_arch,\n",
    "            hf_config=hf_config,\n",
    "            hf_model=hf_model,\n",
    "            n_inp=n_inp,\n",
    "            data_collator_cls=data_collator_cls,\n",
    "            data_collator_kwargs=data_collator_kwargs,\n",
    "        )\n",
    "\n",
    "    # used to give the labels/targets the right shape\n",
    "    def _proc_targets(self, inputs_d, targs):\n",
    "        # the code below comes pretty much straight from the `DataCollatorForTokenClassification` class\n",
    "        max_seq_length = np.max([len(input_ids) for input_ids in inputs_d[\"input_ids\"]])\n",
    "        padding_side = self.hf_tokenizer.padding_side\n",
    "\n",
    "        if padding_side == \"right\":\n",
    "            targs = [\n",
    "                (list(trg.numpy()) if torch.is_tensor(trg) else trg) + [self.ignore_token_id] * (max_seq_length - len(trg)) for trg in targs\n",
    "            ]\n",
    "        else:\n",
    "            targs = [\n",
    "                [self.ignore_token_id] * (max_seq_length - len(trg)) + (list(trg.numpy()) if torch.is_tensor(trg) else trg) for trg in targs\n",
    "            ]\n",
    "\n",
    "        if is_listy(targs[0]):\n",
    "            targs = torch.stack([tensor(lbls) for lbls in targs])\n",
    "        elif isinstance(targs[0], torch.Tensor) and len(targs[0].size()) > 0:\n",
    "            targs = torch.stack(targs)\n",
    "        else:\n",
    "            targs = torch.tensor(targs)\n",
    "\n",
    "        return targs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Base API: Examples\n",
    "\n",
    "This section demonstrates how you can use standard `Dataset` objects (PyTorch and Hugging Face) to build PyTorch `DataLoader`s"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1: HF objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== config ===\n",
      "# of labels:\t9\n",
      "\n",
      "=== tokenizer ===\n",
      "Vocab size:\t\t128000\n",
      "Max # of tokens:\t1000000000000000019884624838656\n",
      "Attributes expected by model in forward pass:\t['input_ids', 'token_type_ids', 'attention_mask']\n"
     ]
    }
   ],
   "source": [
    "hf_arch, hf_config, hf_tokenizer, hf_model = get_task_hf_objects(\"microsoft/deberta-v3-small\", label_names, verbose=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: `Dataset`s (PyTorch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train|Validation examples:  1000 200\n",
      "{'id': ['11503', '598'], 'tokens': [['Chile', 'has', 'proposed', 'a', 'resolution', ',', 'still', 'under', 'discussion', ',', 'that', 'would', 'impose', 'an', 'immediate', 'arms', 'embargo', 'on', 'Burundi', 'and', 'call', 'for', 'negotiations', '.'], ['(', 'Hungary', '1', '(', '0-0', ')']], 'pos_tags': [[22, 42, 40, 12, 21, 6, 30, 15, 21, 6, 43, 20, 37, 12, 16, 24, 21, 15, 22, 10, 37, 15, 24, 7], [4, 22, 11, 4, 11, 5]], 'chunk_tags': [[11, 21, 22, 11, 12, 0, 3, 13, 11, 0, 11, 21, 22, 11, 12, 12, 12, 13, 11, 0, 21, 13, 11, 0], [0, 11, 12, 0, 11, 0]], 'ner_tags': [[5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0], [0, 5, 0, 0, 0, 0]], 'is_valid': [False, False]}\n",
      "\n",
      "[['Chile', 'has', 'proposed', 'a', 'resolution', ',', 'still', 'under', 'discussion', ',', 'that', 'would', 'impose', 'an', 'immediate', 'arms', 'embargo', 'on', 'Burundi', 'and', 'call', 'for', 'negotiations', '.'], ['(', 'Hungary', '1', '(', '0-0', ')']]\n"
     ]
    }
   ],
   "source": [
    "print(\"Train|Validation examples: \", len(train_ds), len(valid_ds))\n",
    "\n",
    "print(train_ds[:2])\n",
    "print(\"\")\n",
    "print(train_ds[\"tokens\"][:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "010ba9f34d7441f9978607c98de38f70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "226b58aab6c547e494e7c712e7189dff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'input_ids': [1,\n",
       "  11588,\n",
       "  303,\n",
       "  2640,\n",
       "  266,\n",
       "  2946,\n",
       "  366,\n",
       "  449,\n",
       "  494,\n",
       "  2302,\n",
       "  366,\n",
       "  272,\n",
       "  338,\n",
       "  10708,\n",
       "  299,\n",
       "  3470,\n",
       "  3010,\n",
       "  32389,\n",
       "  277,\n",
       "  47079,\n",
       "  263,\n",
       "  660,\n",
       "  270,\n",
       "  6800,\n",
       "  323,\n",
       "  2],\n",
       " 'token_type_ids': [0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " 'attention_mask': [1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1],\n",
       " 'label': [-100,\n",
       "  5,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  5,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  -100]}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define our subword tokenized labeling strategy\n",
    "labeling_strat = BILabelingStrategy(hf_tokenizer=hf_tokenizer, label_names=label_names)\n",
    "\n",
    "# tokenize the dataset\n",
    "def tokenize_func(examples):\n",
    "    inputs = hf_tokenizer(examples[\"tokens\"], truncation=True, is_split_into_words=True)\n",
    "\n",
    "    all_labels = examples[\"ner_tags\"]\n",
    "    new_labels = []\n",
    "    for i, labels in enumerate(all_labels):\n",
    "        word_ids = inputs.word_ids(i)\n",
    "        new_labels.append(labeling_strat.align_labels_with_tokens(word_ids, labels))\n",
    "\n",
    "    inputs[\"label\"] = new_labels\n",
    "    return inputs\n",
    "\n",
    "\n",
    "proc_train_ds = train_ds.map(tokenize_func, batched=True, remove_columns=train_ds.column_names)\n",
    "proc_valid_ds = valid_ds.map(tokenize_func, batched=True, remove_columns=valid_ds.column_names)\n",
    "\n",
    "proc_train_ds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define our PyTorch Dataset class\n",
    "class HFTokenClassificationDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, hf_dataset, hf_tokenizer):\n",
    "        self.hf_dataset = hf_dataset\n",
    "        self.hf_tokenizer = hf_tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.hf_dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.hf_dataset[idx]\n",
    "        return item\n",
    "\n",
    "\n",
    "# build our PyTorch training and validation Datasets\n",
    "pt_proc_train_ds = HFTokenClassificationDataset(proc_train_ds, hf_tokenizer=hf_tokenizer)\n",
    "pt_proc_valid_ds = HFTokenClassificationDataset(proc_valid_ds, hf_tokenizer=hf_tokenizer)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3: `DataLoader`s  (PyTorch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build your fastai `DataLoaders` from Pytorch `DataLoader` objects\n",
    "batch_size = 4\n",
    "data_collator = TokenClassTextCollatorWithPadding(hf_tokenizer)\n",
    "train_dl = torch.utils.data.DataLoader(pt_proc_train_ds, batch_size=batch_size, shuffle=True, collate_fn=data_collator)\n",
    "valid_dl = torch.utils.data.DataLoader(pt_proc_valid_ds, batch_size=batch_size * 2, shuffle=False, collate_fn=data_collator)\n",
    "\n",
    "dls = DataLoaders(train_dl, valid_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250 25\n",
      "2\n",
      "\n",
      "[CLS] Weinstein was found dead last weekend alongside the bodies of eight-year-olds Julie Lejeune and Melissa Russo in a house belonging to Detroux, who said they starved to death earlier this year, nine months after being abducted in June 1995.[SEP]\n",
      "\n",
      "tensor([[-100,    1,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    1,    2,    0,    1,    2,    0,    0,    0,\n",
      "            0,    0,    1,    2,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "         -100],\n",
      "        [-100,    3,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100],\n",
      "        [-100,    3,    4,    0,    0,    0,    0, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100],\n",
      "        [-100,    0,    0,    0,    0,    7,    8,    8,    8,    0,    0,    0,\n",
      "            0,    0,    0, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100]])\n"
     ]
    }
   ],
   "source": [
    "print(len(train_dl), len(valid_dl))\n",
    "\n",
    "b = next(iter(train_dl))\n",
    "print(len(b))\n",
    "print(\"\")\n",
    "print(hf_tokenizer.decode(b[0][\"input_ids\"][0][:200]))\n",
    "print(\"\")\n",
    "print(b[1])\n",
    "\n",
    "# b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOPE: Won't work with PyTorch DataLoaders\n",
    "# AttributeError: 'DataLoader' object has no attribute 'show_batch'\n",
    "# dls.show_batch(dataloaders=dls, max_n=2, trunc_at=800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |echo:false\n",
    "try:\n",
    "    del dls, hf_model\n",
    "except:\n",
    "    pass\n",
    "finally:\n",
    "    clean_memory()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hugging Face"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1: HF objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== config ===\n",
      "# of labels:\t9\n",
      "\n",
      "=== tokenizer ===\n",
      "Vocab size:\t\t128000\n",
      "Max # of tokens:\t1000000000000000019884624838656\n",
      "Attributes expected by model in forward pass:\t['input_ids', 'token_type_ids', 'attention_mask']\n"
     ]
    }
   ],
   "source": [
    "hf_arch, hf_config, hf_tokenizer, hf_model = get_task_hf_objects(\"microsoft/deberta-v3-small\", label_names, verbose=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: `Datasets` (huggingface)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train|Validation examples:  1000 200\n",
      "{'id': ['11503', '598'], 'tokens': [['Chile', 'has', 'proposed', 'a', 'resolution', ',', 'still', 'under', 'discussion', ',', 'that', 'would', 'impose', 'an', 'immediate', 'arms', 'embargo', 'on', 'Burundi', 'and', 'call', 'for', 'negotiations', '.'], ['(', 'Hungary', '1', '(', '0-0', ')']], 'pos_tags': [[22, 42, 40, 12, 21, 6, 30, 15, 21, 6, 43, 20, 37, 12, 16, 24, 21, 15, 22, 10, 37, 15, 24, 7], [4, 22, 11, 4, 11, 5]], 'chunk_tags': [[11, 21, 22, 11, 12, 0, 3, 13, 11, 0, 11, 21, 22, 11, 12, 12, 12, 13, 11, 0, 21, 13, 11, 0], [0, 11, 12, 0, 11, 0]], 'ner_tags': [[5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0], [0, 5, 0, 0, 0, 0]], 'is_valid': [False, False]}\n",
      "\n",
      "[['Chile', 'has', 'proposed', 'a', 'resolution', ',', 'still', 'under', 'discussion', ',', 'that', 'would', 'impose', 'an', 'immediate', 'arms', 'embargo', 'on', 'Burundi', 'and', 'call', 'for', 'negotiations', '.'], ['(', 'Hungary', '1', '(', '0-0', ')']]\n"
     ]
    }
   ],
   "source": [
    "print(\"Train|Validation examples: \", len(train_ds), len(valid_ds))\n",
    "\n",
    "print(train_ds[:2])\n",
    "print(\"\")\n",
    "print(train_ds[\"tokens\"][:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/wgilliam/.cache/huggingface/datasets/conll2003/conll2003/1.0.0/9a4d16a94f8674ba3466315300359b0acd891b68b6c8743ddf60b9c702adce98/cache-931c67064e001979.arrow\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5e28c57a45a4b0c8bcdec9f3c5f70d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['input_ids', 'token_type_ids', 'attention_mask', 'label'],\n",
      "    num_rows: 1000\n",
      "})\n",
      "Dataset({\n",
      "    features: ['input_ids', 'token_type_ids', 'attention_mask', 'label'],\n",
      "    num_rows: 200\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# define our subword tokenized labeling strategy\n",
    "labeling_strat = BILabelingStrategy(hf_tokenizer=hf_tokenizer, label_names=label_names)\n",
    "\n",
    "# tokenize the dataset\n",
    "def tokenize_func(examples):\n",
    "    inputs = hf_tokenizer(examples[\"tokens\"], truncation=True, is_split_into_words=True)\n",
    "\n",
    "    all_labels = examples[\"ner_tags\"]\n",
    "    new_labels = []\n",
    "    for i, labels in enumerate(all_labels):\n",
    "        word_ids = inputs.word_ids(i)\n",
    "        new_labels.append(labeling_strat.align_labels_with_tokens(word_ids, labels))\n",
    "\n",
    "    inputs[\"label\"] = new_labels\n",
    "    return inputs\n",
    "\n",
    "\n",
    "proc_train_ds = train_ds.map(tokenize_func, batched=True, remove_columns=train_ds.column_names)\n",
    "proc_valid_ds = valid_ds.map(tokenize_func, batched=True, remove_columns=valid_ds.column_names)\n",
    "\n",
    "print(proc_train_ds)\n",
    "print(proc_valid_ds)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3: `DataLoader`s (PyTorch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build your fastai `DataLoaders` from Pytorch `DataLoader` objects\n",
    "batch_size = 4\n",
    "data_collator = TokenClassTextCollatorWithPadding(hf_tokenizer)\n",
    "train_dl = torch.utils.data.DataLoader(proc_train_ds, batch_size=batch_size, shuffle=True, collate_fn=data_collator)\n",
    "valid_dl = torch.utils.data.DataLoader(proc_valid_ds, batch_size=batch_size * 2, shuffle=False, collate_fn=data_collator)\n",
    "\n",
    "dls = DataLoaders(train_dl, valid_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250 25\n",
      "2\n",
      "\n",
      "[CLS] Palestinians reopened their shops on Thursday at the end of a four-hour strike called by President Yasser Arafat to protest against Israel's policy on Jewish settlements and Jerusalem, witnesses said.[SEP][PAD][PAD][PAD][PAD][PAD][PAD]\n",
      "\n",
      "tensor([[-100,    7,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    1,    2,    0,    0,    0,\n",
      "            5,    0,    0,    0,    0,    7,    0,    0,    5,    0,    0,    0,\n",
      "            0, -100, -100, -100, -100, -100, -100, -100],\n",
      "        [-100,    0,    0,    0,    5,    0,    5,    6,    6,    6,    6,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100],\n",
      "        [-100,    0,    5,    6,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            7,    8,    8,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0, -100],\n",
      "        [-100,    0,    0,    0,    0,    1,    2,    2,    2,    2,    0,    0,\n",
      "            5,    6,    6,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100]])\n"
     ]
    }
   ],
   "source": [
    "print(len(train_dl), len(valid_dl))\n",
    "\n",
    "b = next(iter(train_dl))\n",
    "print(len(b))\n",
    "print(\"\")\n",
    "print(hf_tokenizer.decode(b[0][\"input_ids\"][0][:200]))\n",
    "print(\"\")\n",
    "print(b[1])\n",
    "\n",
    "# b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |echo:false\n",
    "try:\n",
    "    del dls, hf_model\n",
    "except:\n",
    "    pass\n",
    "finally:\n",
    "    clean_memory()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Low-Level API\n",
    "\n",
    "This section demonstrates how you can migrate from using PyTorch/Hugging Face to fast.ai `Datasets` and `DataLoaders` to recapture much of the fast.ai specific features unavailable when using basic PyTorch."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `TokenClassTextInput` -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "class TokenClassTextInput(TextInput):\n",
    "    pass"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we define a custom class, `TokenClassTextInput`, for the `@typedispatched` methods to use so that we can override how token classification inputs/targets are assembled, as well as, how the data is shown via methods like `show_batch` and `show_results`."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `show_batch` -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "@typedispatch\n",
    "def show_batch(\n",
    "    # This typedispatched `show_batch` will be called for `TokenClassTextInput` typed inputs\n",
    "    x: TokenClassTextInput,\n",
    "    # Your targets\n",
    "    y,\n",
    "    # Your raw inputs/targets\n",
    "    samples,\n",
    "    # Your `DataLoaders`. This is required so as to get at the Hugging Face objects for\n",
    "    # decoding them into something understandable\n",
    "    dataloaders,\n",
    "    # Your `show_batch` context\n",
    "    ctxs=None,\n",
    "    # The maximum number of items to show\n",
    "    max_n=6,\n",
    "    # Any truncation your want applied to your decoded inputs\n",
    "    trunc_at=None,\n",
    "    # Any other keyword arguments you want applied to `show_batch`\n",
    "    **kwargs,\n",
    "):\n",
    "    # grab our tokenizer\n",
    "    tfm = first_blurr_tfm(dataloaders)\n",
    "    hf_arch, hf_tokenizer = tfm.hf_arch, tfm.hf_tokenizer\n",
    "\n",
    "    # if we've included our labels list, we'll use it to look up the value of our target(s)\n",
    "    trg_labels = tfm.kwargs[\"label_names\"] if (\"label_names\" in tfm.kwargs) else None\n",
    "    if trg_labels is None and dataloaders.vocab is not None:\n",
    "        trg_labels = dataloaders.vocab\n",
    "\n",
    "    res = L()\n",
    "    n_inp = dataloaders.n_inp\n",
    "\n",
    "    n_samples = min(max_n, dataloaders.bs)\n",
    "    for idx in range(n_samples):\n",
    "        input_ids = x[idx]\n",
    "        trgs = y[idx]\n",
    "        sample = samples[idx] if samples is not None else None\n",
    "\n",
    "        # align \"tokens\" with labels\n",
    "        tok_labels = get_token_labels_from_input_ids(hf_tokenizer, input_ids, trgs, trg_labels)\n",
    "        # align \"words\" with labels\n",
    "        word_labels = get_word_labels_from_token_labels(hf_arch, hf_tokenizer, tok_labels)\n",
    "        # stringify list of (word,label) for example\n",
    "        res.append([f\"{[ word_targ for idx, word_targ in enumerate(word_labels) if (trunc_at is None or idx < trunc_at) ]}\"])\n",
    "\n",
    "    display_df(pd.DataFrame(res, columns=[\"word / target label\"])[:max_n])\n",
    "    return ctxs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Low-Level API: Examples"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using fast.ai `Datasets` and `DataLoaders`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1: HF objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== config ===\n",
      "# of labels:\t9\n",
      "\n",
      "=== tokenizer ===\n",
      "Vocab size:\t\t128000\n",
      "Max # of tokens:\t1000000000000000019884624838656\n",
      "Attributes expected by model in forward pass:\t['input_ids', 'token_type_ids', 'attention_mask']\n"
     ]
    }
   ],
   "source": [
    "hf_arch, hf_config, hf_tokenizer, hf_model = get_task_hf_objects(\"microsoft/deberta-v3-small\", label_names, verbose=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: `Datasets` (fast.ai)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6043fffb4ac64727a10a77f81e6a2416",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# define our subword tokenized labeling strategy\n",
    "labeling_strat = BILabelingStrategy(hf_tokenizer=hf_tokenizer, label_names=label_names)\n",
    "\n",
    "# tokenize the dataset\n",
    "def tokenize_func(examples):\n",
    "    inputs = hf_tokenizer(examples[\"tokens\"], truncation=True, is_split_into_words=True)\n",
    "\n",
    "    all_labels = examples[\"ner_tags\"]\n",
    "    new_labels = []\n",
    "    for i, labels in enumerate(all_labels):\n",
    "        word_ids = inputs.word_ids(i)\n",
    "        new_labels.append(labeling_strat.align_labels_with_tokens(word_ids, labels))\n",
    "\n",
    "    inputs[\"label\"] = new_labels\n",
    "    return inputs\n",
    "\n",
    "\n",
    "proc_conll2003_ds = conll2003_ds.map(tokenize_func, batched=True)\n",
    "\n",
    "# turn Arrow into DataFrame (`ColSplitter` only works with `DataFrame`s)\n",
    "train_df = pd.DataFrame(proc_conll2003_ds)\n",
    "train_df.head()\n",
    "\n",
    "# define dataset splitter\n",
    "splitter = ColSplitter(\"is_valid\")\n",
    "splits = splitter(train_df)\n",
    "\n",
    "\n",
    "# define how we want to build our inputs and targets\n",
    "def _build_inputs(example):\n",
    "    return {fwd_arg_name: example[fwd_arg_name] for fwd_arg_name in hf_tokenizer.model_input_names if fwd_arg_name in list(example.keys())}\n",
    "\n",
    "\n",
    "def _build_targets(example):\n",
    "    return example[\"label\"]\n",
    "\n",
    "\n",
    "# create our fastai `Datasets` object\n",
    "dsets = Datasets(items=train_df, splits=splits, tfms=[[_build_inputs], _build_targets], n_inp=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Items in train|validation datasets:  1000 200\n",
      "Items in each example: 2\n",
      "Example inputs: ['input_ids', 'token_type_ids', 'attention_mask']\n",
      "Example target(s): [-100, 3, 0, 0, 0, 0, 0, -100]\n"
     ]
    }
   ],
   "source": [
    "print(\"Items in train|validation datasets: \", len(dsets.train), len(dsets.valid))\n",
    "\n",
    "example = dsets.valid[0]\n",
    "# example\n",
    "\n",
    "print(f\"Items in each example: {len(example)}\")\n",
    "print(f\"Example inputs: {list(example[0].keys())}\")\n",
    "print(f\"Example target(s): {example[1]}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3: `DataLoaders` (fast.ai)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = TokenClassTextCollatorWithPadding(hf_tokenizer)\n",
    "sort_func = partial(sorted_dl_func, hf_tokenizer=hf_tokenizer)\n",
    "\n",
    "dls = dsets.dataloaders(\n",
    "    batch_size=4,\n",
    "    create_batch=data_collator,\n",
    "    dl_type=partial(SortedDL, sort_func=sort_func),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250 50\n",
      "2\n",
      "\n",
      "[CLS] Compared with the end of last year, when T&N predicted a sluggish first half and a rebound later in 1996, Hope said : \" I think the difference ( now ) is the first half has not actually been as bad as some felt it was going to be, but equally we're certainly not predicting a recovery in the second half. \"[SEP]\n",
      "\n",
      "tensor([[-100,    0,    0,    0,    0,    0,    0,    0,    0,    0,    3,    4,\n",
      "            4,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    1,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0, -100],\n",
      "        [-100,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    5,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    7,    8,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100],\n",
      "        [-100,    0,    0,    0,    0,    3,    4,    0,    3,    4,    0,    0,\n",
      "            0,    0,    0,    0,    3,    4,    4,    4,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100],\n",
      "        [-100,    0,    0,    0,    0,    0,    0,    0,    0,    5,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    7,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(len(dls.train), len(dls.valid))\n",
    "\n",
    "b = next(iter(dls.train))\n",
    "print(len(b))\n",
    "print(\"\")\n",
    "print(hf_tokenizer.decode(b[0][\"input_ids\"][0][:200]))\n",
    "print(\"\")\n",
    "print(b[1])\n",
    "\n",
    "# b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |echo:false\n",
    "try:\n",
    "    del dls, hf_model\n",
    "except:\n",
    "    pass\n",
    "finally:\n",
    "    clean_memory()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using `BatchDecodeTransform` and `TextDataLoader`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1: HF objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== config ===\n",
      "# of labels:\t9\n",
      "\n",
      "=== tokenizer ===\n",
      "Vocab size:\t\t128000\n",
      "Max # of tokens:\t1000000000000000019884624838656\n",
      "Attributes expected by model in forward pass:\t['input_ids', 'token_type_ids', 'attention_mask']\n"
     ]
    }
   ],
   "source": [
    "hf_arch, hf_config, hf_tokenizer, hf_model = get_task_hf_objects(\"microsoft/deberta-v3-small\", label_names, verbose=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: `Datasets` (fast.ai)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/wgilliam/.cache/huggingface/datasets/conll2003/conll2003/1.0.0/9a4d16a94f8674ba3466315300359b0acd891b68b6c8743ddf60b9c702adce98/cache-93834033366613d9.arrow\n"
     ]
    }
   ],
   "source": [
    "# define our subword tokenized labeling strategy\n",
    "labeling_strat = BILabelingStrategy(hf_tokenizer=hf_tokenizer, label_names=label_names)\n",
    "\n",
    "# tokenize the dataset\n",
    "def tokenize_func(examples):\n",
    "    inputs = hf_tokenizer(examples[\"tokens\"], truncation=True, is_split_into_words=True)\n",
    "\n",
    "    all_labels = examples[\"ner_tags\"]\n",
    "    new_labels = []\n",
    "    for i, labels in enumerate(all_labels):\n",
    "        word_ids = inputs.word_ids(i)\n",
    "        new_labels.append(labeling_strat.align_labels_with_tokens(word_ids, labels))\n",
    "\n",
    "    inputs[\"label\"] = new_labels\n",
    "    return inputs\n",
    "\n",
    "\n",
    "proc_conll2003_ds = conll2003_ds.map(tokenize_func, batched=True)\n",
    "\n",
    "# define dataset splitter\n",
    "def _split_func(example):\n",
    "    return example[\"is_valid\"] == True\n",
    "\n",
    "\n",
    "splitter = FuncSplitter(_split_func)\n",
    "splits = splitter(proc_conll2003_ds)\n",
    "\n",
    "\n",
    "# define how we want to build our inputs and targets\n",
    "def _build_inputs(example):\n",
    "    return {fwd_arg_name: example[fwd_arg_name] for fwd_arg_name in hf_tokenizer.model_input_names if fwd_arg_name in list(example.keys())}\n",
    "\n",
    "\n",
    "def _build_targets(example):\n",
    "    return example[\"label\"]\n",
    "\n",
    "\n",
    "# create our fastai `Datasets` object\n",
    "dsets = Datasets(items=proc_conll2003_ds, splits=splits, tfms=[[_build_inputs], _build_targets], n_inp=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3: `DataLoaders` (blurr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "\n",
    "trn_dl = TextDataLoader(\n",
    "    dsets.train,\n",
    "    hf_tokenizer,\n",
    "    text_collator=TokenClassTextCollatorWithPadding(hf_tokenizer=hf_tokenizer),\n",
    "    batch_decode_kwargs={\"label_names\": label_names},\n",
    "    input_return_type=TokenClassTextInput,\n",
    "    shuffle=True,\n",
    "    batch_size=batch_size,\n",
    ")\n",
    "\n",
    "val_dl = TextDataLoader(\n",
    "    dsets.valid,\n",
    "    hf_tokenizer,\n",
    "    text_collator=TokenClassTextCollatorWithPadding(hf_tokenizer=hf_tokenizer),\n",
    "    input_return_type=TokenClassTextInput,\n",
    "    batch_decode_kwargs={\"label_names\": label_names},\n",
    "    batch_size=batch_size * 2,\n",
    ")\n",
    "\n",
    "dls = DataLoaders(trn_dl, val_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250 25\n",
      "2\n",
      "\n",
      "[CLS] 1 - Pete Sampras ( U.S. ) beat Jimy Szymanski ( Venezuela ) 6-2 6-2 6 - 1[SEP][PAD][PAD][PAD][PAD][PAD][PAD][PAD]\n",
      "\n",
      "tensor([[-100,    0,    0,    1,    2,    2,    2,    0,    5,    6,    6,    6,\n",
      "            0,    0,    1,    2,    2,    2,    2,    0,    5,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100],\n",
      "        [-100,    3,    0,    0,    0,    0,    0, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100],\n",
      "        [-100,    0,    0,    0,    0,    0,    0,    3,    0,    0,    0,    7,\n",
      "            0,    3,    4,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    1,    2,    0,    0,    7,    0,    0,    0,\n",
      "            0,    0, -100],\n",
      "        [-100,    0,    0,    0,    5,    0,    0,    0,    0,    1,    2,    2,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0, -100, -100, -100,\n",
      "         -100, -100, -100]])\n"
     ]
    }
   ],
   "source": [
    "print(len(dls.train), len(dls.valid))\n",
    "\n",
    "b = dls.train.one_batch()\n",
    "print(len(b))\n",
    "print(\"\")\n",
    "print(hf_tokenizer.decode(b[0][\"input_ids\"][0][:200]))\n",
    "print(\"\")\n",
    "print(b[1])\n",
    "\n",
    "# b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word / target label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[('1.', 'O'), ('Roberta', 'B-PER'), ('Brunet', 'I-PER'), ('(', 'O'), ('Italy', 'B-LOC'), (')', 'O'), ('14', 'O'), ('minutes', 'O'), ('48.96', 'O'), ('seconds', 'O')]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[('matches', 'O'), ('played', 'O'), ('at', 'O'), ('the', 'O'), ('weekend', 'O'), (':', 'O')]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dls.show_batch(dataloaders=dls, max_n=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |echo:false\n",
    "try:\n",
    "    del dls, hf_model\n",
    "except:\n",
    "    pass\n",
    "finally:\n",
    "    clean_memory()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mid-Level API\n",
    "\n",
    "BLURR's mid-level API provides a way to build your `DataLoaders` using fast.ai's mid-level `DataBlock` API.  \n",
    "\n",
    "BLURR supports three ways of doing this in the mid-level API: \n",
    "\n",
    "1. Using pre-tokenized data (the traditional approach)\n",
    "\n",
    "2. batch-time tokenization (the default approach in previous versions of blurr)\n",
    "\n",
    "2. item-time tokenization (e.g., to apply tokenization on individual items as they are pulled from their respective `Dataset`)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Targets -"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `TokenTensorCategory` -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "class TokenTensorCategory(TensorBase):\n",
    "    pass"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `TokenCategorize` -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "class TokenCategorize(Transform):\n",
    "    \"\"\"Reversible transform of a list of category string to `vocab` id\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        # The unique list of entities (e.g., B-LOC) (default: CategoryMap(vocab))\n",
    "        vocab: List[str] = None,\n",
    "        # The token used to identifiy ignored tokens (default: xIGNx)\n",
    "        ignore_token: str = \"[xIGNx]\",\n",
    "        # The token ID that should be ignored when calculating the loss (default: CrossEntropyLossFlat().ignore_index)\n",
    "        ignore_token_id: int = CrossEntropyLossFlat().ignore_index,\n",
    "    ):\n",
    "        self.vocab = None if vocab is None else CategoryMap(vocab, sort=False)\n",
    "        self.ignore_token, self.ignore_token_id = ignore_token, ignore_token_id\n",
    "\n",
    "        self.loss_func, self.order = CrossEntropyLossFlat(ignore_index=self.ignore_token_id), 1\n",
    "\n",
    "    def setups(self, dsets):\n",
    "        if self.vocab is None and dsets is not None:\n",
    "            self.vocab = CategoryMap(dsets)\n",
    "        self.c = len(self.vocab)\n",
    "\n",
    "    def encodes(self, labels):\n",
    "        # if `val` is the label name (e.g., B-PER, I-PER, etc...), lookup the corresponding index in the vocab using\n",
    "        # `self.vocab.o2i`\n",
    "        ids = [val if (isinstance(val, int)) else self.vocab.o2i[val] for val in labels]\n",
    "        return TokenTensorCategory(ids)\n",
    "\n",
    "    def decodes(self, encoded_labels):\n",
    "        return Category([(self.vocab[lbl_id]) for lbl_id in encoded_labels if lbl_id != self.ignore_token_id])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`TokenCategorize` modifies the fastai `Categorize` transform in a couple of ways.\n",
    "\n",
    "First, it allows your targets to consist of a `Category` *per* token, and second, it uses the idea of an `ignore_token_id` to mask subtokens that don't need a prediction. For example, the target of special tokens (e.g., pad, cls, sep) are set to `ignore_token_id` as are subsequent sub-tokens of a given token should more than 1 sub-token make it up."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `TokenCategoryBlock` -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "def TokenCategoryBlock(\n",
    "    # The unique list of entities (e.g., B-LOC) (default: CategoryMap(vocab))\n",
    "    vocab: Optional[List[str]] = None,\n",
    "    # The token used to identifiy ignored tokens (default: xIGNx)\n",
    "    ignore_token: str = \"[xIGNx]\",\n",
    "    # The token ID that should be ignored when calculating the loss (default: CrossEntropyLossFlat().ignore_index)\n",
    "    ignore_token_id: int = CrossEntropyLossFlat().ignore_index,\n",
    "):\n",
    "    \"\"\"`TransformBlock` for per-token categorical targets\"\"\"\n",
    "    return TransformBlock(type_tfms=TokenCategorize(vocab=vocab, ignore_token=ignore_token, ignore_token_id=ignore_token_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/ohmeow/blurr/blob/dev-3.0.0 #master/blurr/text/data/token_classification.py#L379){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### TokenCategoryBlock\n",
       "\n",
       ">      TokenCategoryBlock (vocab:Optional[List[str]]=None,\n",
       ">                          ignore_token:str='[xIGNx]', ignore_token_id:int=-100)\n",
       "\n",
       "`TransformBlock` for per-token categorical targets\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| vocab | Optional[List[str]] | None | The unique list of entities (e.g., B-LOC) (default: CategoryMap(vocab)) |\n",
       "| ignore_token | str | [xIGNx] | The token used to identifiy ignored tokens (default: xIGNx) |\n",
       "| ignore_token_id | int | -100 | The token ID that should be ignored when calculating the loss (default: CrossEntropyLossFlat().ignore_index) |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/ohmeow/blurr/blob/dev-3.0.0 #master/blurr/text/data/token_classification.py#L379){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### TokenCategoryBlock\n",
       "\n",
       ">      TokenCategoryBlock (vocab:Optional[List[str]]=None,\n",
       ">                          ignore_token:str='[xIGNx]', ignore_token_id:int=-100)\n",
       "\n",
       "`TransformBlock` for per-token categorical targets\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| vocab | Optional[List[str]] | None | The unique list of entities (e.g., B-LOC) (default: CategoryMap(vocab)) |\n",
       "| ignore_token | str | [xIGNx] | The token used to identifiy ignored tokens (default: xIGNx) |\n",
       "| ignore_token_id | int | -100 | The token ID that should be ignored when calculating the loss (default: CrossEntropyLossFlat().ignore_index) |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nbdev.show_doc(TokenCategoryBlock, title_level=3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inputs -"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `TokenClassBatchTokenizeTransform` - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "class TokenClassBatchTokenizeTransform(BatchTokenizeTransform):\n",
    "    def __init__(\n",
    "        self,\n",
    "        # The abbreviation/name of your Hugging Face transformer architecture (e.b., bert, bart, etc..)\n",
    "        hf_arch: str,\n",
    "        # A specific configuration instance you want to use\n",
    "        hf_config: PretrainedConfig,\n",
    "        # A Hugging Face tokenizer\n",
    "        hf_tokenizer: PreTrainedTokenizerBase,\n",
    "        # A Hugging Face model\n",
    "        hf_model: PreTrainedModel,\n",
    "        # To control whether the \"labels\" are included in your inputs. If they are, the loss will be calculated in\n",
    "        # the model's forward function and you can simply use `PreCalculatedLoss` as your `Learner`'s loss function to use it\n",
    "        include_labels: bool = True,\n",
    "        # The token ID that should be ignored when calculating the loss\n",
    "        ignore_token_id: int = CrossEntropyLossFlat().ignore_index,\n",
    "        # The labeling strategy you want to apply when associating labels with word tokens\n",
    "        labeling_strategy_cls: BaseLabelingStrategy = BILabelingStrategy,\n",
    "        # the target label names\n",
    "        target_label_names: Optional[List[str]] = None,\n",
    "        # the label for non-entity\n",
    "        non_entity_label: str = \"O\",\n",
    "        # To control the length of the padding/truncation. It can be an integer or None,\n",
    "        # in which case it will default to the maximum length the model can accept. If the model has no\n",
    "        # specific maximum input length, truncation/padding to max_length is deactivated.\n",
    "        # See [Everything you always wanted to know about padding and truncation](https://huggingface.co/transformers/preprocessing.html#everything-you-always-wanted-to-know-about-padding-and-truncation)\n",
    "        max_length: Optional[int] = None,\n",
    "        # To control the `padding` applied to your `hf_tokenizer` during tokenization. If None, will default to\n",
    "        # `False` or `'do_not_pad'.\n",
    "        # See [Everything you always wanted to know about padding and truncation](https://huggingface.co/transformers/preprocessing.html#everything-you-always-wanted-to-know-about-padding-and-truncation)\n",
    "        padding: Union[bool, str] = True,\n",
    "        # To control `truncation` applied to your `hf_tokenizer` during tokenization. If None, will default to\n",
    "        # `False` or `do_not_truncate`.\n",
    "        # See [Everything you always wanted to know about padding and truncation](https://huggingface.co/transformers/preprocessing.html#everything-you-always-wanted-to-know-about-padding-and-truncation)\n",
    "        truncation: Union[bool, str] = True,\n",
    "        # The `is_split_into_words` argument applied to your `hf_tokenizer` during tokenization. Set this to `True`\n",
    "        # if your inputs are pre-tokenized (not numericalized)\n",
    "        is_split_into_words: bool = True,\n",
    "        # If using a slow tokenizer, users will need to prove a `slow_word_ids_func` that accepts a\n",
    "        # tokenizzer, example index, and a batch encoding as arguments and in turn returnes the\n",
    "        # equavlient of fast tokenizer's `word_ids``\n",
    "        slow_word_ids_func: Optional[Callable] = None,\n",
    "        # Any other keyword arguments you want included when using your `hf_tokenizer` to tokenize your inputs\n",
    "        tok_kwargs: dict = {},\n",
    "        # Keyword arguments to apply to `TokenClassBatchTokenizeTransform`\n",
    "        **kwargs,\n",
    "    ):\n",
    "\n",
    "        super().__init__(\n",
    "            hf_arch,\n",
    "            hf_config,\n",
    "            hf_tokenizer,\n",
    "            hf_model,\n",
    "            include_labels=include_labels,\n",
    "            ignore_token_id=ignore_token_id,\n",
    "            max_length=max_length,\n",
    "            padding=padding,\n",
    "            truncation=truncation,\n",
    "            is_split_into_words=is_split_into_words,\n",
    "            tok_kwargs=tok_kwargs,\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "        self.target_label_names = target_label_names\n",
    "        self.non_entity_label = non_entity_label\n",
    "        self.slow_word_ids_func = slow_word_ids_func\n",
    "\n",
    "        self.labeling_strategy = labeling_strategy_cls(\n",
    "            hf_tokenizer, label_names=self.target_label_names, non_entity_label=self.non_entity_label, ignore_token_id=ignore_token_id\n",
    "        )\n",
    "\n",
    "    def encodes(self, samples, return_batch_encoding=False):\n",
    "        encoded_samples, inputs = super().encodes(samples, return_batch_encoding=True)\n",
    "\n",
    "        # if there are no targets (e.g., when used for inference)\n",
    "        if len(encoded_samples[0]) == 1:\n",
    "            return encoded_samples\n",
    "\n",
    "        # get the type of our targets (by default will be TokenTensorCategory)\n",
    "        target_cls = type(encoded_samples[0][1])\n",
    "\n",
    "        updated_samples = []\n",
    "        for idx, s in enumerate(encoded_samples):\n",
    "            # with batch-time tokenization, we have to align each token with the correct label using the `word_ids` in the\n",
    "            # batch encoding object we get from calling our *fast* tokenizer\n",
    "            word_ids = inputs.word_ids(idx) if self.hf_tokenizer.is_fast else self.slow_word_ids_func(self.hf_tokenizer, idx, inputs)\n",
    "            targ_ids = target_cls(self.labeling_strategy.align_labels_with_tokens(word_ids, s[-1].tolist()))\n",
    "\n",
    "            if self.include_labels and len(targ_ids) > 0:\n",
    "                s[0][\"label\"] = targ_ids\n",
    "\n",
    "            updated_samples.append((s[0], targ_ids))\n",
    "\n",
    "        if return_batch_encoding:\n",
    "            return updated_samples, inputs\n",
    "\n",
    "        return updated_samples"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`TokenClassBatchTokenizeTransform` is used to exclude any of the target's tokens we don't want to include in the loss calcuation (e.g. padding, cls, sep, etc...).\n",
    "\n",
    "Note also that we default `is_split_into_words = True` since token classification tasks expect a list of words and labels for each word."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mid-Level API: Examples"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pretokenized"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1: HF objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== config ===\n",
      "# of labels:\t9\n",
      "\n",
      "=== tokenizer ===\n",
      "Vocab size:\t\t128000\n",
      "Max # of tokens:\t1000000000000000019884624838656\n",
      "Attributes expected by model in forward pass:\t['input_ids', 'token_type_ids', 'attention_mask']\n"
     ]
    }
   ],
   "source": [
    "hf_arch, hf_config, hf_tokenizer, hf_model = get_task_hf_objects(\"microsoft/deberta-v3-small\", label_names, verbose=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Step 2: `DataBlock`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define DataBlock splitter\n",
    "def _split_func(example):\n",
    "    return example[\"is_valid\"] == True\n",
    "\n",
    "\n",
    "splitter = FuncSplitter(_split_func)\n",
    "\n",
    "\n",
    "# define how we want to build our targets\n",
    "# note: we don't need to define how to build our inputs because we're using an HF `Dataset` in this example\n",
    "def get_y(example):\n",
    "    return example[\"label\"]\n",
    "\n",
    "\n",
    "# define the DataBlock\n",
    "data_collator = TokenClassTextCollatorWithPadding(hf_tokenizer)\n",
    "\n",
    "txt_block = TextBlock(\n",
    "    hf_arch=hf_arch,\n",
    "    hf_config=hf_config,\n",
    "    hf_tokenizer=hf_tokenizer,\n",
    "    hf_model=hf_model,\n",
    "    input_return_type=TokenClassTextInput,\n",
    "    data_collator=data_collator,\n",
    "    batch_decode_kwargs={\"label_names\": label_names},\n",
    ")\n",
    "\n",
    "blocks = (txt_block, noop)\n",
    "dblock = DataBlock(blocks=blocks, get_y=get_y, splitter=splitter)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3: `DataLoaders`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/wgilliam/.cache/huggingface/datasets/conll2003/conll2003/1.0.0/9a4d16a94f8674ba3466315300359b0acd891b68b6c8743ddf60b9c702adce98/cache-93834033366613d9.arrow\n"
     ]
    }
   ],
   "source": [
    "# define our subword tokenized labeling strategy\n",
    "labeling_strat = BILabelingStrategy(hf_tokenizer=hf_tokenizer, label_names=label_names)\n",
    "\n",
    "# tokenize the dataset\n",
    "def tokenize_func(examples):\n",
    "    inputs = hf_tokenizer(examples[\"tokens\"], truncation=True, is_split_into_words=True)\n",
    "\n",
    "    all_labels = examples[\"ner_tags\"]\n",
    "    new_labels = []\n",
    "    for i, labels in enumerate(all_labels):\n",
    "        word_ids = inputs.word_ids(i)\n",
    "        new_labels.append(labeling_strat.align_labels_with_tokens(word_ids, labels))\n",
    "\n",
    "    inputs[\"label\"] = new_labels\n",
    "    return inputs\n",
    "\n",
    "\n",
    "proc_conll2003_ds = conll2003_ds.map(tokenize_func, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dls = dblock.dataloaders(proc_conll2003_ds, bs=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 4, torch.Size([4, 72]), torch.Size([4, 72]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = dls.one_batch()\n",
    "len(b), len(b[0][\"input_ids\"]), b[0][\"input_ids\"].shape, b[1].shape\n",
    "# b[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the actual types represented by our batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word / target label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[('Compared', 'O'), ('with', 'O'), ('the', 'O'), ('end', 'O'), ('of', 'O'), ('last', 'O'), ('year', 'O'), (',', 'O'), ('when', 'O'), ('T&amp;N', 'B-ORG'), ('predicted', 'O'), ('a', 'O'), ('sluggish', 'O'), ('first', 'O'), ('half', 'O'), ('and', 'O'), ('a', 'O'), ('rebound', 'O'), ('later', 'O'), ('in', 'O'), ('1996', 'O'), (',', 'O'), ('Hope', 'B-PER'), ('said', 'O'), (':', 'O'), ('\"', 'O'), ('I', 'O'), ('think', 'O'), ('the', 'O'), ('difference', 'O'), ('(', 'O'), ('now', 'O'), (')', 'O'), ('is', 'O'), ('the', 'O'), ('first', 'O'), ('half', 'O'), ('has', 'O'), ('not', 'O'), ('actually', 'O'), ('been', 'O'), ('as', 'O'), ('bad', 'O'), ('as', 'O'), ('some', 'O'), ('felt', 'O'), ('it', 'O'), ('was', 'O'), ('going', 'O'), ('to', 'O'), ('be', 'O'), (',', 'O'), ('but', 'O'), ('equally', 'O'), ('we', 'O'), (\"'re\", 'O'), ('certainly', 'O'), ('not', 'O'), ('predicting', 'O'), ('a', 'O'), ('recovery', 'O'), ('in', 'O'), ('the', 'O'), ('second', 'O'), ('half', 'O'), ('.', 'O'), ('\"', 'O')]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[('For', 'O'), ('the', 'O'), ('foreign', 'O'), ('powers', 'O'), ('which', 'O'), ('back', 'O'), ('last', 'O'), ('year', 'O'), (\"'s\", 'O'), ('Dayton', 'B-LOC'), ('peace', 'O'), ('agreement', 'O'), (',', 'O'), ('the', 'O'), ('main', 'O'), ('point', 'O'), ('of', 'O'), ('the', 'O'), ('election', 'O'), ('rules', 'O'), ('is', 'O'), ('that', 'O'), ('by', 'O'), ('voting', 'O'), ('as', 'O'), ('though', 'O'), ('they', 'O'), ('were', 'O'), ('still', 'O'), ('in', 'O'), ('their', 'O'), ('pre-war', 'O'), ('homes', 'O'), (',', 'O'), ('Bosnians', 'B-MISC'), ('should', 'O'), ('override', 'O'), ('the', 'O'), ('effects', 'O'), ('of', 'O'), ('ethnic', 'O'), ('cleansing', 'O'), ('and', 'O'), ('reassert', 'O'), ('the', 'O'), ('concept', 'O'), ('of', 'O'), ('a', 'O'), ('single', 'O'), ('multi-ethnic', 'O'), ('state', 'O'), ('.', 'O')]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dls.show_batch(dataloaders=dls, max_n=2, trunc_at=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |echo:false\n",
    "try:\n",
    "    del dls, hf_model\n",
    "except:\n",
    "    pass\n",
    "finally:\n",
    "    clean_memory()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch-Time Tokenization"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1: HF objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== config ===\n",
      "# of labels:\t9\n",
      "\n",
      "=== tokenizer ===\n",
      "Vocab size:\t\t128000\n",
      "Max # of tokens:\t1000000000000000019884624838656\n",
      "Attributes expected by model in forward pass:\t['input_ids', 'token_type_ids', 'attention_mask']\n"
     ]
    }
   ],
   "source": [
    "# | output: false\n",
    "hf_arch, hf_config, hf_tokenizer, hf_model = get_task_hf_objects(\"microsoft/deberta-v3-small\", label_names, verbose=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Step 2: `DataBlock`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenize_tfm = TokenClassBatchTokenizeTransform(\n",
    "    hf_arch, hf_config, hf_tokenizer, hf_model, labeling_strategy_cls=BILabelingStrategy, target_label_names=label_names\n",
    ")\n",
    "\n",
    "blocks = (TextBlock(tokenize_tfm=tokenize_tfm, input_return_type=TokenClassTextInput), TokenCategoryBlock(vocab=label_names))\n",
    "dblock = DataBlock(\n",
    "    blocks=blocks,\n",
    "    get_x=ColReader(\"tokens\"),\n",
    "    get_y=ColReader(\"ner_tags\"),\n",
    "    splitter=ColSplitter(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |hide\n",
    "# dblock.summary(conll2003_df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3: `DataLoaders`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dls = dblock.dataloaders(conll2003_df, bs=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 4, torch.Size([4, 72]), torch.Size([4, 72]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = dls.one_batch()\n",
    "len(b), len(b[0][\"input_ids\"]), b[0][\"input_ids\"].shape, b[1].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the actual types represented by our batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word / target label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[('Compared', 'O'), ('with', 'O'), ('the', 'O'), ('end', 'O'), ('of', 'O'), ('last', 'O'), ('year', 'O'), (',', 'O'), ('when', 'O'), ('T&amp;N', 'B-ORG'), ('predicted', 'O'), ('a', 'O'), ('sluggish', 'O'), ('first', 'O'), ('half', 'O'), ('and', 'O'), ('a', 'O'), ('rebound', 'O'), ('later', 'O'), ('in', 'O'), ('1996', 'O'), (',', 'O'), ('Hope', 'B-PER'), ('said', 'O'), (':', 'O'), ('\"', 'O'), ('I', 'O'), ('think', 'O'), ('the', 'O'), ('difference', 'O'), ('(', 'O'), ('now', 'O'), (')', 'O'), ('is', 'O'), ('the', 'O'), ('first', 'O'), ('half', 'O'), ('has', 'O'), ('not', 'O'), ('actually', 'O'), ('been', 'O'), ('as', 'O'), ('bad', 'O'), ('as', 'O'), ('some', 'O'), ('felt', 'O'), ('it', 'O'), ('was', 'O'), ('going', 'O'), ('to', 'O'), ('be', 'O'), (',', 'O'), ('but', 'O'), ('equally', 'O'), ('we', 'O'), (\"'re\", 'O'), ('certainly', 'O'), ('not', 'O'), ('predicting', 'O'), ('a', 'O'), ('recovery', 'O'), ('in', 'O'), ('the', 'O'), ('second', 'O'), ('half', 'O'), ('.', 'O'), ('\"', 'O')]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[('\"', 'O'), ('They', 'O'), ('asked', 'O'), ('me', 'O'), ('what', 'O'), ('would', 'O'), ('be', 'O'), ('the', 'O'), ('first', 'O'), ('thing', 'O'), ('I', 'O'), ('would', 'O'), ('do', 'O'), ('if', 'O'), ('I', 'O'), ('were', 'O'), ('president', 'O'), (',', 'O'), ('and', 'O'), ('I', 'O'), ('said', 'O'), ('the', 'O'), ('first', 'O'), ('thing', 'O'), ('I', 'O'), ('would', 'O'), ('do', 'O'), ('would', 'O'), ('be', 'O'), ('to', 'O'), ('resign', 'O'), ('straight', 'O'), ('away', 'O'), (',', 'O'), ('\"', 'O'), ('Archbishop', 'O'), ('Antonio', 'B-PER'), ('Quarracino', 'I-PER'), ('said', 'O'), ('at', 'O'), ('a', 'O'), ('sermon', 'O'), ('attended', 'O'), ('by', 'O'), ('several', 'O'), ('cabinet', 'O'), ('ministers', 'O'), ('.', 'O')]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dls.show_batch(dataloaders=dls, max_n=2, trunc_at=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |echo:false\n",
    "try:\n",
    "    del dls, hf_model\n",
    "except:\n",
    "    pass\n",
    "finally:\n",
    "    clean_memory()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Item-Time Tokenization"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1: HF objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== config ===\n",
      "# of labels:\t9\n",
      "\n",
      "=== tokenizer ===\n",
      "Vocab size:\t\t128000\n",
      "Max # of tokens:\t1000000000000000019884624838656\n",
      "Attributes expected by model in forward pass:\t['input_ids', 'token_type_ids', 'attention_mask']\n"
     ]
    }
   ],
   "source": [
    "hf_arch, hf_config, hf_tokenizer, hf_model = get_task_hf_objects(\"microsoft/deberta-v3-small\", label_names, verbose=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Step 2: `DataBlock`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenClassItemTokenizeTransform(ItemTransform):\n",
    "    split_idx = None\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        # A Hugging Face configuration object\n",
    "        hf_config: PretrainedConfig = None,\n",
    "        # A Hugging Face tokenizer\n",
    "        hf_tokenizer: PreTrainedTokenizerBase = None,\n",
    "        text_attr: str = \"text\",\n",
    "        labels_attr: str = \"labels\",\n",
    "        # The token ID that should be ignored when calculating the loss\n",
    "        ignore_token_id: int = CrossEntropyLossFlat().ignore_index,\n",
    "        # The labeling strategy you want to apply when associating labels with word tokens\n",
    "        labeling_strategy_cls: BaseLabelingStrategy = BILabelingStrategy,\n",
    "        # the target label names\n",
    "        target_label_names: Optional[List[str]] = None,\n",
    "        # the label for non-entity\n",
    "        non_entity_label: str = \"O\",\n",
    "        # Any keyword arguments you want your Hugging Face tokenizer to use during tokenization\n",
    "        tok_kwargs: dict = {},\n",
    "        # Any keyword arguments you want applied to `ItemTokenizeTransform`\n",
    "        **kwargs,\n",
    "    ) -> None:\n",
    "        store_attr()\n",
    "\n",
    "        if tok_kwargs.get(\"truncation\", None) is None:\n",
    "            self.tok_kwargs[\"truncation\"] = True\n",
    "        if tok_kwargs.get(\"max_length\", None) is None:\n",
    "            self.tok_kwargs[\"max_length\"] = True\n",
    "\n",
    "        self.labeling_strategy = labeling_strategy_cls(\n",
    "            hf_tokenizer, label_names=self.target_label_names, non_entity_label=self.non_entity_label, ignore_token_id=ignore_token_id\n",
    "        )\n",
    "\n",
    "    def encodes(self, example, **kwargs):\n",
    "        inputs = self.hf_tokenizer(example[self.text_attr], **self.tok_kwargs)\n",
    "        word_ids = inputs.word_ids() if self.hf_tokenizer.is_fast else self.slow_word_ids_func(self.hf_tokenizer, 0, inputs)\n",
    "        targ_ids = self.labeling_strategy.align_labels_with_tokens(word_ids, list(example[self.labels_attr]))\n",
    "\n",
    "        if len(targ_ids) > 0:\n",
    "            inputs[\"label\"] = targ_ids\n",
    "        return dict(inputs), targ_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfm = TokenClassItemTokenizeTransform(\n",
    "    hf_config=hf_config,\n",
    "    hf_tokenizer=hf_tokenizer,\n",
    "    text_attr=\"tokens\",\n",
    "    labels_attr=\"ner_tags\",\n",
    "    target_label_names=label_names,\n",
    "    tok_kwargs={\"is_split_into_words\": True},\n",
    ")\n",
    "tfm.split_idx = 0\n",
    "\n",
    "tfm2 = TokenClassItemTokenizeTransform(\n",
    "    hf_config=hf_config,\n",
    "    hf_tokenizer=hf_tokenizer,\n",
    "    text_attr=\"tokens\",\n",
    "    labels_attr=\"ner_tags\",\n",
    "    target_label_names=label_names,\n",
    "    tok_kwargs={\"is_split_into_words\": True},\n",
    ")\n",
    "tfm2.split_idx = 1\n",
    "\n",
    "\n",
    "def sorted_dl_func(\n",
    "    example,\n",
    "    # A Hugging Face tokenizer\n",
    "    hf_tokenizer: PreTrainedTokenizerBase,\n",
    "    # The `is_split_into_words` argument applied to your `hf_tokenizer` during tokenization. \\\n",
    "    # Set this to 'True' if your inputs are pre-tokenized (not numericalized)\n",
    "    is_split_into_words: bool = False,\n",
    "    # Any other keyword arguments you want to include during tokenization\n",
    "    tok_kwargs: dict = {},\n",
    "):\n",
    "    \"\"\"This method is used by the `SortedDL` to ensure your dataset is sorted *after* tokenization\"\"\"\n",
    "    txt = None\n",
    "    if isinstance(example[0], dict) or isinstance(example[0], tuple):\n",
    "        inp = example[0] if isinstance(example[0], dict) else example[0][0]\n",
    "        if \"input_ids\" in inp:\n",
    "            # if inputs are pretokenized\n",
    "            return len(inp[\"input_ids\"])\n",
    "        else:\n",
    "            txt = inp[\"text\"]\n",
    "    else:\n",
    "        txt = example[0]\n",
    "\n",
    "    return len(txt) if is_split_into_words else len(hf_tokenizer.tokenize(txt, **tok_kwargs))\n",
    "\n",
    "\n",
    "dl_type = partial(SortedDL, sort_func=partial(sorted_dl_func, hf_tokenizer=hf_tokenizer, is_split_into_words=True))\n",
    "\n",
    "blocks = (\n",
    "    TextBlock(\n",
    "        hf_arch=hf_arch,\n",
    "        hf_config=hf_config,\n",
    "        hf_tokenizer=hf_tokenizer,\n",
    "        hf_model=hf_model,\n",
    "        type_tfms=[tfm, tfm2],\n",
    "        data_collator=TokenClassTextCollatorWithPadding(hf_tokenizer),\n",
    "        input_return_type=TokenClassTextInput,\n",
    "        dl_type=dl_type,\n",
    "    ),\n",
    "    noop,\n",
    ")\n",
    "\n",
    "\n",
    "def _get_y(example):\n",
    "    return example\n",
    "\n",
    "\n",
    "dblock = DataBlock(blocks=blocks, splitter=ColSplitter(), n_inp=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3: `DataLoaders`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not do one pass in your dataloader, there is something wrong in it. Please see the stack trace below:\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'keys'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[145], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m dls \u001b[39m=\u001b[39m dblock\u001b[39m.\u001b[39;49mdataloaders(conll2003_df, bs\u001b[39m=\u001b[39;49m\u001b[39m4\u001b[39;49m)\n",
      "File \u001b[0;32m~/mambaforge/envs/blurr-v3/lib/python3.10/site-packages/fastai/data/block.py:157\u001b[0m, in \u001b[0;36mDataBlock.dataloaders\u001b[0;34m(self, source, path, verbose, **kwargs)\u001b[0m\n\u001b[1;32m    155\u001b[0m dsets \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdatasets(source, verbose\u001b[39m=\u001b[39mverbose)\n\u001b[1;32m    156\u001b[0m kwargs \u001b[39m=\u001b[39m {\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdls_kwargs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs, \u001b[39m'\u001b[39m\u001b[39mverbose\u001b[39m\u001b[39m'\u001b[39m: verbose}\n\u001b[0;32m--> 157\u001b[0m \u001b[39mreturn\u001b[39;00m dsets\u001b[39m.\u001b[39;49mdataloaders(path\u001b[39m=\u001b[39;49mpath, after_item\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mitem_tfms, after_batch\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbatch_tfms, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/mambaforge/envs/blurr-v3/lib/python3.10/site-packages/fastai/data/core.py:337\u001b[0m, in \u001b[0;36mFilteredBase.dataloaders\u001b[0;34m(self, bs, shuffle_train, shuffle, val_shuffle, n, path, dl_type, dl_kwargs, device, drop_last, val_bs, **kwargs)\u001b[0m\n\u001b[1;32m    335\u001b[0m dl \u001b[39m=\u001b[39m dl_type(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msubset(\u001b[39m0\u001b[39m), \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmerge(kwargs,def_kwargs, dl_kwargs[\u001b[39m0\u001b[39m]))\n\u001b[1;32m    336\u001b[0m def_kwargs \u001b[39m=\u001b[39m {\u001b[39m'\u001b[39m\u001b[39mbs\u001b[39m\u001b[39m'\u001b[39m:bs \u001b[39mif\u001b[39;00m val_bs \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m val_bs,\u001b[39m'\u001b[39m\u001b[39mshuffle\u001b[39m\u001b[39m'\u001b[39m:val_shuffle,\u001b[39m'\u001b[39m\u001b[39mn\u001b[39m\u001b[39m'\u001b[39m:\u001b[39mNone\u001b[39;00m,\u001b[39m'\u001b[39m\u001b[39mdrop_last\u001b[39m\u001b[39m'\u001b[39m:\u001b[39mFalse\u001b[39;00m}\n\u001b[0;32m--> 337\u001b[0m dls \u001b[39m=\u001b[39m [dl] \u001b[39m+\u001b[39m [dl\u001b[39m.\u001b[39mnew(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msubset(i), \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmerge(kwargs,def_kwargs,val_kwargs,dl_kwargs[i]))\n\u001b[1;32m    338\u001b[0m               \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_subsets)]\n\u001b[1;32m    339\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dbunch_type(\u001b[39m*\u001b[39mdls, path\u001b[39m=\u001b[39mpath, device\u001b[39m=\u001b[39mdevice)\n",
      "File \u001b[0;32m~/mambaforge/envs/blurr-v3/lib/python3.10/site-packages/fastai/data/core.py:337\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    335\u001b[0m dl \u001b[39m=\u001b[39m dl_type(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msubset(\u001b[39m0\u001b[39m), \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmerge(kwargs,def_kwargs, dl_kwargs[\u001b[39m0\u001b[39m]))\n\u001b[1;32m    336\u001b[0m def_kwargs \u001b[39m=\u001b[39m {\u001b[39m'\u001b[39m\u001b[39mbs\u001b[39m\u001b[39m'\u001b[39m:bs \u001b[39mif\u001b[39;00m val_bs \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m val_bs,\u001b[39m'\u001b[39m\u001b[39mshuffle\u001b[39m\u001b[39m'\u001b[39m:val_shuffle,\u001b[39m'\u001b[39m\u001b[39mn\u001b[39m\u001b[39m'\u001b[39m:\u001b[39mNone\u001b[39;00m,\u001b[39m'\u001b[39m\u001b[39mdrop_last\u001b[39m\u001b[39m'\u001b[39m:\u001b[39mFalse\u001b[39;00m}\n\u001b[0;32m--> 337\u001b[0m dls \u001b[39m=\u001b[39m [dl] \u001b[39m+\u001b[39m [dl\u001b[39m.\u001b[39;49mnew(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msubset(i), \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmerge(kwargs,def_kwargs,val_kwargs,dl_kwargs[i]))\n\u001b[1;32m    338\u001b[0m               \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_subsets)]\n\u001b[1;32m    339\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dbunch_type(\u001b[39m*\u001b[39mdls, path\u001b[39m=\u001b[39mpath, device\u001b[39m=\u001b[39mdevice)\n",
      "File \u001b[0;32m~/mambaforge/envs/blurr-v3/lib/python3.10/site-packages/fastai/text/data.py:218\u001b[0m, in \u001b[0;36mSortedDL.new\u001b[0;34m(self, dataset, **kwargs)\u001b[0m\n\u001b[1;32m    216\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mval_res\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m kwargs \u001b[39mand\u001b[39;00m kwargs[\u001b[39m'\u001b[39m\u001b[39mval_res\u001b[39m\u001b[39m'\u001b[39m] \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m: res \u001b[39m=\u001b[39m kwargs[\u001b[39m'\u001b[39m\u001b[39mval_res\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m    217\u001b[0m \u001b[39melse\u001b[39;00m: res \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mres \u001b[39mif\u001b[39;00m dataset \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 218\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mnew(dataset\u001b[39m=\u001b[39;49mdataset, res\u001b[39m=\u001b[39;49mres, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/mambaforge/envs/blurr-v3/lib/python3.10/site-packages/fastai/data/core.py:97\u001b[0m, in \u001b[0;36mTfmdDL.new\u001b[0;34m(self, dataset, cls, **kwargs)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mhasattr\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m_n_inp\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mhasattr\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m_types\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[1;32m     96\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 97\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_one_pass()\n\u001b[1;32m     98\u001b[0m         res\u001b[39m.\u001b[39m_n_inp,res\u001b[39m.\u001b[39m_types \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_n_inp,\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_types\n\u001b[1;32m     99\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e: \n",
      "File \u001b[0;32m~/mambaforge/envs/blurr-v3/lib/python3.10/site-packages/fastai/data/core.py:78\u001b[0m, in \u001b[0;36mTfmdDL._one_pass\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_one_pass\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m---> 78\u001b[0m     b \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdo_batch([\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdo_item(\u001b[39mNone\u001b[39;49;00m)])\n\u001b[1;32m     79\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m: b \u001b[39m=\u001b[39m to_device(b, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m     80\u001b[0m     its \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mafter_batch(b)\n",
      "File \u001b[0;32m~/mambaforge/envs/blurr-v3/lib/python3.10/site-packages/fastai/data/load.py:168\u001b[0m, in \u001b[0;36mDataLoader.do_batch\u001b[0;34m(self, b)\u001b[0m\n\u001b[0;32m--> 168\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdo_batch\u001b[39m(\u001b[39mself\u001b[39m, b): \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mretain(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcreate_batch(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbefore_batch(b)), b)\n",
      "File \u001b[0;32m~/development/projects/blurr/blurr/text/data/core.py:107\u001b[0m, in \u001b[0;36mTextCollatorWithPadding.__call__\u001b[0;34m(self, features)\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(features[\u001b[39m0\u001b[39m], \u001b[39mtuple\u001b[39m):\n\u001b[1;32m    106\u001b[0m     \u001b[39mfor\u001b[39;00m f_idx \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_inp):\n\u001b[0;32m--> 107\u001b[0m         feature_keys \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(features[\u001b[39m0\u001b[39;49m][f_idx]\u001b[39m.\u001b[39;49mkeys())\n\u001b[1;32m    108\u001b[0m         inputs\u001b[39m.\u001b[39mappend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_inputs_d(features\u001b[39m.\u001b[39mitemgot(f_idx), feature_keys))\n\u001b[1;32m    110\u001b[0m         input_labels \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_input_labels(inputs[\u001b[39m0\u001b[39m], features\u001b[39m.\u001b[39mitemgot(f_idx), feature_keys)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'keys'"
     ]
    }
   ],
   "source": [
    "dls = dblock.dataloaders(conll2003_df, bs=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dls' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[122], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m b \u001b[39m=\u001b[39m dls\u001b[39m.\u001b[39mone_batch()\n\u001b[1;32m      2\u001b[0m \u001b[39mlen\u001b[39m(b), \u001b[39mlen\u001b[39m(b[\u001b[39m0\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39minput_ids\u001b[39m\u001b[39m\"\u001b[39m]), b[\u001b[39m0\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39minput_ids\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mshape, b[\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39mshape\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dls' is not defined"
     ]
    }
   ],
   "source": [
    "b = dls.one_batch()\n",
    "len(b), len(b[0][\"input_ids\"]), b[0][\"input_ids\"].shape, b[1].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the actual types represented by our batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word / target label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[('Compared', 'O'), ('with', 'O'), ('the', 'O'), ('end', 'O'), ('of', 'O'), ('last', 'O'), ('year', 'O'), (',', 'O'), ('when', 'B-ORG'), ('T&amp;N', 'O'), ('predicted', 'O'), ('a', 'O'), ('sluggish', 'O'), ('first', 'O'), ('half', 'O'), ('and', 'O'), ('a', 'O'), ('rebound', 'O'), ('later', 'O'), ('in', 'B-PER'), ('1996', 'O'), (',', 'O'), ('Hope', 'O'), ('said', 'O'), (':', 'O'), ('\"', 'O'), ('I', 'O'), ('think', 'O'), ('the', 'O'), ('difference', 'O'), ('(', 'O'), ('now', 'O'), (')', 'O'), ('is', 'O'), ('the', 'O'), ('first', 'O'), ('half', 'O'), ('has', 'O'), ('not', 'O'), ('actually', 'O'), ('been', 'O'), ('as', 'O'), ('bad', 'O'), ('as', 'O'), ('some', 'O'), ('felt', 'O'), ('it', 'O'), ('was', 'O'), ('going', 'O'), ('to', 'O'), ('be', 'O'), (',', 'O'), ('but', 'O'), ('equally', 'O'), ('we', 'O'), (\"'re\", 'O'), ('certainly', 'O'), ('not', 'O'), ('predicting', 'O'), ('a', 'O'), ('recovery', 'O'), ('in', 'O'), ('the', 'O'), ('second', '[xIGNx]'), ('half', '[xIGNx]'), ('.', '[xIGNx]'), ('\"', '[xIGNx]')]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[('Ischinger', 'O'), ('said', 'O'), ('he', 'B-MISC'), ('met', 'O'), ('three', 'O'), ('Russian', 'O'), ('deputy', 'O'), ('foreign', 'O'), ('ministers', 'O'), ('and', 'O'), ('a', 'O'), ('vice', 'O'), ('defence', 'O'), ('minister', 'O'), (',', 'B-MISC'), ('who', 'O'), ('confirmed', 'O'), ('Russian', 'B-PER'), ('Foreign', 'I-PER'), ('Minister', 'O'), ('Yevgeny', 'O'), ('Primakov', 'O'), (\"'s\", 'O'), ('pledge', 'O'), ('that', 'O'), ('Moscow', 'O'), ('would', 'O'), ('seek', 'O'), ('a', 'O'), ('political', 'B-ORG'), ('solution', 'I-ORG'), ('under', 'I-ORG'), ('the', 'I-ORG'), ('aegis', 'I-ORG'), ('of', 'I-ORG'), ('the', 'I-ORG'), ('Organisation', 'O'), ('for', 'B-ORG'), ('Security', 'O'), ('and', 'O'), ('Cooperation', '[xIGNx]'), ('in', '[xIGNx]'), ('Europe', '[xIGNx]'), ('(', '[xIGNx]'), ('OSCE', '[xIGNx]'), (')', '[xIGNx]'), ('.', '[xIGNx]')]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dls.show_batch(dataloaders=dls, max_n=2, trunc_at=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |echo:false\n",
    "try:\n",
    "    del dls, hf_model\n",
    "except:\n",
    "    pass\n",
    "finally:\n",
    "    clean_memory()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tests\n",
    "\n",
    "The tests below to ensure the core DataBlock code above works for **all** pretrained sequence classification models available in Hugging Face.  These tests are excluded from the CI workflow because of how long they would take to run and the amount of data that would be required to download.\n",
    "\n",
    "**Note**: Feel free to modify the code below to test whatever pretrained classification models you are working with ... and if any of your pretrained sequence classification models fail, please submit a github issue *(or a PR if you'd like to fix it yourself)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "import nbdev\n",
    "\n",
    "nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
