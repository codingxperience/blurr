{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "skip_exec: true\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "\n",
    "> The `data.core` module contains the core bits required to use fast.ai's low-level and/or mid-level APIs to define `Datasets` and build `DataLoaders` suitable for training transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |default_exp data.core\n",
    "# |default_cls_lvl 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "from __future__ import annotations\n",
    "\n",
    "import gc, importlib, sys, traceback\n",
    "\n",
    "from accelerate.logging import get_logger\n",
    "from dataclasses import dataclass\n",
    "from dotenv import load_dotenv\n",
    "from fastai.imports import *\n",
    "from fastai.losses import CrossEntropyLossFlat\n",
    "from fastai.data.block import TransformBlock\n",
    "from fastai.data.transforms import DataLoaders, ItemTransform, Transform\n",
    "from fastai.text.data import SortedDL\n",
    "from fastai.torch_core import *\n",
    "from fastai.torch_imports import *\n",
    "from transformers import PretrainedConfig, PreTrainedTokenizerBase, PreTrainedModel, AutoModelForSequenceClassification\n",
    "from transformers import logging as hf_logging\n",
    "from transformers.data.data_collator import DataCollatorWithPadding\n",
    "\n",
    "from blurr.utils import get_hf_objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |hide\n",
    "import pdb, nbdev\n",
    "\n",
    "from datasets import Dataset\n",
    "from datasets import concatenate_datasets, load_dataset, Dataset, Value\n",
    "from fastai.data.block import CategoryBlock, ColReader, ColSplitter, DataBlock, FuncSplitter, MultiCategoryBlock\n",
    "from fastai.data.transforms import DataLoader, DataLoaders, Datasets, ItemTransform\n",
    "from fastai.losses import BaseLoss, BCEWithLogitsLossFlat\n",
    "from fastcore.test import *\n",
    "from transformers import AutoConfig, AutoTokenizer\n",
    "\n",
    "from blurr.utils import clean_memory, print_versions, set_seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# |export\n",
    "# silence all the HF warnings and load environment variables\n",
    "warnings.simplefilter(\"ignore\")\n",
    "hf_logging.set_verbosity_error()\n",
    "logger = get_logger(__name__)\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU #0: NVIDIA GeForce RTX 3090\n"
     ]
    }
   ],
   "source": [
    "# |hide\n",
    "# |notest\n",
    "torch.cuda.set_device(0)\n",
    "print(f\"Using GPU #{torch.cuda.current_device()}: {torch.cuda.get_device_name()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What we're running with at the time this documentation was generated:\n",
      "torch: 1.13.1\n",
      "fastai: 2.7.11\n",
      "transformers: 4.26.1\n"
     ]
    }
   ],
   "source": [
    "# | echo: false\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "print(\"What we're running with at the time this documentation was generated:\")\n",
    "print_versions(\"torch fastai transformers\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "We'll use a subset of `imdb` to demonstrate how to configure your BLURR for sequence classification tasks. **BLURR** is designed to work with Hugging Face `Dataset` and/or pandas `DataFrame` objects"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset imdb (/home/wgilliam/.cache/huggingface/datasets/imdb/plain_text/1.0.0/d613c88cf8fa3bab83b4ded3713f1f74830d1100e171db75bbddb80b3345c9c0)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d00d9a0733dd48fab7f218129eea14cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 200\n",
      "1000 200\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>is_valid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I've read a number of reviews on this film and I have to say \"What is wrong with you people?!?!\" This was an excellent film! I thought this film was superb from start to finish and the story was extremely well told. I'm convinced that the people that didn't like this film weren't paying very good attention to the film. There are a number of very important scenes that if you aren't paying attention you will be confused and the following scenes may not make sense. I urge anyone who didn't like this film to watch it again and watch it alone so that you can truly pay attention. The story made ...</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Bugs life is a good film. But to me, it doesn't really compare to movies like Toy story and stuff. Don't get me wrong, I liked this movie, but it wasn't as good as Toy story. The film has the visuals, the laughs, and others that Toy story had. But the film didn't feel quite as... I don't know, but I thought it was still a pretty good film. &lt;br /&gt;&lt;br /&gt;A bugs life... I don't want to say this, is a film that I don't remember. I saw it years ago. Of course, I haven't seen Toy story in years, but I still remember it. I shouldn't have reviewed this film, but I am. I am giving it a thumbs up, th...</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sorry did i miss something? did i walk out early? The first ten minutes of unusual (and untrue!) stories had me thinking \"This is going to be a classic\" But it was all down hill from there! The acting was brilliant, for what it's worth William H Macy is fantastic and just gets better and better every film i watch him in. But it never seemed to connect. I was waiting for the big moment where all the stories inter connect and then suddenly..it rains frog?? it was if the writer said \"i've gone to deep how can i pull all these stories together cleverely....Oh sod it i'll just have it raining f...</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I think its pretty safe to say that this is the worst film ever made, When I saw the trailer on TV i knew right from second 1 that this would be a piece of **** and it would be best to avoid it, but I somehow got dragged into seeing this by some friends, I walked into the cinema with low expectations but i was hoping there would be a couple of cheap laughs to keep me awake during this film. The so-called \"jokes\" in this film bring a cringe to the face, they are mostly comprised of people taking hits to the face and balls, the baby looking weird and acting like a horny gangsta and the typic...</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(This is a review of the later English release by Disney, featuring Alison Lohman, Patrick Stewart, and co.) &lt;br /&gt;&lt;br /&gt;I really wanted this film to be good. Really, really. I'm a huge fan of Princess Mononoke and Spirited Away, and after seeing all the glowing reviews on this earlier Miyazaki film, I was eager to see it. But I was shocked, shocked I say, at the quality of this film. Those later films boast well-crafted plots, 3-dimensional characters, and the best film music since...well...ever. This film just doesn't come close.&lt;br /&gt;&lt;br /&gt;Might as well start w/ the positive aspects, th...</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      text  \\\n",
       "0  I've read a number of reviews on this film and I have to say \"What is wrong with you people?!?!\" This was an excellent film! I thought this film was superb from start to finish and the story was extremely well told. I'm convinced that the people that didn't like this film weren't paying very good attention to the film. There are a number of very important scenes that if you aren't paying attention you will be confused and the following scenes may not make sense. I urge anyone who didn't like this film to watch it again and watch it alone so that you can truly pay attention. The story made ...   \n",
       "1  Bugs life is a good film. But to me, it doesn't really compare to movies like Toy story and stuff. Don't get me wrong, I liked this movie, but it wasn't as good as Toy story. The film has the visuals, the laughs, and others that Toy story had. But the film didn't feel quite as... I don't know, but I thought it was still a pretty good film. <br /><br />A bugs life... I don't want to say this, is a film that I don't remember. I saw it years ago. Of course, I haven't seen Toy story in years, but I still remember it. I shouldn't have reviewed this film, but I am. I am giving it a thumbs up, th...   \n",
       "2  Sorry did i miss something? did i walk out early? The first ten minutes of unusual (and untrue!) stories had me thinking \"This is going to be a classic\" But it was all down hill from there! The acting was brilliant, for what it's worth William H Macy is fantastic and just gets better and better every film i watch him in. But it never seemed to connect. I was waiting for the big moment where all the stories inter connect and then suddenly..it rains frog?? it was if the writer said \"i've gone to deep how can i pull all these stories together cleverely....Oh sod it i'll just have it raining f...   \n",
       "3  I think its pretty safe to say that this is the worst film ever made, When I saw the trailer on TV i knew right from second 1 that this would be a piece of **** and it would be best to avoid it, but I somehow got dragged into seeing this by some friends, I walked into the cinema with low expectations but i was hoping there would be a couple of cheap laughs to keep me awake during this film. The so-called \"jokes\" in this film bring a cringe to the face, they are mostly comprised of people taking hits to the face and balls, the baby looking weird and acting like a horny gangsta and the typic...   \n",
       "4  (This is a review of the later English release by Disney, featuring Alison Lohman, Patrick Stewart, and co.) <br /><br />I really wanted this film to be good. Really, really. I'm a huge fan of Princess Mononoke and Spirited Away, and after seeing all the glowing reviews on this earlier Miyazaki film, I was eager to see it. But I was shocked, shocked I say, at the quality of this film. Those later films boast well-crafted plots, 3-dimensional characters, and the best film music since...well...ever. This film just doesn't come close.<br /><br />Might as well start w/ the positive aspects, th...   \n",
       "\n",
       "   label  is_valid  \n",
       "0      1     False  \n",
       "1      1     False  \n",
       "2      0     False  \n",
       "3      0     False  \n",
       "4      0     False  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb_dsd = load_dataset(\"imdb\", split=[\"train\", \"test\"])\n",
    "\n",
    "# build HF `Dataset` objects\n",
    "train_ds = imdb_dsd[0].add_column(\"is_valid\", [False] * len(imdb_dsd[0])).shuffle().select(range(1000))\n",
    "valid_ds = imdb_dsd[1].add_column(\"is_valid\", [True] * len(imdb_dsd[1])).shuffle().select(range(200))\n",
    "imdb_ds = concatenate_datasets([train_ds, valid_ds])\n",
    "\n",
    "# build a `DataFrame` representation as well\n",
    "imdb_df = pd.DataFrame(imdb_ds)\n",
    "\n",
    "print(len(train_ds), len(valid_ds))\n",
    "print(len(imdb_df[imdb_df[\"is_valid\"] == False]), len(imdb_df[imdb_df[\"is_valid\"] == True]))\n",
    "imdb_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['neg', 'pos']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_names = imdb_dsd[0].features[\"label\"].names\n",
    "label_names"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multilabel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset civil_comments (/home/wgilliam/.cache/huggingface/datasets/civil_comments/default/0.9.0/e7a3aacd2ab7d135fa958e7209d10b1fa03807d44c486e3c34897aa08ea8ffab)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f0a73a7bdb843d4b4286b62b08e1d77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aacc5e4c3cff4388b527fe67a3f14e7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad37b7cdafcb452187e7ac6304c58e6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Casting the dataset:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc6cfd365d1649cda122e4383da0a32c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18c76efe1e8a4fafa26b2d86d348ea91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Casting the dataset:   0%|          | 0/200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 200\n",
      "1000 200\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>toxicity</th>\n",
       "      <th>severe_toxicity</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_attack</th>\n",
       "      <th>sexual_explicit</th>\n",
       "      <th>is_valid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>We live in an age of hysteria and histrionics. Critical, rational thinking is needed to deal with the deluge of propaganda emanating from the Grope &amp; Flail and the Toronto Red Star. Head for the hills!!</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>No excuse for poaching, no matter who you are... native, white, half white, half native, whatever.</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Foreign Affairs Minister Chrystia Freeland says the Liberal government will make a “substantial investment” in the military because Canada can no longer rely on Washington for global leadership \\n-\\nsubstantial investment?\\njoke?\\nTrudeau Canada cannot even meet its NATO commitments of 2% of GDP for defense (spending less than 1%)\\ntruly Liberal hot air</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Relying on others.   It must be Liberal thing.</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Let's be fair -- if there's one thing that unites people of all lands, of all colours and creeds, it's the thrill of beholding a giant rubber duck. Some might feel fondness and some even arousal (stay away from these), but all will cherish the bizarre memory.\\nAlas, the more suspicious types might worry that it is in fact a Trumpian Trojan Horse, crammed with his northern militia, his online fifth column, the tiny one that usually hangs out at the online Globe.</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                text  \\\n",
       "0                                                                                                                                                                                                                                                                         We live in an age of hysteria and histrionics. Critical, rational thinking is needed to deal with the deluge of propaganda emanating from the Grope & Flail and the Toronto Red Star. Head for the hills!!   \n",
       "1                                                                                                                                                                                                                                                                                                                                                                                 No excuse for poaching, no matter who you are... native, white, half white, half native, whatever.   \n",
       "2                                                                                                                Foreign Affairs Minister Chrystia Freeland says the Liberal government will make a “substantial investment” in the military because Canada can no longer rely on Washington for global leadership \\n-\\nsubstantial investment?\\njoke?\\nTrudeau Canada cannot even meet its NATO commitments of 2% of GDP for defense (spending less than 1%)\\ntruly Liberal hot air   \n",
       "3                                                                                                                                                                                                                                                                                                                                                                                                                                     Relying on others.   It must be Liberal thing.   \n",
       "4  Let's be fair -- if there's one thing that unites people of all lands, of all colours and creeds, it's the thrill of beholding a giant rubber duck. Some might feel fondness and some even arousal (stay away from these), but all will cherish the bizarre memory.\\nAlas, the more suspicious types might worry that it is in fact a Trumpian Trojan Horse, crammed with his northern militia, his online fifth column, the tiny one that usually hangs out at the online Globe.   \n",
       "\n",
       "   toxicity  severe_toxicity  obscene  threat  insult  identity_attack  \\\n",
       "0         0                0        0       0       0                0   \n",
       "1         0                0        0       0       0                0   \n",
       "2         0                0        0       0       0                0   \n",
       "3         0                0        0       0       0                0   \n",
       "4         0                0        0       0       0                0   \n",
       "\n",
       "   sexual_explicit  is_valid  \n",
       "0                0     False  \n",
       "1                0     False  \n",
       "2                0     False  \n",
       "3                0     False  \n",
       "4                0     False  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "civil_dsd = load_dataset(\"civil_comments\", split=[\"train\", \"validation\"])\n",
    "\n",
    "# round the floats\n",
    "civil_label_names = [\"toxicity\", \"severe_toxicity\", \"obscene\", \"threat\", \"insult\", \"identity_attack\", \"sexual_explicit\"]\n",
    "\n",
    "\n",
    "def round_targs(example):\n",
    "    for lbl in civil_label_names:\n",
    "        example[lbl] = np.round(example[lbl])\n",
    "    return example\n",
    "\n",
    "\n",
    "# convert floats to ints\n",
    "def fix_dtypes(ds):\n",
    "    new_features = ds.features.copy()\n",
    "    for lbl in civil_label_names:\n",
    "        new_features[lbl] = Value(\"int32\")\n",
    "    return ds.cast(new_features)\n",
    "\n",
    "\n",
    "# build HF `Dataset` objects\n",
    "civil_train_ds = civil_dsd[0].add_column(\"is_valid\", [False] * len(civil_dsd[0])).shuffle().select(range(1000))\n",
    "civil_train_ds = civil_train_ds.map(round_targs)\n",
    "civil_train_ds = fix_dtypes(civil_train_ds)\n",
    "\n",
    "civil_valid_ds = civil_dsd[1].add_column(\"is_valid\", [True] * len(civil_dsd[1])).shuffle().select(range(200))\n",
    "civil_valid_ds = civil_valid_ds.map(round_targs)\n",
    "civil_valid_ds = fix_dtypes(civil_valid_ds)\n",
    "\n",
    "civil_ds = concatenate_datasets([civil_train_ds, civil_valid_ds])\n",
    "\n",
    "# build a `DataFrame` representation as well\n",
    "civil_df = pd.DataFrame(civil_ds)\n",
    "\n",
    "print(len(civil_train_ds), len(civil_valid_ds))\n",
    "print(len(civil_df[civil_df[\"is_valid\"] == False]), len(civil_df[civil_df[\"is_valid\"] == True]))\n",
    "civil_df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Base API"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task specific functions\n",
    "\n",
    "The below functions provide a basic way to fetch your Hugging Face objects and pretokenize your inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "def get_task_hf_objects(pretrained_model_name: str, label_names: list[str] = [\"neg\", \"pos\"], verbose: bool = False):\n",
    "    \"\"\"A helper function for getting the Hugging Face objects that works out of the box for most sequence classification tasks\"\"\"\n",
    "    model_cls = AutoModelForSequenceClassification\n",
    "    n_label_names = len(label_names)\n",
    "\n",
    "    hf_arch, hf_config, hf_tokenizer, hf_model = get_hf_objects(\n",
    "        pretrained_model_name, model_cls=model_cls, config_kwargs={\"num_labels\": n_label_names}\n",
    "    )\n",
    "\n",
    "    if verbose:\n",
    "        hf_arch, type(hf_config), type(hf_tokenizer), type(hf_model)\n",
    "\n",
    "        print(\"=== config ===\")\n",
    "        print(f\"# of labels:\\t{hf_config.num_labels}\")\n",
    "        print(\"\")\n",
    "        print(\"=== tokenizer ===\")\n",
    "        print(f\"Vocab size:\\t\\t{hf_tokenizer.vocab_size}\")\n",
    "        print(f\"Max # of tokens:\\t{hf_tokenizer.model_max_length}\")\n",
    "        print(f\"Attributes expected by model in forward pass:\\t{hf_tokenizer.model_input_names}\")\n",
    "\n",
    "    return hf_arch, hf_config, hf_tokenizer, hf_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/ohmeow/blurr/blob/dev-3.0.0 #master/blurr/data/token_classification.py#L130){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### get_task_hf_objects\n",
       "\n",
       ">      get_task_hf_objects (pretrained_model_name:str,\n",
       ">                           label_names:list[str]=['neg', 'pos'],\n",
       ">                           verbose:bool=False)\n",
       "\n",
       "A helper function for getting the Hugging Face objects that works out of the box for most sequence classification tasks"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/ohmeow/blurr/blob/dev-3.0.0 #master/blurr/data/token_classification.py#L130){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### get_task_hf_objects\n",
       "\n",
       ">      get_task_hf_objects (pretrained_model_name:str,\n",
       ">                           label_names:list[str]=['neg', 'pos'],\n",
       ">                           verbose:bool=False)\n",
       "\n",
       "A helper function for getting the Hugging Face objects that works out of the box for most sequence classification tasks"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nbdev.show_doc(get_task_hf_objects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |eval:true\n",
    "hf_arch, hf_config, hf_tokenizer, hf_model = get_task_hf_objects(\n",
    "    \"distilbert-base-cased\", [\"great\", \"good\", \"bad\", \"horrific\"], verbose=False\n",
    ")\n",
    "\n",
    "test_eq(hf_arch, \"distilbert\")\n",
    "test_eq(hf_config.num_labels, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "def multiclass_tokenize_func(\n",
    "    examples,\n",
    "    hf_tokenizer: PreTrainedTokenizerBase,\n",
    "    text_attr: str = \"text\",\n",
    "    text_pair_attr: str = None,\n",
    "    max_length: int = None,\n",
    "    padding: bool | str = True,\n",
    "    truncation: bool | str = True,\n",
    "    tok_kwargs: dict = {},\n",
    "):\n",
    "    \"\"\"A tokenization function that works out of the box for most multiclassification tasks\"\"\"\n",
    "    txts = [examples[text_attr]] if text_pair_attr is None else [examples[text_attr], examples[text_pair_attr]]\n",
    "    return hf_tokenizer(*txts, max_length=max_length, padding=padding, truncation=truncation, **tok_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/ohmeow/blurr/blob/dev-3.0.0 #master/blurr/data/core.py#L61){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### multiclass_tokenize_func\n",
       "\n",
       ">      multiclass_tokenize_func (examples, hf_tokenizer:transformers.tokenizatio\n",
       ">                                n_utils_base.PreTrainedTokenizerBase,\n",
       ">                                text_attr:str='text', text_pair_attr:str=None,\n",
       ">                                max_length:int=None, padding:bool|str=True,\n",
       ">                                truncation:bool|str=True, tok_kwargs:dict={})\n",
       "\n",
       "A tokenization function that works out of the box for most multiclassification tasks"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/ohmeow/blurr/blob/dev-3.0.0 #master/blurr/data/core.py#L61){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### multiclass_tokenize_func\n",
       "\n",
       ">      multiclass_tokenize_func (examples, hf_tokenizer:transformers.tokenizatio\n",
       ">                                n_utils_base.PreTrainedTokenizerBase,\n",
       ">                                text_attr:str='text', text_pair_attr:str=None,\n",
       ">                                max_length:int=None, padding:bool|str=True,\n",
       ">                                truncation:bool|str=True, tok_kwargs:dict={})\n",
       "\n",
       "A tokenization function that works out of the box for most multiclassification tasks"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nbdev.show_doc(multiclass_tokenize_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "edbe814445854f40872b3c99e3c4991f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# |eval:true\n",
    "my_dict = {\"id\": [0, 1, 2], \"text\": [\"This is great!\", \"This is so horrible\", \"It was, uh, kinda meh.\"], \"label\": [1, 0, 0]}\n",
    "test_ds = Dataset.from_dict(my_dict)\n",
    "\n",
    "tokenize_func = partial(multiclass_tokenize_func, hf_tokenizer=hf_tokenizer)\n",
    "proc_test_ds = test_ds.map(tokenize_func, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |eval:true\n",
    "proc_example = proc_test_ds[0]\n",
    "\n",
    "test_eq(isinstance(proc_example, dict), True)\n",
    "test_eq(\"input_ids\" in proc_example.keys(), True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "def multilabel_tokenize_func(\n",
    "    examples,\n",
    "    hf_tokenizer: PreTrainedTokenizerBase,\n",
    "    label_attrs: list[str],\n",
    "    text_attr: str = \"text\",\n",
    "    text_pair_attr: str = None,\n",
    "    max_length: int = None,\n",
    "    padding: bool | str = True,\n",
    "    truncation: bool | str = True,\n",
    "    tok_kwargs: dict = {},\n",
    "):\n",
    "    \"\"\"A tokenization function that works out of the box for most multilabel classification tasks\"\"\"\n",
    "    txts = [examples[text_attr]] if text_pair_attr is None else [examples[text_attr], examples[text_pair_attr]]\n",
    "    inputs = hf_tokenizer(*txts, max_length=max_length, padding=padding, truncation=truncation, **tok_kwargs)\n",
    "\n",
    "    label_names = torch.stack([tensor(examples[lbl]) for lbl in label_attrs], dim=-1)\n",
    "    inputs[\"label\"] = label_names\n",
    "\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/ohmeow/blurr/blob/dev-3.0.0 #master/blurr/data/core.py#L76){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### multilabel_tokenize_func\n",
       "\n",
       ">      multilabel_tokenize_func (examples, hf_tokenizer:transformers.tokenizatio\n",
       ">                                n_utils_base.PreTrainedTokenizerBase,\n",
       ">                                label_attrs:list[str], text_attr:str='text',\n",
       ">                                text_pair_attr:str=None, max_length:int=None,\n",
       ">                                padding:bool|str=True,\n",
       ">                                truncation:bool|str=True, tok_kwargs:dict={})\n",
       "\n",
       "A tokenization function that works out of the box for most multilabel classification tasks"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/ohmeow/blurr/blob/dev-3.0.0 #master/blurr/data/core.py#L76){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### multilabel_tokenize_func\n",
       "\n",
       ">      multilabel_tokenize_func (examples, hf_tokenizer:transformers.tokenizatio\n",
       ">                                n_utils_base.PreTrainedTokenizerBase,\n",
       ">                                label_attrs:list[str], text_attr:str='text',\n",
       ">                                text_pair_attr:str=None, max_length:int=None,\n",
       ">                                padding:bool|str=True,\n",
       ">                                truncation:bool|str=True, tok_kwargs:dict={})\n",
       "\n",
       "A tokenization function that works out of the box for most multilabel classification tasks"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nbdev.show_doc(multilabel_tokenize_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |eval:true\n",
    "hf_arch, hf_config, hf_tokenizer, hf_model = get_task_hf_objects(\"distilbert-base-cased\", [\"label1\", \"label2\"], verbose=False)\n",
    "\n",
    "test_eq(hf_arch, \"distilbert\")\n",
    "test_eq(hf_config.num_labels, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4a09e5cb3d342b0b02d15e4e31ff7f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# |eval:true\n",
    "my_dict = {\n",
    "    \"id\": [0, 1, 2],\n",
    "    \"text\": [\"This is great!\", \"This is so horrible\", \"It was, uh, kinda meh.\"],\n",
    "    \"label1\": [1, 0, 0],\n",
    "    \"label2\": [0, 1, 1],\n",
    "}\n",
    "\n",
    "test_ds = Dataset.from_dict(my_dict)\n",
    "\n",
    "tokenize_func = partial(multilabel_tokenize_func, hf_tokenizer=hf_tokenizer, label_attrs=[\"label1\", \"label2\"])\n",
    "proc_test_ds = test_ds.map(tokenize_func, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |eval:true\n",
    "proc_example = proc_test_ds[0]\n",
    "\n",
    "test_eq(isinstance(proc_example, dict), True)\n",
    "test_eq(\"input_ids\" in proc_example.keys(), True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `TextCollatorWithPadding`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "@dataclass\n",
    "class TextCollatorWithPadding:\n",
    "    def __init__(\n",
    "        self,\n",
    "        # A Hugging Face tokenizer\n",
    "        hf_tokenizer: PreTrainedTokenizerBase,\n",
    "        # The abbreviation/name of your Hugging Face transformer architecture (e.b., bert, bart, etc..)\n",
    "        hf_arch: str = None,\n",
    "        # A specific configuration instance you want to use\n",
    "        hf_config: PretrainedConfig = None,\n",
    "        # A Hugging Face model\n",
    "        hf_model: PreTrainedModel = None,\n",
    "        # The number of inputs expected by your model\n",
    "        n_inp: int = 1,\n",
    "        # Defaults to use Hugging Face's DataCollatorWithPadding(tokenizer=hf_tokenizer)\n",
    "        data_collator_cls: type = DataCollatorWithPadding,\n",
    "        # kwyargs specific for the instantiation of the `data_collator`\n",
    "        data_collator_kwargs: dict = {},\n",
    "    ):\n",
    "        \"\"\"A data collation function that can be used across blurr's base, low-level, and mid-level APIs\"\"\"\n",
    "        store_attr()\n",
    "        self.hf_tokenizer = data_collator_kwargs.pop(\"tokenizer\", self.hf_tokenizer)\n",
    "        self.data_collator = data_collator_cls(tokenizer=self.hf_tokenizer, **data_collator_kwargs)\n",
    "\n",
    "    def __call__(self, features):\n",
    "        features = L(features)\n",
    "        inputs, labels, targs = [], [], []\n",
    "\n",
    "        # features contain dictionaries\n",
    "        if isinstance(features[0], dict):\n",
    "            feature_keys = list(features[0].keys())\n",
    "            inputs = [self._build_inputs_d(features, feature_keys)]\n",
    "\n",
    "            input_labels = self._build_input_labels(inputs[0], features, feature_keys)\n",
    "            if input_labels is not None:\n",
    "                labels, targs = [input_labels], [input_labels.clone()]\n",
    "        # features contains tuples, each of which can contain multiple inputs and/or targets\n",
    "        elif isinstance(features[0], tuple):\n",
    "            for f_idx in range(self.n_inp):\n",
    "                feature_keys = list(features[0][f_idx].keys())\n",
    "                inputs.append(self._build_inputs_d(features.itemgot(f_idx), feature_keys))\n",
    "\n",
    "                input_labels = self._build_input_labels(inputs[0], features.itemgot(f_idx), feature_keys)\n",
    "                labels.append(input_labels if input_labels is not None else [])\n",
    "\n",
    "            targs = [self._proc_targets(inputs[0], list(features.itemgot(f_idx))) for f_idx in range(self.n_inp, len(features[0]))]\n",
    "\n",
    "        return self._build_batch(inputs, labels, targs)\n",
    "\n",
    "    # ----- utility methods -----\n",
    "\n",
    "    # to build the inputs dictionary\n",
    "    def _build_inputs_d(self, features, feature_keys):\n",
    "        return {fwd_arg: list(features.attrgot(fwd_arg)) for fwd_arg in self.hf_tokenizer.model_input_names if fwd_arg in feature_keys}\n",
    "\n",
    "    # to build the input \"labels\"\n",
    "    def _build_input_labels(self, inputs_d, features, feature_keys):\n",
    "        if \"label\" in feature_keys:\n",
    "            labels = list(features.attrgot(\"label\"))\n",
    "            return self._proc_targets(inputs_d, labels)\n",
    "        return None\n",
    "\n",
    "    # used to give the labels/targets the right shape\n",
    "    def _proc_targets(self, inputs_d, targs):\n",
    "        if is_listy(targs[0]):\n",
    "            targs = torch.stack([tensor(lbls) for lbls in targs])\n",
    "        elif isinstance(targs[0], torch.Tensor) and len(targs[0].size()) > 0:\n",
    "            targs = torch.stack(targs)\n",
    "        else:\n",
    "            targs = torch.tensor(targs)\n",
    "\n",
    "        return targs\n",
    "\n",
    "    # will properly assemble are batch given a list of inputs, labels, and targets\n",
    "    def _build_batch(self, inputs, labels, targs):\n",
    "        batch = []\n",
    "\n",
    "        for input, input_labels in zip(inputs, labels):\n",
    "            input_d = dict(self.data_collator(input))\n",
    "            if len(input_labels) > 0:\n",
    "                input_d[\"labels\"] = input_labels\n",
    "\n",
    "            batch.append(input_d)\n",
    "\n",
    "        for targ in targs:\n",
    "            batch.append(targ)\n",
    "\n",
    "        return tuplify(batch)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Base API: Examples\n",
    "\n",
    "This section demonstrates how you can use standard `Dataset` objects (PyTorch and Hugging Face) to build PyTorch `DataLoader`s\n",
    "\n",
    "**Note** that most fast.ai specific features such as `DataLoaders.one_batch` and `DataLoader.show_batch` are not available when using PyTorch."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multiclass"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 1: HF objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== config ===\n",
      "# of labels:\t2\n",
      "\n",
      "=== tokenizer ===\n",
      "Vocab size:\t\t128000\n",
      "Max # of tokens:\t1000000000000000019884624838656\n",
      "Attributes expected by model in forward pass:\t['input_ids', 'token_type_ids', 'attention_mask']\n"
     ]
    }
   ],
   "source": [
    "hf_arch, hf_config, hf_tokenizer, hf_model = get_task_hf_objects(\"microsoft/deberta-v3-small\", label_names, verbose=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 2: `Dataset`s (PyTorch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4abcee645a8e4ca0b4da3dd27ff466a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c737021f41ac4d5b8899f5d970253fed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# tokenize the dataset\n",
    "tokenize_func = partial(multiclass_tokenize_func, hf_tokenizer=hf_tokenizer)\n",
    "proc_train_ds = train_ds.map(tokenize_func, batched=True)\n",
    "proc_valid_ds = valid_ds.map(tokenize_func, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define our PyTorch Dataset class\n",
    "class HFTextClassificationDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, hf_dataset, hf_tokenizer):\n",
    "        self.hf_dataset = hf_dataset\n",
    "        self.hf_tokenizer = hf_tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.hf_dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.hf_dataset[idx]\n",
    "        return item\n",
    "\n",
    "\n",
    "# build our PyTorch training and validation Datasets\n",
    "pt_proc_train_ds = HFTextClassificationDataset(proc_train_ds, hf_tokenizer=hf_tokenizer)\n",
    "pt_proc_valid_ds = HFTextClassificationDataset(proc_valid_ds, hf_tokenizer=hf_tokenizer)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 3: `DataLoader`s (PyTorch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build your fastai `DataLoaders` from Pytorch `DataLoader` objects\n",
    "batch_size = 4\n",
    "data_collator = TextCollatorWithPadding(hf_tokenizer)\n",
    "train_dl = torch.utils.data.DataLoader(pt_proc_train_ds, batch_size=batch_size, shuffle=True, collate_fn=data_collator)\n",
    "valid_dl = torch.utils.data.DataLoader(pt_proc_valid_ds, batch_size=batch_size * 2, shuffle=False, collate_fn=data_collator)\n",
    "\n",
    "dls = DataLoaders(train_dl, valid_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of batches in train|validation dataloaders: 250 25\n",
      "# of items in each batch: 2\n",
      "\n",
      "Decoded input_ids: [CLS] When watching A Bug's Life for the ... \n",
      "Targets: tensor([1, 1, 0, 0])\n"
     ]
    }
   ],
   "source": [
    "print(\"# of batches in train|validation dataloaders:\", len(train_dl), len(valid_dl))\n",
    "\n",
    "b = next(iter(train_dl))\n",
    "print(\"# of items in each batch:\", len(b))\n",
    "print(\"\")\n",
    "print(f\"Decoded input_ids: {hf_tokenizer.decode(b[0]['input_ids'][0][:10])} ... \")\n",
    "print(\"Targets:\", b[1])\n",
    "\n",
    "# b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOPE: Won't work with PyTorch DataLoaders\n",
    "# AttributeError: 'DataLoader' object has no attribute 'show_batch'\n",
    "# dls.show_batch(dataloaders=dls, max_n=2, trunc_at=800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |echo:false\n",
    "try:\n",
    "    del dls, hf_model\n",
    "except:\n",
    "    pass\n",
    "finally:\n",
    "    clean_memory()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multilabel"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 1: HF objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== config ===\n",
      "# of labels:\t7\n",
      "\n",
      "=== tokenizer ===\n",
      "Vocab size:\t\t128000\n",
      "Max # of tokens:\t1000000000000000019884624838656\n",
      "Attributes expected by model in forward pass:\t['input_ids', 'token_type_ids', 'attention_mask']\n"
     ]
    }
   ],
   "source": [
    "hf_arch, hf_config, hf_tokenizer, hf_model = get_task_hf_objects(\"microsoft/deberta-v3-small\", civil_label_names, verbose=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 2: `Dataset`s (PyTorch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2409a25001f41e7995c056163931721",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ad0dd5f473a494e9e5fb8b921a0c731",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# tokenize the dataset\n",
    "tokenize_func = partial(multilabel_tokenize_func, hf_tokenizer=hf_tokenizer, label_attrs=civil_label_names)\n",
    "proc_civil_train_ds = civil_train_ds.map(tokenize_func, batched=True)\n",
    "proc_civil_valid_ds = civil_valid_ds.map(tokenize_func, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define our PyTorch Dataset class\n",
    "class HFTextMultilabelClassificationDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, hf_dataset, hf_tokenizer, label_names):\n",
    "        self.hf_dataset = hf_dataset\n",
    "        self.hf_tokenizer = hf_tokenizer\n",
    "        self.label_names = label_names\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.hf_dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.hf_dataset[idx]\n",
    "        # item[\"label\"] = [item[lbl] for lbl in self.label_names]\n",
    "        return item\n",
    "\n",
    "\n",
    "# build our PyTorch training and validation Datasets\n",
    "pt_proc_civil_train_ds = HFTextMultilabelClassificationDataset(\n",
    "    proc_civil_train_ds, hf_tokenizer=hf_tokenizer, label_names=civil_label_names\n",
    ")\n",
    "pt_proc_civil_valid_ds = HFTextMultilabelClassificationDataset(\n",
    "    proc_civil_valid_ds, hf_tokenizer=hf_tokenizer, label_names=civil_label_names\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 3: `DataLoader`s (PyTorch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build your fastai `DataLoaders` from Pytorch `DataLoader` objects\n",
    "batch_size = 4\n",
    "data_collator = TextCollatorWithPadding(hf_tokenizer)\n",
    "train_dl = torch.utils.data.DataLoader(pt_proc_civil_train_ds, batch_size=batch_size, shuffle=True, collate_fn=data_collator)\n",
    "valid_dl = torch.utils.data.DataLoader(pt_proc_civil_valid_ds, batch_size=batch_size * 2, shuffle=False, collate_fn=data_collator)\n",
    "\n",
    "dls = DataLoaders(train_dl, valid_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of batches in train|validation dataloaders: 250 25\n",
      "# of items in each batch: 2\n",
      "\n",
      "Decoded input_ids: [CLS] There's no such thing as a National ... \n",
      "Targets: tensor([[0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0]])\n"
     ]
    }
   ],
   "source": [
    "print(\"# of batches in train|validation dataloaders:\", len(train_dl), len(valid_dl))\n",
    "\n",
    "b = next(iter(train_dl))\n",
    "print(\"# of items in each batch:\", len(b))\n",
    "print(\"\")\n",
    "print(f\"Decoded input_ids: {hf_tokenizer.decode(b[0]['input_ids'][0][:10])} ... \")\n",
    "print(\"Targets:\", b[1])\n",
    "\n",
    "# b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |echo:false\n",
    "try:\n",
    "    del dls, hf_model\n",
    "except:\n",
    "    pass\n",
    "finally:\n",
    "    clean_memory()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hugging Face"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multiclass"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 1: HF objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== config ===\n",
      "# of labels:\t2\n",
      "\n",
      "=== tokenizer ===\n",
      "Vocab size:\t\t128000\n",
      "Max # of tokens:\t1000000000000000019884624838656\n",
      "Attributes expected by model in forward pass:\t['input_ids', 'token_type_ids', 'attention_mask']\n"
     ]
    }
   ],
   "source": [
    "hf_arch, hf_config, hf_tokenizer, hf_model = get_task_hf_objects(\"microsoft/deberta-v3-small\", label_names, verbose=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 2: `Datasets` (huggingface)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/wgilliam/.cache/huggingface/datasets/imdb/plain_text/1.0.0/d613c88cf8fa3bab83b4ded3713f1f74830d1100e171db75bbddb80b3345c9c0/cache-bf198da76a267503.arrow\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b18721c51a84de68c6147c1d64ab441",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# tokenize the dataset\n",
    "tokenize_func = partial(multiclass_tokenize_func, hf_tokenizer=hf_tokenizer)\n",
    "proc_train_ds = train_ds.map(tokenize_func, batched=True)\n",
    "proc_valid_ds = valid_ds.map(tokenize_func, batched=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 3: `DataLoader`s (PyTorch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build your fastai `DataLoaders` from Pytorch `DataLoader` objects\n",
    "batch_size = 4\n",
    "data_collator = TextCollatorWithPadding(hf_tokenizer)\n",
    "train_dl = torch.utils.data.DataLoader(proc_train_ds, batch_size=batch_size, shuffle=True, collate_fn=data_collator)\n",
    "valid_dl = torch.utils.data.DataLoader(proc_valid_ds, batch_size=batch_size * 2, shuffle=False, collate_fn=data_collator)\n",
    "\n",
    "dls = DataLoaders(train_dl, valid_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of batches in train|validation dataloaders: 250 25\n",
      "# of items in each batch: 2\n",
      "\n",
      "Decoded input_ids: [CLS] 'One-Round' Jack Sander is called ... \n",
      "Targets: tensor([1, 0, 1, 0])\n"
     ]
    }
   ],
   "source": [
    "print(\"# of batches in train|validation dataloaders:\", len(train_dl), len(valid_dl))\n",
    "\n",
    "b = next(iter(train_dl))\n",
    "print(\"# of items in each batch:\", len(b))\n",
    "print(\"\")\n",
    "print(f\"Decoded input_ids: {hf_tokenizer.decode(b[0]['input_ids'][0][:10])} ... \")\n",
    "print(\"Targets:\", b[1])\n",
    "\n",
    "# b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |echo:false\n",
    "try:\n",
    "    del dls, hf_model\n",
    "except:\n",
    "    pass\n",
    "finally:\n",
    "    clean_memory()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multilabel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 1: HF objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== config ===\n",
      "# of labels:\t7\n",
      "\n",
      "=== tokenizer ===\n",
      "Vocab size:\t\t128000\n",
      "Max # of tokens:\t1000000000000000019884624838656\n",
      "Attributes expected by model in forward pass:\t['input_ids', 'token_type_ids', 'attention_mask']\n"
     ]
    }
   ],
   "source": [
    "hf_arch, hf_config, hf_tokenizer, hf_model = get_task_hf_objects(\"microsoft/deberta-v3-small\", civil_label_names, verbose=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 2: `Datasets` (huggingface)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/wgilliam/.cache/huggingface/datasets/civil_comments/default/0.9.0/e7a3aacd2ab7d135fa958e7209d10b1fa03807d44c486e3c34897aa08ea8ffab/cache-3d64576ee130c580.arrow\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b818559ce0c43a0910363e02ecc2581",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# tokenize the dataset\n",
    "tokenize_func = partial(multilabel_tokenize_func, hf_tokenizer=hf_tokenizer, label_attrs=civil_label_names)\n",
    "proc_civil_train_ds = civil_train_ds.map(tokenize_func, batched=True)\n",
    "proc_civil_valid_ds = civil_valid_ds.map(tokenize_func, batched=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 3: `DataLoader`s (PyTorch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build your fastai `DataLoaders` from Pytorch `DataLoader` objects\n",
    "batch_size = 4\n",
    "data_collator = TextCollatorWithPadding(hf_tokenizer)\n",
    "train_dl = torch.utils.data.DataLoader(proc_civil_train_ds, batch_size=batch_size, shuffle=True, collate_fn=data_collator)\n",
    "valid_dl = torch.utils.data.DataLoader(proc_civil_valid_ds, batch_size=batch_size * 2, shuffle=False, collate_fn=data_collator)\n",
    "\n",
    "dls = DataLoaders(train_dl, valid_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of batches in train|validation dataloaders: 250 25\n",
      "# of items in each batch: 2\n",
      "\n",
      "Decoded input_ids: [CLS] How do you think most adults vote? Often ... \n",
      "Targets: tensor([[0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0]])\n"
     ]
    }
   ],
   "source": [
    "print(\"# of batches in train|validation dataloaders:\", len(train_dl), len(valid_dl))\n",
    "\n",
    "b = next(iter(train_dl))\n",
    "print(\"# of items in each batch:\", len(b))\n",
    "print(\"\")\n",
    "print(f\"Decoded input_ids: {hf_tokenizer.decode(b[0]['input_ids'][0][:10])} ... \")\n",
    "print(\"Targets:\", b[1])\n",
    "\n",
    "# b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |echo:false\n",
    "try:\n",
    "    del dls, hf_model\n",
    "except:\n",
    "    pass\n",
    "finally:\n",
    "    clean_memory()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Low-Level API\n",
    "\n",
    "This section demonstrates how you can migrate from using PyTorch/Hugging Face to fast.ai `Datasets` and `DataLoaders` to recapture much of the fast.ai specific features unavailable when using basic PyTorch. This includes:\n",
    "\n",
    "- `DataLoaders.one_batch()`\n",
    "- `DataLoaders.show_batch()`\n",
    "- `Leaner.export()`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `TextInput` -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "class TextInput(TensorBase):\n",
    "    \"\"\"The base represenation of your inputs; used by the various fastai `show` methods\"\"\"\n",
    "\n",
    "    pass"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A `TextInput` object is returned from the decodes method of `BatchDecodeTransform` as a means to customize `@typedispatch`ed functions like `DataLoaders.show_batch` and `Learner.show_results`. The value will the your \"input_ids\"."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `BatchDecodeTransform` -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "class BatchDecodeTransform(Transform):\n",
    "    \"\"\"A class used to cast your inputs as `input_return_type` for fastai `show` methods\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        # A Hugging Face tokenizer (not required if passing in an instance of `BatchTokenizeTransform` to `before_batch_tfm`)\n",
    "        hf_tokenizer: PreTrainedTokenizerBase,\n",
    "        # The abbreviation/name of your Hugging Face transformer architecture (not required if passing in an instance of `BatchTokenizeTransform` to `before_batch_tfm`)\n",
    "        hf_arch: str = None,\n",
    "        # A Hugging Face configuration object (not required if passing in an instance of `BatchTokenizeTransform` to `before_batch_tfm`)\n",
    "        hf_config: PretrainedConfig = None,\n",
    "        # A Hugging Face model (not required if passing in an instance of `BatchTokenizeTransform` to `before_batch_tfm`)\n",
    "        hf_model: PreTrainedModel = None,\n",
    "        # Used by typedispatched show methods\n",
    "        input_return_type: type = TextInput,\n",
    "        # The token ID that should be ignored when calculating the loss\n",
    "        ignore_token_id: int = CrossEntropyLossFlat().ignore_index,\n",
    "        # Any other keyword arguments\n",
    "        **kwargs,\n",
    "    ):\n",
    "        store_attr()\n",
    "        self.kwargs = kwargs\n",
    "\n",
    "    def decodes(self, items: dict):\n",
    "        \"\"\"Returns the proper object and data for show related fastai methods\"\"\"\n",
    "        return self.input_return_type(items[\"input_ids\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As of fastai 2.1.5, before batch transforms no longer have a `decodes` method ... and so, I've introduced a standard batch transform here, `BatchDecodeTransform`, (one that occurs \"after\" the batch has been created) that will do the decoding for us."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility classes and methods \n",
    "\n",
    "These methods are use internally for getting blurr transforms associated to your `DataLoaders`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "def get_blurr_tfm(\n",
    "    # A list of transforms (e.g., dls.after_batch, dls.before_batch, etc...)\n",
    "    tfms_list: Pipeline,\n",
    "    # The transform to find\n",
    "    tfm_class: Transform = BatchDecodeTransform,\n",
    "):\n",
    "    \"\"\"\n",
    "    Given a fastai DataLoaders batch transforms, this method can be used to get at a transform\n",
    "    instance used in your Blurr DataBlock\n",
    "    \"\"\"\n",
    "    return next(filter(lambda el: issubclass(type(el), tfm_class), tfms_list), None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/ohmeow/blurr/blob/dev-3.0.0 #master/blurr/data/core.py#L221){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### get_blurr_tfm\n",
       "\n",
       ">      get_blurr_tfm (tfms_list:fastcore.transform.Pipeline,\n",
       ">                     tfm_class:fastcore.transform.Transform=<class\n",
       ">                     '__main__.BatchDecodeTransform'>)\n",
       "\n",
       "Given a fastai DataLoaders batch transforms, this method can be used to get at a transform\n",
       "instance used in your Blurr DataBlock\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| tfms_list | Pipeline |  | A list of transforms (e.g., dls.after_batch, dls.before_batch, etc...) |\n",
       "| tfm_class | Transform | BatchDecodeTransform | The transform to find |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/ohmeow/blurr/blob/dev-3.0.0 #master/blurr/data/core.py#L221){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### get_blurr_tfm\n",
       "\n",
       ">      get_blurr_tfm (tfms_list:fastcore.transform.Pipeline,\n",
       ">                     tfm_class:fastcore.transform.Transform=<class\n",
       ">                     '__main__.BatchDecodeTransform'>)\n",
       "\n",
       "Given a fastai DataLoaders batch transforms, this method can be used to get at a transform\n",
       "instance used in your Blurr DataBlock\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| tfms_list | Pipeline |  | A list of transforms (e.g., dls.after_batch, dls.before_batch, etc...) |\n",
       "| tfm_class | Transform | BatchDecodeTransform | The transform to find |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nbdev.show_doc(get_blurr_tfm, title_level=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "def first_blurr_tfm(\n",
    "    # Your fast.ai `DataLoaders\n",
    "    dls: DataLoaders,\n",
    "    # The Blurr transforms to look for in order\n",
    "    tfms: list[Transform] = [BatchDecodeTransform],\n",
    "):\n",
    "    \"\"\"\n",
    "    This convenience method will find the first Blurr transform required for methods such as\n",
    "    `show_batch` and `show_results`. The returned transform should have everything you need to properly\n",
    "    decode and 'show' your Hugging Face inputs/targets\n",
    "    \"\"\"\n",
    "    for tfm in tfms:\n",
    "        found_tfm = get_blurr_tfm(dls.before_batch, tfm_class=tfm)\n",
    "        if found_tfm:\n",
    "            return found_tfm\n",
    "\n",
    "        found_tfm = get_blurr_tfm(dls.after_batch, tfm_class=tfm)\n",
    "        if found_tfm:\n",
    "            return found_tfm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/ohmeow/blurr/blob/dev-3.0.0 #master/blurr/data/core.py#L234){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### first_blurr_tfm\n",
       "\n",
       ">      first_blurr_tfm (dls:fastai.data.core.DataLoaders,\n",
       ">                       tfms:list[fastcore.transform.Transform]=[<class\n",
       ">                       '__main__.BatchDecodeTransform'>])\n",
       "\n",
       "This convenience method will find the first Blurr transform required for methods such as\n",
       "`show_batch` and `show_results`. The returned transform should have everything you need to properly\n",
       "decode and 'show' your Hugging Face inputs/targets\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| dls | DataLoaders |  | Your fast.ai `DataLoaders |\n",
       "| tfms | list[Transform] | [<class '__main__.BatchDecodeTransform'>] | The Blurr transforms to look for in order |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/ohmeow/blurr/blob/dev-3.0.0 #master/blurr/data/core.py#L234){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### first_blurr_tfm\n",
       "\n",
       ">      first_blurr_tfm (dls:fastai.data.core.DataLoaders,\n",
       ">                       tfms:list[fastcore.transform.Transform]=[<class\n",
       ">                       '__main__.BatchDecodeTransform'>])\n",
       "\n",
       "This convenience method will find the first Blurr transform required for methods such as\n",
       "`show_batch` and `show_results`. The returned transform should have everything you need to properly\n",
       "decode and 'show' your Hugging Face inputs/targets\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| dls | DataLoaders |  | Your fast.ai `DataLoaders |\n",
       "| tfms | list[Transform] | [<class '__main__.BatchDecodeTransform'>] | The Blurr transforms to look for in order |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nbdev.show_doc(first_blurr_tfm, title_level=3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `show_batch` -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "@typedispatch\n",
    "def show_batch(\n",
    "    # This typedispatched `show_batch` will be called for `TextInput` typed inputs\n",
    "    x: TextInput,\n",
    "    # Your targets\n",
    "    y,\n",
    "    # Your raw inputs/targets\n",
    "    samples,\n",
    "    # Your `DataLoaders`. This is required so as to get at the Hugging Face objects for\n",
    "    # decoding them into something understandable\n",
    "    dataloaders,\n",
    "    # Your `show_batch` context\n",
    "    ctxs=None,\n",
    "    # The maximum number of items to show\n",
    "    max_n=6,\n",
    "    # Any truncation your want applied to your decoded inputs\n",
    "    trunc_at=None,\n",
    "    # Any other keyword arguments you want applied to `show_batch`\n",
    "    **kwargs,\n",
    "):\n",
    "    # grab our tokenizer\n",
    "    tfm = first_blurr_tfm(dataloaders)\n",
    "    hf_tokenizer = tfm.hf_tokenizer\n",
    "\n",
    "    # if we've included our label_names list, we'll use it to look up the value of our target(s)\n",
    "    trg_label_names = tfm.kwargs[\"label_names\"] if (\"label_names\" in tfm.kwargs) else None\n",
    "    if trg_label_names is None and dataloaders.vocab is not None:\n",
    "        trg_label_names = dataloaders.vocab\n",
    "\n",
    "    res = L()\n",
    "    n_inp = dataloaders.n_inp\n",
    "\n",
    "    n_samples = min(max_n, dataloaders.bs)\n",
    "    for idx in range(n_samples):\n",
    "        input_ids = x[idx]\n",
    "        rets = [hf_tokenizer.decode(input_ids, skip_special_tokens=True)[:trunc_at]]\n",
    "\n",
    "        sample = samples[idx] if samples is not None else None\n",
    "        for item_idx, item in enumerate(sample[n_inp:]):\n",
    "            label = y[item_idx] if y is not None else item\n",
    "\n",
    "            if torch.is_tensor(label):\n",
    "                label = list(label.numpy()) if len(label.size()) > 0 else label.item()\n",
    "\n",
    "            if is_listy(label):\n",
    "                trg = [trg_label_names[int(idx)] for idx, val in enumerate(label) if (val == 1)] if trg_label_names else label\n",
    "            else:\n",
    "                trg = trg_label_names[int(item)] if trg_label_names else item\n",
    "\n",
    "            rets.append(trg)\n",
    "        res.append(tuplify(rets))\n",
    "\n",
    "    cols = [\"text\"] + [\"target\" if (i == 0) else f\"target_{i}\" for i in range(len(res[0]) - n_inp)]\n",
    "    display_df(pd.DataFrame(res, columns=cols)[:max_n])\n",
    "    return ctxs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `sorted_dl_func` -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "def sorted_dl_func(\n",
    "    example,\n",
    "    # A Hugging Face tokenizer\n",
    "    hf_tokenizer: PreTrainedTokenizerBase,\n",
    "    # The `is_split_into_words` argument applied to your `hf_tokenizer` during tokenization. \\\n",
    "    # Set this to 'True' if your inputs are pre-tokenized (not numericalized)\n",
    "    is_split_into_words: bool = False,\n",
    "    # Any other keyword arguments you want to include during tokenization\n",
    "    tok_kwargs: dict = {},\n",
    "):\n",
    "    \"\"\"This method is used by the `SortedDL` to ensure your dataset is sorted *after* tokenization\"\"\"\n",
    "    txt = None\n",
    "    if isinstance(example[0], dict):\n",
    "        if \"input_ids\" in example[0]:\n",
    "            # if inputs are pretokenized\n",
    "            return len(example[0][\"input_ids\"])\n",
    "        else:\n",
    "            txt = example[0][\"text\"]\n",
    "    else:\n",
    "        txt = example[0]\n",
    "\n",
    "    return len(txt) if is_split_into_words else len(hf_tokenizer.tokenize(txt, **tok_kwargs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/ohmeow/blurr/blob/dev-3.0.0 #master/blurr/data/core.py#L312){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### sorted_dl_func\n",
       "\n",
       ">      sorted_dl_func (example, hf_tokenizer:transformers.tokenization_utils_bas\n",
       ">                      e.PreTrainedTokenizerBase,\n",
       ">                      is_split_into_words:bool=False, tok_kwargs:dict={})\n",
       "\n",
       "This method is used by the `SortedDL` to ensure your dataset is sorted *after* tokenization\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| example |  |  |  |\n",
       "| hf_tokenizer | PreTrainedTokenizerBase |  | A Hugging Face tokenizer |\n",
       "| is_split_into_words | bool | False | The `is_split_into_words` argument applied to your `hf_tokenizer` during tokenization. \\<br>Set this to 'True' if your inputs are pre-tokenized (not numericalized) |\n",
       "| tok_kwargs | dict | {} | Any other keyword arguments you want to include during tokenization |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/ohmeow/blurr/blob/dev-3.0.0 #master/blurr/data/core.py#L312){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### sorted_dl_func\n",
       "\n",
       ">      sorted_dl_func (example, hf_tokenizer:transformers.tokenization_utils_bas\n",
       ">                      e.PreTrainedTokenizerBase,\n",
       ">                      is_split_into_words:bool=False, tok_kwargs:dict={})\n",
       "\n",
       "This method is used by the `SortedDL` to ensure your dataset is sorted *after* tokenization\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| example |  |  |  |\n",
       "| hf_tokenizer | PreTrainedTokenizerBase |  | A Hugging Face tokenizer |\n",
       "| is_split_into_words | bool | False | The `is_split_into_words` argument applied to your `hf_tokenizer` during tokenization. \\<br>Set this to 'True' if your inputs are pre-tokenized (not numericalized) |\n",
       "| tok_kwargs | dict | {} | Any other keyword arguments you want to include during tokenization |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nbdev.show_doc(sorted_dl_func, title_level=3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Low-Level API: Examples"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiclass"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1: HF objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== config ===\n",
      "# of labels:\t2\n",
      "\n",
      "=== tokenizer ===\n",
      "Vocab size:\t\t128000\n",
      "Max # of tokens:\t1000000000000000019884624838656\n",
      "Attributes expected by model in forward pass:\t['input_ids', 'token_type_ids', 'attention_mask']\n"
     ]
    }
   ],
   "source": [
    "hf_arch, hf_config, hf_tokenizer, hf_model = get_task_hf_objects(\"microsoft/deberta-v3-small\", label_names, verbose=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: `Datasets` (fastai)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ab84a59d3ab451cae37e376dcf8c881",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# tokenize the dataset\n",
    "tokenize_func = partial(multiclass_tokenize_func, hf_tokenizer=hf_tokenizer)\n",
    "proc_imdb_ds = imdb_ds.map(tokenize_func, batched=True)\n",
    "\n",
    "# turn Arrow into DataFrame (`ColSplitter` only works with `DataFrame`s)\n",
    "train_df = pd.DataFrame(proc_imdb_ds)\n",
    "train_df.head()\n",
    "\n",
    "# define dataset splitter\n",
    "splitter = ColSplitter(\"is_valid\")\n",
    "splits = splitter(imdb_df)\n",
    "\n",
    "\n",
    "# define how we want to build our inputs and targets\n",
    "def _build_inputs(example):\n",
    "    return {fwd_arg_name: example[fwd_arg_name] for fwd_arg_name in hf_tokenizer.model_input_names if fwd_arg_name in list(example.keys())}\n",
    "\n",
    "\n",
    "def _build_targets(example):\n",
    "    return example[\"label\"]\n",
    "\n",
    "\n",
    "# create our fastai `Datasets` object\n",
    "dsets = Datasets(items=train_df, splits=splits, tfms=[[_build_inputs], _build_targets], n_inp=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Items in train|validation datasets:  1000 200\n",
      "Items in each example: 2\n",
      "Example inputs: ['input_ids', 'token_type_ids', 'attention_mask']\n",
      "Example target(s): 0\n"
     ]
    }
   ],
   "source": [
    "print(\"Items in train|validation datasets: \", len(dsets.train), len(dsets.valid))\n",
    "\n",
    "example = dsets.valid[0]\n",
    "# example\n",
    "\n",
    "print(f\"Items in each example: {len(example)}\")\n",
    "print(f\"Example inputs: {list(example[0].keys())}\")\n",
    "print(f\"Example target(s): {example[1]}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3: `DataLoaders` (fastai)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = TextCollatorWithPadding(hf_tokenizer)\n",
    "sort_func = partial(sorted_dl_func, hf_tokenizer=hf_tokenizer)\n",
    "batch_decode_tfm = BatchDecodeTransform(hf_tokenizer, hf_arch, hf_config, hf_model, label_names=label_names)\n",
    "\n",
    "dls = dsets.dataloaders(\n",
    "    batch_size=4,\n",
    "    create_batch=data_collator,\n",
    "    after_batch=batch_decode_tfm,\n",
    "    dl_type=partial(SortedDL, sort_func=sort_func),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of batches in train|validation dataloaders: 250 25\n",
      "# of items in each batch: 2\n",
      "\n",
      "Decoded input_ids: [CLS] It is no different than the guys who will ... \n",
      "Targets: tensor([[0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 0, 0, 0, 1, 0, 0]])\n"
     ]
    }
   ],
   "source": [
    "print(\"# of batches in train|validation dataloaders:\", len(train_dl), len(valid_dl))\n",
    "\n",
    "b = next(iter(train_dl))\n",
    "print(\"# of items in each batch:\", len(b))\n",
    "print(\"\")\n",
    "print(f\"Decoded input_ids: {hf_tokenizer.decode(b[0]['input_ids'][0][:10])} ... \")\n",
    "print(\"Targets:\", b[1])\n",
    "\n",
    "# b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I've read a number of reviews on this film and I have to say \"What is wrong with you people?!?!\" This was an excellent film! I thought this film was superb from start to finish and the story was extremely well told. I'm convinced that the people that didn't like this film weren't paying very good attention to the film. There are a number of very important scenes that if you aren't paying attention you will be confused and the following scenes may not make sense. I urge anyone who didn't like this film to watch it again and watch it alone so that you can truly pay attention. The story made perfect sense to me and as I said, was very well told. Every scene in the film has a point and everything fits together at the end of the film.&lt;br /&gt;&lt;br /&gt;All the actors did a fantastic job! Sean Connery</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I'm not going to criticize the movie. There isn't that much to talk about. It has good animal actions scenes which were probably pretty astonishing at the time. Clyde Beatty isn't exactly a matinée idol. He's a little slight and not particularly good looking. But that's OK. He's the man in that lion cage. We know that when he can't take the time away from his lions to tend to his girlfriend, he will end up on an island with her and have to save the day. Someone said earlier that it is a history lesson. The scenes at the circus are of another day, especially the kids who hang around. I didn't realize that even back in the thirties, they sailed on three masted schooners. It looked like something out of 1860. I guess that's the stock footage they had. No wonder the thing got wrecked. They're</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dls.show_batch(dataloaders=dls, max_n=2, trunc_at=800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |echo:false\n",
    "try:\n",
    "    del dls, hf_model\n",
    "except:\n",
    "    pass\n",
    "finally:\n",
    "    clean_memory()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multilabel"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1: HF objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== config ===\n",
      "# of labels:\t7\n",
      "\n",
      "=== tokenizer ===\n",
      "Vocab size:\t\t128000\n",
      "Max # of tokens:\t1000000000000000019884624838656\n",
      "Attributes expected by model in forward pass:\t['input_ids', 'token_type_ids', 'attention_mask']\n"
     ]
    }
   ],
   "source": [
    "hf_arch, hf_config, hf_tokenizer, hf_model = get_task_hf_objects(\"microsoft/deberta-v3-small\", civil_label_names, verbose=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: `Datasets` (fastai)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8cd7ff5806b148839adab280f7d5d9b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# tokenize the dataset\n",
    "tokenize_func = partial(multilabel_tokenize_func, hf_tokenizer=hf_tokenizer, label_attrs=civil_label_names)\n",
    "proc_civil_ds = civil_ds.map(tokenize_func, batched=True)\n",
    "\n",
    "# turn Arrow into DataFrame (`ColSplitter` only works with `DataFrame`s)\n",
    "train_df = pd.DataFrame(proc_civil_ds)\n",
    "train_df.head()\n",
    "\n",
    "# define dataset splitter\n",
    "splitter = ColSplitter(\"is_valid\")\n",
    "splits = splitter(civil_df)\n",
    "\n",
    "\n",
    "# define how we want to build our inputs and targets\n",
    "def _build_inputs(example):\n",
    "    return {fwd_arg_name: example[fwd_arg_name] for fwd_arg_name in hf_tokenizer.model_input_names if fwd_arg_name in list(example.keys())}\n",
    "\n",
    "\n",
    "def _build_targets(example):\n",
    "    return example[\"label\"]\n",
    "\n",
    "\n",
    "# create our fastai `Datasets` object\n",
    "dsets = Datasets(items=train_df, splits=splits, tfms=[[_build_inputs], _build_targets], n_inp=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Items in train|validation datasets:  1000 200\n",
      "Items in each example: 2\n",
      "Example inputs: ['input_ids', 'token_type_ids', 'attention_mask']\n",
      "Example target(s): [1, 0, 0, 0, 1, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "print(\"Items in train|validation datasets: \", len(dsets.train), len(dsets.valid))\n",
    "\n",
    "example = dsets.valid[0]\n",
    "# example\n",
    "\n",
    "print(f\"Items in each example: {len(example)}\")\n",
    "print(f\"Example inputs: {list(example[0].keys())}\")\n",
    "print(f\"Example target(s): {example[1]}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3: `DataLoaders` (fastai)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = TextCollatorWithPadding(hf_tokenizer)\n",
    "sort_func = partial(sorted_dl_func, hf_tokenizer=hf_tokenizer)\n",
    "batch_decode_tfm = BatchDecodeTransform(hf_tokenizer, hf_arch, hf_config, hf_model, label_names=civil_label_names)\n",
    "\n",
    "dls = dsets.dataloaders(\n",
    "    batch_size=4,\n",
    "    create_batch=data_collator,\n",
    "    after_batch=batch_decode_tfm,\n",
    "    dl_type=partial(SortedDL, sort_func=sort_func),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of batches in train|validation dataloaders: 250 25\n",
      "# of items in each batch: 2\n",
      "\n",
      "Decoded input_ids: [CLS] \"Fr. Martin envisions a bridge that one ... \n",
      "Targets: tensor([[0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0]])\n"
     ]
    }
   ],
   "source": [
    "print(\"# of batches in train|validation dataloaders:\", len(train_dl), len(valid_dl))\n",
    "\n",
    "b = next(iter(train_dl))\n",
    "print(\"# of items in each batch:\", len(b))\n",
    "print(\"\")\n",
    "print(f\"Decoded input_ids: {hf_tokenizer.decode(b[0]['input_ids'][0][:10])} ... \")\n",
    "print(\"Targets:\", b[1])\n",
    "\n",
    "# b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>We live in an age of hysteria and histrionics. Critical, rational thinking is needed to deal with the deluge of propaganda emanating from the Grope &amp; Flail and the Toronto Red Star. Head for the hills!!</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>JimmyJ: You, sir, are the BIGLY WINNER. Your prize. On February 3rd, at 6:00 a number of us are gathering at Planktown in Springfield for a beer or soda pop (sarsaparilla) and a visit......Your prize is a free beer (orsarsaparilla) on me. Actually, it should be fun....if you can make it, please do. Should you have any questions, please call me. best regards, Gary</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dls.show_batch(dataloaders=dls, max_n=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |echo:false\n",
    "try:\n",
    "    del dls, hf_model\n",
    "except:\n",
    "    pass\n",
    "finally:\n",
    "    clean_memory()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mid-Level API\n",
    "\n",
    "BLURR's mid-level API provides a way to build your `DataLoaders` using fast.ai's mid-level `DataBlock` API utilizing one of these three tokenization strategies:\n",
    "\n",
    "1. Using pre-tokenized data (the traditional approach)\n",
    "\n",
    "2. batch-time tokenization (the default approach in previous versions of blurr)\n",
    "\n",
    "2. item-time tokenization (e.g., to apply tokenization on individual items as they are pulled from their respective `Dataset`)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `BatchTokenizeTransform` -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "class BatchTokenizeTransform(Transform):\n",
    "    \"\"\"\n",
    "    Handles everything you need to assemble a mini-batch of inputs and targets, as well as\n",
    "    decode the dictionary produced as a byproduct of the tokenization process in the `encodes` method.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        # The abbreviation/name of your Hugging Face transformer architecture (e.b., bert, bart, etc..)\n",
    "        hf_arch: str,\n",
    "        # A specific configuration instance you want to use\n",
    "        hf_config: PretrainedConfig,\n",
    "        # A Hugging Face tokenizer\n",
    "        hf_tokenizer: PreTrainedTokenizerBase,\n",
    "        # A Hugging Face model\n",
    "        hf_model: PreTrainedModel,\n",
    "        # To control whether the \"include_labels\" are included in your inputs. If they are, the loss will be calculated in \\\n",
    "        # the model's forward function and you can simply use `PreCalculatedLoss` as your `Learner`'s loss function to use it\n",
    "        include_labels: bool = True,\n",
    "        # The token ID that should be ignored when calculating the loss\n",
    "        ignore_token_id: int = CrossEntropyLossFlat().ignore_index,\n",
    "        # To control the length of the padding/truncation. It can be an integer or None, \\\n",
    "        # in which case it will default to the maximum length the model can accept. \\\n",
    "        # If the model has no specific maximum input length, truncation/padding to max_length is deactivated. \\\n",
    "        # See [Everything you always wanted to know about padding and truncation](https://huggingface.co/transformers/preprocessing.html#everything-you-always-wanted-to-know-about-padding-and-truncation)\n",
    "        max_length: int = None,\n",
    "        # To control the `padding` applied to your `hf_tokenizer` during tokenization. \\\n",
    "        # If None, will default to 'False' or 'do_not_pad'. \\\n",
    "        # See [Everything you always wanted to know about padding and truncation](https://huggingface.co/transformers/preprocessing.html#everything-you-always-wanted-to-know-about-padding-and-truncation)\n",
    "        padding: bool | str = True,\n",
    "        # To control `truncation` applied to your `hf_tokenizer` during tokenization. \\\n",
    "        # If None, will default to 'False' or 'do_not_truncate'. \\\n",
    "        # See [Everything you always wanted to know about padding and truncation](https://huggingface.co/transformers/preprocessing.html#everything-you-always-wanted-to-know-about-padding-and-truncation)\n",
    "        truncation: bool | str = True,\n",
    "        # The `is_split_into_words` argument applied to your `hf_tokenizer` during tokenization. \\\n",
    "        # Set this to 'True' if your inputs are pre-tokenized (not numericalized) \\\n",
    "        is_split_into_words: bool = False,\n",
    "        # Any other keyword arguments you want included when using your `hf_tokenizer` to tokenize your inputs\n",
    "        tok_kwargs: dict = {},\n",
    "        # Keyword arguments to apply to `BatchTokenizeTransform`\n",
    "        **kwargs,\n",
    "    ):\n",
    "        store_attr()\n",
    "        self.kwargs = kwargs\n",
    "\n",
    "    def encodes(self, samples, return_batch_encoding=False):\n",
    "        \"\"\"\n",
    "        This method peforms on-the-fly, batch-time tokenization of your data. In other words, your raw inputs\n",
    "        are tokenized as needed for each mini-batch of data rather than requiring pre-tokenization of your full\n",
    "        dataset ahead of time.\n",
    "        \"\"\"\n",
    "        samples = L(samples)\n",
    "\n",
    "        # grab inputs\n",
    "        is_dict = isinstance(samples[0][0], dict)\n",
    "        test_inp = samples[0][0][\"text\"] if is_dict else samples[0][0]\n",
    "\n",
    "        if is_listy(test_inp) and not self.is_split_into_words:\n",
    "            if is_dict:\n",
    "                inps = [(item[\"text\"][0], item[\"text\"][1]) for item in samples.itemgot(0).items]\n",
    "            else:\n",
    "                inps = list(zip(samples.itemgot(0, 0), samples.itemgot(0, 1)))\n",
    "        else:\n",
    "            inps = [item[\"text\"] for item in samples.itemgot(0).items] if is_dict else samples.itemgot(0).items\n",
    "\n",
    "        inputs = self.hf_tokenizer(\n",
    "            inps,\n",
    "            max_length=self.max_length,\n",
    "            padding=self.padding,\n",
    "            truncation=self.truncation,\n",
    "            is_split_into_words=self.is_split_into_words,\n",
    "            return_tensors=\"pt\",\n",
    "            **self.tok_kwargs,\n",
    "        )\n",
    "\n",
    "        d_keys = inputs.keys()\n",
    "\n",
    "        # update the samples with tokenized inputs (e.g. input_ids, attention_mask, etc...), as well as extra information\n",
    "        # if the inputs is a dictionary.\n",
    "        # (< 2.0.0): updated_samples = [(*[{k: inputs[k][idx] for k in d_keys}], *sample[1:]) for idx, sample in enumerate(samples)]\n",
    "        updated_samples = []\n",
    "        for idx, sample in enumerate(samples):\n",
    "            inps = {k: inputs[k][idx] for k in d_keys}\n",
    "            if is_dict:\n",
    "                inps = {\n",
    "                    **inps,\n",
    "                    **{k: v for k, v in sample[0].items() if k not in [\"text\"]},\n",
    "                }\n",
    "\n",
    "            trgs = sample[1:]\n",
    "            if self.include_labels and len(trgs) > 0:\n",
    "                inps[\"label\"] = trgs[0]\n",
    "\n",
    "            updated_samples.append((*[inps], *trgs))\n",
    "\n",
    "        if return_batch_encoding:\n",
    "            return updated_samples, inputs\n",
    "\n",
    "        return updated_samples"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspired by this [article](https://docs.fast.ai/tutorial.transformers.html), `BatchTokenizeTransform` inputs can come in as raw **text**, **a list of words** (e.g., tasks like Named Entity Recognition (NER), where you want to predict the label of each token), or as a **dictionary** that includes extra information you want to use during post-processing.\n",
    "\n",
    "Part of the inspiration for this derives from the mechanics of Hugging Face tokenizers, in particular it can return a collated mini-batch of data given a list of sequences. As such, the collating required for our inputs can be done during tokenization ***before*** our batch transforms run in a `before_batch_tfms` transform (where we get a list of examples)! This allows users of BLURR to have everything done dynamically at batch-time without prior preprocessing with at least four potential benefits:\n",
    "\n",
    "1. No need to pretokenize your text\n",
    "2. Less code\n",
    "3. Faster mini-batch creation\n",
    "4. Less RAM utilization and time spent tokenizing beforehand (this really helps with very large datasets)\n",
    "5. Batch level flexibility"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `ItemTokenizeTransform` -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "class ItemTokenizeTransform(ItemTransform):\n",
    "    split_idx = None\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        # A Hugging Face configuration object\n",
    "        hf_config: PretrainedConfig = None,\n",
    "        # A Hugging Face tokenizer\n",
    "        hf_tokenizer: PreTrainedTokenizerBase = None,\n",
    "        # Any keyword arguments you want your Hugging Face tokenizer to use during tokenization\n",
    "        tok_kwargs: dict = {},\n",
    "        # Any keyword arguments you want applied to `ItemTokenizeTransform`\n",
    "        **kwargs,\n",
    "    ) -> None:\n",
    "        store_attr()\n",
    "\n",
    "        if tok_kwargs.get(\"truncation\", None) is None:\n",
    "            tok_kwargs[\"truncation\"] = True\n",
    "        if tok_kwargs.get(\"max_length\", None) is None:\n",
    "            tok_kwargs[\"max_length\"] = True\n",
    "\n",
    "    def encodes(self, txt, **kwargs):\n",
    "        inputs = self.hf_tokenizer(txt, **self.tok_kwargs)\n",
    "        return dict(inputs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whereas the `BatchTokenizeTransform` allows you to tokenize text at batch-time, `ItemTokenizeTransform` allows you to tokenize text at item-time (e.g., when you get an item out of the dataset). This may be very useful if applying some form of data augmentation to your inputs.\n",
    "\n",
    "In order for this transform to run when getting an item, it needs to be specified as one of your `DataBlock`s `type_tfms`.\n",
    "\n",
    "Potential benefits:\n",
    "\n",
    "1. No need to pretokenize your text\n",
    "2. The ability to apply data augmentation each time an item is pulled from your dataset\n",
    "3. Less RAM utilization and time spent tokenizing beforehand (this really helps with very large datasets)\n",
    "4. Item level flexibility"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `TextBlock` -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "class TextBlock(TransformBlock):\n",
    "    \"\"\"The core `TransformBlock` to prepare your inputs for training in Blurr with fastai's `DataBlock` API\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        # The abbreviation/name of your Hugging Face transformer architecture (not required if passing in an \\\n",
    "        # instance of `BatchTokenizeTransform` to `before_batch_tfm`)\n",
    "        hf_arch: str = None,\n",
    "        # A Hugging Face configuration object (not required if passing in an \\\n",
    "        # instance of `BatchTokenizeTransform` to `before_batch_tfm`)\n",
    "        hf_config: PretrainedConfig = None,\n",
    "        # A Hugging Face tokenizer (not required if passing in an \\\n",
    "        # instance of `BatchTokenizeTransform` to `before_batch_tfm`)\n",
    "        hf_tokenizer: PreTrainedTokenizerBase = None,\n",
    "        # A Hugging Face model (not required if passing in an \\\n",
    "        # instance of `BatchTokenizeTransform` to `before_batch_tfm`)\n",
    "        hf_model: PreTrainedModel = None,\n",
    "        # Any transforms to apply when getting an item from a dataset (useufl for item-time tokenization)\n",
    "        type_tfms: list[ItemTokenizeTransform] = None,\n",
    "        # The \"before_batch\" transform you want to use if tokenizing your raw data on the fly (optional)\n",
    "        tokenize_tfm: Transform = None,\n",
    "        # The batch_tfm you want to decode your inputs into a type that can be used in the fastai show methods, \\\n",
    "        # (defaults to BatchDecodeTransform)\n",
    "        batch_decode_tfm: BatchDecodeTransform = None,\n",
    "        # An instance of `TextCollatorWithPadding` to use when not performing batch-time tokenization, \\\n",
    "        # (defaults to `TextCollatorWithPadding` when using pretokenized or item-time tokenization)\n",
    "        data_collator: TextCollatorWithPadding = None,\n",
    "        # To control whether the \"include_labels\" are included in your inputs. If they are, the loss will be calculated in \\\n",
    "        # the model's forward function and you can simply use `PreCalculatedLoss` as your `Learner`'s loss function to use it\n",
    "        include_labels: bool = True,\n",
    "        # The `is_split_into_words` argument applied to your `hf_tokenizer` during tokenization. \\\n",
    "        # Set this to `True` if your inputs are pre-tokenized (not numericalized)\n",
    "        is_split_into_words: bool = False,\n",
    "        # The return type your decoded inputs should be cast too (used by methods such as `show_batch`)\n",
    "        input_return_type: type = TextInput,\n",
    "        # The type of `DataLoader` you want created (defaults to `SortedDL`)\n",
    "        dl_type: DataLoader = None,\n",
    "        # Any keyword arguments you want applied to your `batch_decode_tfm` (will be set as a fastai `batch_tfms`)\n",
    "        batch_decode_kwargs: dict = {},\n",
    "        # Any keyword arguments you want your Hugging Face tokenizer to use during tokenization\n",
    "        tok_kwargs: dict = {},\n",
    "        # Any keyword arguments you want to have applied with generating text\n",
    "        text_gen_kwargs: dict = {},\n",
    "        # Any keyword arguments you want applied to `TextBlock`\n",
    "        **kwargs,\n",
    "    ):\n",
    "        if (not all([hf_arch, hf_config, hf_tokenizer, hf_model])) and tokenize_tfm is None:\n",
    "            raise ValueError(\"You must supply an hf_arch, hf_config, hf_tokenizer, hf_model -or- a tokenize_tfm\")\n",
    "\n",
    "        # if we are using a transform to tokenize our inputs, grab the HF objects from it\n",
    "        if tokenize_tfm is not None:\n",
    "            hf_arch = getattr(tokenize_tfm, \"hf_arch\", hf_arch)\n",
    "            hf_config = getattr(tokenize_tfm, \"hf_config\", hf_config)\n",
    "            hf_tokenizer = getattr(tokenize_tfm, \"hf_tokenizer\", hf_tokenizer)\n",
    "            hf_model = getattr(tokenize_tfm, \"hf_model\", hf_model)\n",
    "            is_split_into_words = getattr(tokenize_tfm, \"is_split_into_words\", is_split_into_words)\n",
    "            include_labels = getattr(tokenize_tfm, \"include_labels\", include_labels)\n",
    "\n",
    "        # configure our batch decode transform (used by show_batch/results methods)\n",
    "        if batch_decode_tfm is None:\n",
    "            batch_decode_tfm = BatchDecodeTransform(\n",
    "                hf_arch=hf_arch,\n",
    "                hf_config=hf_config,\n",
    "                hf_tokenizer=hf_tokenizer,\n",
    "                hf_model=hf_model,\n",
    "                input_return_type=input_return_type,\n",
    "                **batch_decode_kwargs.copy(),\n",
    "            )\n",
    "\n",
    "        # default to SortedDL using our custom sort function if no `dl_type` is specified\n",
    "        if dl_type is None:\n",
    "            dl_sort_func = partial(\n",
    "                sorted_dl_func, hf_tokenizer=hf_tokenizer, is_split_into_words=is_split_into_words, tok_kwargs=tok_kwargs.copy()\n",
    "            )\n",
    "            dl_type = partial(SortedDL, sort_func=dl_sort_func)\n",
    "\n",
    "        # build our custom `TransformBlock`\n",
    "        if tokenize_tfm is None:\n",
    "            if data_collator is None:\n",
    "                data_collator = TextCollatorWithPadding(hf_tokenizer)\n",
    "            dl_kwargs = {\"create_batch\": data_collator}\n",
    "        else:\n",
    "            dl_kwargs = {\"before_batch\": tokenize_tfm}\n",
    "\n",
    "        return super().__init__(dl_type=dl_type, dls_kwargs=dl_kwargs, type_tfms=type_tfms, batch_tfms=batch_decode_tfm)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mid-Level API: Examples"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pretokenized"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multiclass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 1: HF objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== config ===\n",
      "# of labels:\t2\n",
      "\n",
      "=== tokenizer ===\n",
      "Vocab size:\t\t128000\n",
      "Max # of tokens:\t1000000000000000019884624838656\n",
      "Attributes expected by model in forward pass:\t['input_ids', 'token_type_ids', 'attention_mask']\n"
     ]
    }
   ],
   "source": [
    "hf_arch, hf_config, hf_tokenizer, hf_model = get_task_hf_objects(\"microsoft/deberta-v3-small\", label_names, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  Step 2: `DataBlock`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define DataBlock splitter\n",
    "def _split_func(example):\n",
    "    return example[\"is_valid\"] == True\n",
    "\n",
    "\n",
    "# define how we want to build our targets\n",
    "# note: we don't need to define how to build our inputs because we're using an HF `Dataset` in this example\n",
    "def get_y(example):\n",
    "    return example[\"label\"]\n",
    "\n",
    "\n",
    "# define the DataBlock\n",
    "txt_block = TextBlock(\n",
    "    hf_arch=hf_arch, hf_config=hf_config, hf_tokenizer=hf_tokenizer, hf_model=hf_model, batch_decode_kwargs={\"label_names\": label_names}\n",
    ")\n",
    "\n",
    "blocks = (txt_block, CategoryBlock)\n",
    "dblock = DataBlock(blocks=blocks, get_y=get_y, splitter=FuncSplitter(_split_func))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 3: `DataLoaders`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/wgilliam/.cache/huggingface/datasets/imdb/plain_text/1.0.0/d613c88cf8fa3bab83b4ded3713f1f74830d1100e171db75bbddb80b3345c9c0/cache-172edc75a088c5ea.arrow\n"
     ]
    }
   ],
   "source": [
    "# tokenize the dataset\n",
    "tokenize_func = partial(multiclass_tokenize_func, hf_tokenizer=hf_tokenizer)\n",
    "proc_imdb_ds = imdb_ds.map(tokenize_func, batched=True)\n",
    "\n",
    "# build your `DataLoaders`\n",
    "dls = dblock.dataloaders(proc_imdb_ds, bs=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of items in each batch: 2\n",
      "# of inputs in each batch: 4\n",
      "# of targets in each batch: 4\n",
      "Shape of input_ids (bsz, seq): torch.Size([4, 1742])\n"
     ]
    }
   ],
   "source": [
    "b = dls.one_batch()\n",
    "print(\"# of items in each batch:\", len(b))\n",
    "print(\"# of inputs in each batch:\", len(b[0][\"input_ids\"]))\n",
    "print(\"# of targets in each batch:\", len(b[1]))\n",
    "print(\"Shape of input_ids (bsz, seq):\", b[0][\"input_ids\"].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the actual types represented by our batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{tuple: [dict, fastai.torch_core.TensorCategory]}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "explode_types(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I've read a number of reviews on this film and I have to say \"What is wrong with you people?!?!\" This was an excellent film! I thought this film was superb from start to finish and the story was extremely well told. I'm convinced that the people that didn't like this film weren't paying very good attention to the film. There are a number of very important scenes that if you aren't paying attention you will be confused and the following scenes may not make sense. I urge anyone who didn't like thi</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>this is one of the funniest shows i have ever seen. it is really refreshing to watch and i was in stitches many times. i guess there is a social awareness factor to this too which makes it quite interesting. if these were white girls would they get the same reaction? maybe they would, maybe they wouldn't? the characters know no limits (check my lyrics) and do not exclude anyone from their twisted sense of fun!There are so many funny sketches. my favorites is the bob the builder one. it's so sill</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dls.show_batch(dataloaders=dls, max_n=2, trunc_at=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |echo:false\n",
    "try:\n",
    "    del dls, hf_model\n",
    "except:\n",
    "    pass\n",
    "finally:\n",
    "    clean_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multilabel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 1: HF objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== config ===\n",
      "# of labels:\t7\n",
      "\n",
      "=== tokenizer ===\n",
      "Vocab size:\t\t128000\n",
      "Max # of tokens:\t1000000000000000019884624838656\n",
      "Attributes expected by model in forward pass:\t['input_ids', 'token_type_ids', 'attention_mask']\n"
     ]
    }
   ],
   "source": [
    "hf_arch, hf_config, hf_tokenizer, hf_model = get_task_hf_objects(\"microsoft/deberta-v3-small\", civil_label_names, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  Step 2: `DataBlock`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define DataBlock splitter\n",
    "def _split_func(example):\n",
    "    return example[\"is_valid\"] == True\n",
    "\n",
    "\n",
    "# define how we want to build our targets\n",
    "# note: we don't need to define how to build our inputs because we're using an HF `Dataset` in this example\n",
    "def get_y(example):\n",
    "    return example[\"label\"]\n",
    "\n",
    "\n",
    "# define the DataBlock\n",
    "blocks = (\n",
    "    TextBlock(hf_arch=hf_arch, hf_config=hf_config, hf_tokenizer=hf_tokenizer, hf_model=hf_model),\n",
    "    MultiCategoryBlock(encoded=True, vocab=civil_label_names),\n",
    ")\n",
    "dblock = DataBlock(blocks=blocks, get_y=get_y, splitter=FuncSplitter(_split_func))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 3: `DataLoaders`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/wgilliam/.cache/huggingface/datasets/civil_comments/default/0.9.0/e7a3aacd2ab7d135fa958e7209d10b1fa03807d44c486e3c34897aa08ea8ffab/cache-1f3711404ab09b4a.arrow\n"
     ]
    }
   ],
   "source": [
    "# tokenize the dataset\n",
    "tokenize_func = partial(multilabel_tokenize_func, hf_tokenizer=hf_tokenizer, label_attrs=civil_label_names)\n",
    "proc_civil_ds = civil_ds.map(tokenize_func, batched=True)\n",
    "\n",
    "dls = dblock.dataloaders(proc_civil_ds, bs=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of items in each batch: 2\n",
      "# of inputs in each batch: 4\n",
      "# of targets in each batch: 4\n",
      "Shape of input_ids (bsz, seq): torch.Size([4, 244])\n"
     ]
    }
   ],
   "source": [
    "b = dls.one_batch()\n",
    "print(\"# of items in each batch:\", len(b))\n",
    "print(\"# of inputs in each batch:\", len(b[0][\"input_ids\"]))\n",
    "print(\"# of targets in each batch:\", len(b[1]))\n",
    "print(\"Shape of input_ids (bsz, seq):\", b[0][\"input_ids\"].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the actual types represented by our batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{tuple: [dict, fastai.torch_core.TensorMultiCategory]}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "explode_types(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>We live in an age of hysteria and histrionics. Critical, rational thinking is needed to deal with the deluge of propaganda emanating from the Grope &amp; Flail and the Toronto Red Star. Head for the hills!!</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Yes. Eat meat, just eat less of it and push for policies that ensure animals are treated as humanely as possible. Same for wearing animal products. Choose non animal products first. When we know better, we do better. I don't eat pate and I won't be buying down products in the future.</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dls.show_batch(dataloaders=dls, max_n=2, trunc_at=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |echo:false\n",
    "try:\n",
    "    del dls, hf_model\n",
    "except:\n",
    "    pass\n",
    "finally:\n",
    "    clean_memory()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch-Time Tokenization"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multiclass"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 1: HF objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== config ===\n",
      "# of labels:\t2\n",
      "\n",
      "=== tokenizer ===\n",
      "Vocab size:\t\t128000\n",
      "Max # of tokens:\t1000000000000000019884624838656\n",
      "Attributes expected by model in forward pass:\t['input_ids', 'token_type_ids', 'attention_mask']\n"
     ]
    }
   ],
   "source": [
    "hf_arch, hf_config, hf_tokenizer, hf_model = get_task_hf_objects(\"microsoft/deberta-v3-small\", label_names, verbose=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  Step 2: `DataBlock`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenize_tfm = BatchTokenizeTransform(hf_arch, hf_config, hf_tokenizer, hf_model)\n",
    "\n",
    "# note: we pass the label_names here because the labels in the dataset are already encoded as 0 or 1\n",
    "blocks = (\n",
    "    TextBlock(tokenize_tfm=tokenize_tfm, batch_decode_kwargs={\"label_names\": label_names}),\n",
    "    CategoryBlock,\n",
    ")\n",
    "dblock = DataBlock(\n",
    "    blocks=blocks,\n",
    "    get_x=ColReader(\"text\"),\n",
    "    get_y=ColReader(\"label\"),\n",
    "    splitter=ColSplitter(),\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 3: `DataLoaders`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dls = dblock.dataloaders(imdb_df, bs=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of items in each batch: 2\n",
      "# of inputs in each batch: 4\n",
      "# of targets in each batch: 4\n",
      "Shape of input_ids (bsz, seq): torch.Size([4, 1742])\n"
     ]
    }
   ],
   "source": [
    "b = dls.one_batch()\n",
    "print(\"# of items in each batch:\", len(b))\n",
    "print(\"# of inputs in each batch:\", len(b[0][\"input_ids\"]))\n",
    "print(\"# of targets in each batch:\", len(b[1]))\n",
    "print(\"Shape of input_ids (bsz, seq):\", b[0][\"input_ids\"].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the actual types represented by our batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{tuple: [dict, fastai.torch_core.TensorCategory]}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "explode_types(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CitizenX(1995) is the developing world's answer to Silence of the Lambs. Where `Silence' terrorized our peace of mind, `Citizen' exhausts and saddens us instead. This dramatization of the Chikatilo case translates rather well, thanks to a Westernized friendship between two Rostov cops who become equals.&lt;br /&gt;&lt;br /&gt;CitizenX may also argue against(!) the death penalty far better than Kevin Spacey's The Life of David Gayle(2002).&lt;br /&gt;&lt;br /&gt;Humans are Machiavellian mammals, under which lie limbic b</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>In 1983, Director Brian De Palma set out to make a film about the rise and fall of an American gangster, and that he did-- with the help of a terrific screenplay by Oliver Stone and some impeccable work by an outstanding cast. The result was `Scarface,' starring Al Pacino in one of his most memorable roles. The story begins in May of 1980, when Castro opened the harbor at Mariel, Cuba, to allow Cuban nationals to join their families in the United States. 125,000 left Cuba at that time, for the g</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dls.show_batch(dataloaders=dls, max_n=2, trunc_at=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |echo:false\n",
    "try:\n",
    "    del dls, hf_model\n",
    "except:\n",
    "    pass\n",
    "finally:\n",
    "    clean_memory()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multilabel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 1: HF objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== config ===\n",
      "# of labels:\t7\n",
      "\n",
      "=== tokenizer ===\n",
      "Vocab size:\t\t128000\n",
      "Max # of tokens:\t1000000000000000019884624838656\n",
      "Attributes expected by model in forward pass:\t['input_ids', 'token_type_ids', 'attention_mask']\n"
     ]
    }
   ],
   "source": [
    "hf_arch, hf_config, hf_tokenizer, hf_model = get_task_hf_objects(\"microsoft/deberta-v3-small\", civil_label_names, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  Step 2: `DataBlock`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenize_tfm = BatchTokenizeTransform(hf_arch, hf_config, hf_tokenizer, hf_model)\n",
    "\n",
    "blocks = (TextBlock(tokenize_tfm=tokenize_tfm), MultiCategoryBlock(encoded=True, vocab=civil_label_names))\n",
    "dblock = DataBlock(blocks=blocks, get_x=ColReader(\"text\"), get_y=ColReader(civil_label_names), splitter=ColSplitter())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 3: `DataLoaders`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dls = dblock.dataloaders(civil_df, bs=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of items in each batch: 2\n",
      "# of inputs in each batch: 4\n",
      "# of targets in each batch: 4\n",
      "Shape of input_ids (bsz, seq): torch.Size([4, 244])\n"
     ]
    }
   ],
   "source": [
    "b = dls.one_batch()\n",
    "print(\"# of items in each batch:\", len(b))\n",
    "print(\"# of inputs in each batch:\", len(b[0][\"input_ids\"]))\n",
    "print(\"# of targets in each batch:\", len(b[1]))\n",
    "print(\"Shape of input_ids (bsz, seq):\", b[0][\"input_ids\"].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the actual types represented by our batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{tuple: [dict, fastai.torch_core.TensorMultiCategory]}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "explode_types(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>You're much more generous than I could be on this subject. Little doubt this article (press release) is for the purpose of outrage, and intended for monies to mysteriously be available. After all, the biennial budget for WSU is $1,539,578,000, i.e. $1.54 Billion! Have a look for a laugh: http://fiscal.wa.gov/BudgetOAgy.aspx Select under \"Agency\", Washington State University. Then, have the laugh that they're dropping $13 million of stuff designed for public outcry while retaining a cool $1 milli</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The Congress gets to choose. They have been doing that since the beginning of the country. The ability for the Congress, the President and the Attorney General to do whatever they want in their sole discretion under those statutes goes back that far. The real restrictions have come from Congress, mostly since 1952 and on. The last Immigration case was Sessions v. Morales-Santana. (2017) In that case, they stuck closely to what the Congress had actually written in the law and ruled against the im</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dls.show_batch(dataloaders=dls, max_n=2, trunc_at=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |echo:false\n",
    "try:\n",
    "    del dls, hf_model\n",
    "except:\n",
    "    pass\n",
    "finally:\n",
    "    clean_memory()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Item-Time Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multiclass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 1: HF objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== config ===\n",
      "# of labels:\t2\n",
      "\n",
      "=== tokenizer ===\n",
      "Vocab size:\t\t128000\n",
      "Max # of tokens:\t1000000000000000019884624838656\n",
      "Attributes expected by model in forward pass:\t['input_ids', 'token_type_ids', 'attention_mask']\n"
     ]
    }
   ],
   "source": [
    "hf_arch, hf_config, hf_tokenizer, hf_model = get_task_hf_objects(\"microsoft/deberta-v3-small\", label_names, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  Step 2: `DataBlock`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfm = ItemTokenizeTransform(hf_config=hf_config, hf_tokenizer=hf_tokenizer)\n",
    "\n",
    "blocks = (\n",
    "    TextBlock(\n",
    "        hf_arch=hf_arch,\n",
    "        hf_config=hf_config,\n",
    "        hf_tokenizer=hf_tokenizer,\n",
    "        hf_model=hf_model,\n",
    "        type_tfms=[tfm],\n",
    "        batch_decode_kwargs={\"label_names\": label_names},\n",
    "    ),\n",
    "    CategoryBlock,\n",
    ")\n",
    "\n",
    "dblock = DataBlock(\n",
    "    blocks=blocks,\n",
    "    get_x=ColReader(\"text\"),\n",
    "    get_y=ColReader(\"label\"),\n",
    "    splitter=ColSplitter(),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 3: `DataLoaders`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dls = dblock.dataloaders(imdb_df, bs=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of items in each batch: 2\n",
      "# of inputs in each batch: 4\n",
      "# of targets in each batch: 4\n",
      "Shape of input_ids (bsz, seq): torch.Size([4, 521])\n"
     ]
    }
   ],
   "source": [
    "b = dls.valid.one_batch()\n",
    "print(\"# of items in each batch:\", len(b))\n",
    "print(\"# of inputs in each batch:\", len(b[0][\"input_ids\"]))\n",
    "print(\"# of targets in each batch:\", len(b[1]))\n",
    "print(\"Shape of input_ids (bsz, seq):\", b[0][\"input_ids\"].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the actual types represented by our batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{tuple: [dict, fastai.torch_core.TensorCategory]}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "explode_types(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Dreadful! A friend of mine (who obviously thought I had an abysmal sense of humour) recommended this.&lt;br /&gt;&lt;br /&gt;It's bobbins. I almost switched it off. It is only my anal desire to not leave things unfinished that prevented me doing so.&lt;br /&gt;&lt;br /&gt;This was evidently a British attempt to make a movie with a bunch of also ran TV actors using some lame script from their mate in the business. I struggle to think of anything even approaching the paucity of this movie. Less funny than global warming.</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>First, I'm sorry for my English. Second, the true story of this episode: 39 soldiers, operation \"Magistral'\". 6 soldiers were killed. Hundreds of insurgents were killed too. Within 10 years the Soviet Army has lost less than 15 thousand person and killed over 900000 insurgents and civilians. There is no insurgents without permanent help of USA The veteran of war: \"Traditions. There are no traditions in this film. There is no military oath, there is no first jump, no farewell to the Fighting Bann</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dls.valid.show_batch(dataloaders=dls, max_n=2, trunc_at=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |echo:false\n",
    "try:\n",
    "    del dls, hf_model\n",
    "except:\n",
    "    pass\n",
    "finally:\n",
    "    clean_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multilabel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 1: HF objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== config ===\n",
      "# of labels:\t7\n",
      "\n",
      "=== tokenizer ===\n",
      "Vocab size:\t\t128000\n",
      "Max # of tokens:\t1000000000000000019884624838656\n",
      "Attributes expected by model in forward pass:\t['input_ids', 'token_type_ids', 'attention_mask']\n"
     ]
    }
   ],
   "source": [
    "hf_arch, hf_config, hf_tokenizer, hf_model = get_task_hf_objects(\"microsoft/deberta-v3-small\", civil_label_names, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  Step 2: `DataBlock`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfm = ItemTokenizeTransform(hf_config=hf_config, hf_tokenizer=hf_tokenizer)\n",
    "\n",
    "blocks = (\n",
    "    TextBlock(hf_arch=hf_arch, hf_config=hf_config, hf_tokenizer=hf_tokenizer, hf_model=hf_model, type_tfms=[tfm]),\n",
    "    MultiCategoryBlock(encoded=True, vocab=civil_label_names),\n",
    ")\n",
    "\n",
    "dblock = DataBlock(\n",
    "    blocks=blocks,\n",
    "    get_x=ColReader(\"text\"),\n",
    "    get_y=ColReader(civil_label_names),\n",
    "    splitter=ColSplitter(),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 3: `DataLoaders`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dls = dblock.dataloaders(civil_df, bs=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of items in each batch: 2\n",
      "# of inputs in each batch: 4\n",
      "# of targets in each batch: 4\n",
      "Shape of input_ids (bsz, seq): torch.Size([4, 168])\n"
     ]
    }
   ],
   "source": [
    "b = dls.valid.one_batch()\n",
    "print(\"# of items in each batch:\", len(b))\n",
    "print(\"# of inputs in each batch:\", len(b[0][\"input_ids\"]))\n",
    "print(\"# of targets in each batch:\", len(b[1]))\n",
    "print(\"Shape of input_ids (bsz, seq):\", b[0][\"input_ids\"].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the actual types represented by our batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{tuple: [dict, fastai.torch_core.TensorMultiCategory]}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "explode_types(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>....and useful idiots.</td>\n",
       "      <td>[toxicity, insult]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Even some Canadians have been implicated; UN figures show that three Canadian police officers deployed to Haiti have been accused of sexual abuse or exploitation since 2015. ----- Nice. It's not enough that the RCMP are attacking female RCMP members and women in Canada, who are somewhat better equipped to stand up against the bullying, harassment and violence, the RCMP are now exporting their criminality and sexually exploitative behavior to highly vulnerable young women in 3rd world countries.</td>\n",
       "      <td>[toxicity, insult]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dls.valid.show_batch(dataloaders=dls, max_n=2, trunc_at=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |echo:false\n",
    "try:\n",
    "    del dls, hf_model\n",
    "except:\n",
    "    pass\n",
    "finally:\n",
    "    clean_memory()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tests\n",
    "\n",
    "The tests below to ensure the core DataBlock code above works for **all** pretrained sequence classification models available in Hugging Face.  These tests are excluded from the CI workflow because of how long they would take to run and the amount of data that would be required to download.\n",
    "\n",
    "**Note**: Feel free to modify the code below to test whatever pretrained classification models you are working with ... and if any of your pretrained sequence classification models fail, please submit a github issue *(or a PR if you'd like to fix it yourself)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "import nbdev\n",
    "\n",
    "nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
