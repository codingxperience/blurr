{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "skip_exec: true\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Core: Data\n",
    "\n",
    "> The `data.core` module contains the core bits required to use fast.ai's low-level and/or mid-level APIs to define `Datasets` and build `DataLoaders` suitable for training transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |default_exp data.core\n",
    "# |default_cls_lvl 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "from __future__ import annotations\n",
    "\n",
    "import gc, importlib, sys, traceback\n",
    "\n",
    "from accelerate.logging import get_logger\n",
    "from dataclasses import dataclass\n",
    "from dotenv import load_dotenv\n",
    "from fastai.imports import *\n",
    "from fastai.losses import CrossEntropyLossFlat\n",
    "from fastai.data.block import TransformBlock\n",
    "from fastai.data.transforms import DataLoaders, ItemTransform, Transform\n",
    "from fastai.text.data import SortedDL\n",
    "from fastai.torch_core import *\n",
    "from fastai.torch_imports import *\n",
    "from transformers import PretrainedConfig, PreTrainedTokenizerBase, PreTrainedModel, AutoModelForSequenceClassification\n",
    "from transformers import logging as hf_logging\n",
    "from transformers.data.data_collator import DataCollatorWithPadding\n",
    "\n",
    "from blurr.utils import get_hf_objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |hide\n",
    "import pdb, nbdev\n",
    "\n",
    "from datasets import Dataset\n",
    "from datasets import concatenate_datasets, load_dataset, Dataset, Value\n",
    "from fastai.data.block import CategoryBlock, ColReader, ColSplitter, DataBlock, FuncSplitter, MultiCategoryBlock\n",
    "from fastai.data.transforms import DataLoader, DataLoaders, Datasets, ItemTransform\n",
    "from fastai.losses import BaseLoss, BCEWithLogitsLossFlat\n",
    "from fastcore.test import *\n",
    "from transformers import AutoConfig, AutoTokenizer\n",
    "\n",
    "from blurr.utils import clean_memory, print_versions, set_seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# |export\n",
    "# silence all the HF warnings and load environment variables\n",
    "warnings.simplefilter(\"ignore\")\n",
    "hf_logging.set_verbosity_error()\n",
    "logger = get_logger(__name__)\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU #0: NVIDIA GeForce RTX 3090\n"
     ]
    }
   ],
   "source": [
    "# |hide\n",
    "# |notest\n",
    "torch.cuda.set_device(0)\n",
    "print(f\"Using GPU #{torch.cuda.current_device()}: {torch.cuda.get_device_name()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What we're running with at the time this documentation was generated:\n",
      "torch: 1.13.1\n",
      "fastai: 2.7.11\n",
      "transformers: 4.26.1\n"
     ]
    }
   ],
   "source": [
    "# | echo: false\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "print(\"What we're running with at the time this documentation was generated:\")\n",
    "print_versions(\"torch fastai transformers\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "We'll use a subset of `imdb` to demonstrate how to configure your BLURR for sequence classification tasks. **BLURR** is designed to work with Hugging Face `Dataset` and/or pandas `DataFrame` objects"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset imdb (/home/wgilliam/.cache/huggingface/datasets/imdb/plain_text/1.0.0/d613c88cf8fa3bab83b4ded3713f1f74830d1100e171db75bbddb80b3345c9c0)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91338cb0b9b84cea8f32327004c9f3c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 200\n",
      "1000 200\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>is_valid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Except for the Brady Variety Hour, this was some of the hokiest television I've seen in a while. The video production qualities weren't too bad, but the overall look and feel were unmistakeably early 80's. And Marie Osmond looks like she did battle with the Avon Lady.. and lost big time. WAY too much eyeliner.&lt;br /&gt;&lt;br /&gt;It was kind of embarrassing to watch veterans Danny Kaye and Eric Severeid take part in this. Even more interesting was watching Alex Haley talk about the African Pavillion in World Showcase that would be opening 'in about a year.'&lt;br /&gt;&lt;br /&gt;As of this writing it is 17 ye...</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Was this meant to be a comedy or a serious drama? This film starts with a light-hearted banter between three women. Fine. It moves into a conflict between the women when one of them meets a man. Fine. There are a few antics between them. Fine. But when the plot thickens and finally becomes black I started to wonder whether I had misinterpreted the first part of the movie. It continues in this vein for a while until, in the end, it tries to go back to the original light-hearted banter. But by now it's too late. It's hard to see why these women would still be talking to one another and the f...</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I went into this film with expectations, from the hype, that it would be insightful and uplifting. Certainly something more than a cheap promotional for the band \"Wilco.\"&lt;br /&gt;&lt;br /&gt;Instead we get a lot of moping and whining about \"the process,\" a dishonorable and no doubt one-sided portrayal of one band members who was kicked out by the prima donna lead singer/songwriter, a gut-wrenching confession by the fallen member's friend -- for like 18 years -- saying the \"friendship had run its course,\" and this whiny, uncompelling story about how one record label \"hurt their feelings\" by dumping ...</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I don't think I'm too far off base saying that this is possibly the worst movie I've ever seen. I've been working on a list of my favorite war movies: \"The Longest Day,\" \"To Hell and Back,\" \"Bridge on the River Kwai,\" (all black and white) which all have good plots, rich characters and great acting. I've seen better dialog and acting in student-written high school one act plays. The plot, however isn't a bad premise - just poorly implemented. It's kind of like a traffic accident, though, I couldn't seem to turn it off! A movie doesn't need big money or great sets i.e. \"Twelve Angry Men\" an...</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>There are really two sections of this film. Firstly there's the laughable prologue to the film which is so hysterical and cornball that it would almost feel appropriate that the 'The Simpsons' Troy McClure should be doing the narration.&lt;br /&gt;&lt;br /&gt;Then the rest of the film begins (starting off with a title song which really doesn't fit in with the rest of the film) which, while technically OK, is killed by a vague, inconsistent and unconvincing plot and not just uninteresting characters, but characters that make no sense.&lt;br /&gt;&lt;br /&gt;This is especially so with Mickey Rooney's Spiventa, who ...</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      text  \\\n",
       "0  Except for the Brady Variety Hour, this was some of the hokiest television I've seen in a while. The video production qualities weren't too bad, but the overall look and feel were unmistakeably early 80's. And Marie Osmond looks like she did battle with the Avon Lady.. and lost big time. WAY too much eyeliner.<br /><br />It was kind of embarrassing to watch veterans Danny Kaye and Eric Severeid take part in this. Even more interesting was watching Alex Haley talk about the African Pavillion in World Showcase that would be opening 'in about a year.'<br /><br />As of this writing it is 17 ye...   \n",
       "1  Was this meant to be a comedy or a serious drama? This film starts with a light-hearted banter between three women. Fine. It moves into a conflict between the women when one of them meets a man. Fine. There are a few antics between them. Fine. But when the plot thickens and finally becomes black I started to wonder whether I had misinterpreted the first part of the movie. It continues in this vein for a while until, in the end, it tries to go back to the original light-hearted banter. But by now it's too late. It's hard to see why these women would still be talking to one another and the f...   \n",
       "2  I went into this film with expectations, from the hype, that it would be insightful and uplifting. Certainly something more than a cheap promotional for the band \"Wilco.\"<br /><br />Instead we get a lot of moping and whining about \"the process,\" a dishonorable and no doubt one-sided portrayal of one band members who was kicked out by the prima donna lead singer/songwriter, a gut-wrenching confession by the fallen member's friend -- for like 18 years -- saying the \"friendship had run its course,\" and this whiny, uncompelling story about how one record label \"hurt their feelings\" by dumping ...   \n",
       "3  I don't think I'm too far off base saying that this is possibly the worst movie I've ever seen. I've been working on a list of my favorite war movies: \"The Longest Day,\" \"To Hell and Back,\" \"Bridge on the River Kwai,\" (all black and white) which all have good plots, rich characters and great acting. I've seen better dialog and acting in student-written high school one act plays. The plot, however isn't a bad premise - just poorly implemented. It's kind of like a traffic accident, though, I couldn't seem to turn it off! A movie doesn't need big money or great sets i.e. \"Twelve Angry Men\" an...   \n",
       "4  There are really two sections of this film. Firstly there's the laughable prologue to the film which is so hysterical and cornball that it would almost feel appropriate that the 'The Simpsons' Troy McClure should be doing the narration.<br /><br />Then the rest of the film begins (starting off with a title song which really doesn't fit in with the rest of the film) which, while technically OK, is killed by a vague, inconsistent and unconvincing plot and not just uninteresting characters, but characters that make no sense.<br /><br />This is especially so with Mickey Rooney's Spiventa, who ...   \n",
       "\n",
       "   label  is_valid  \n",
       "0      0     False  \n",
       "1      0     False  \n",
       "2      0     False  \n",
       "3      0     False  \n",
       "4      0     False  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb_dsd = load_dataset(\"imdb\", split=[\"train\", \"test\"])\n",
    "\n",
    "# build HF `Dataset` objects\n",
    "train_ds = imdb_dsd[0].add_column(\"is_valid\", [False] * len(imdb_dsd[0])).shuffle().select(range(1000))\n",
    "valid_ds = imdb_dsd[1].add_column(\"is_valid\", [True] * len(imdb_dsd[1])).shuffle().select(range(200))\n",
    "imdb_ds = concatenate_datasets([train_ds, valid_ds])\n",
    "\n",
    "# build a `DataFrame` representation as well\n",
    "imdb_df = pd.DataFrame(imdb_ds)\n",
    "\n",
    "print(len(train_ds), len(valid_ds))\n",
    "print(len(imdb_df[imdb_df[\"is_valid\"] == False]), len(imdb_df[imdb_df[\"is_valid\"] == True]))\n",
    "imdb_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['neg', 'pos']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_names = imdb_dsd[0].features[\"label\"].names\n",
    "label_names"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multilabel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset civil_comments (/home/wgilliam/.cache/huggingface/datasets/civil_comments/default/0.9.0/e7a3aacd2ab7d135fa958e7209d10b1fa03807d44c486e3c34897aa08ea8ffab)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca939f030e414620aba9ab3a93c3edbd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4638b652e6447a88c077e3dad697867",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b5b6a4ef8cd47978a1764733d64d212",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Casting the dataset:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "547b6ab14e4149fc9fea4323ba16c770",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3a07b624069495e84ad71ce3efc26fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Casting the dataset:   0%|          | 0/200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 200\n",
      "1000 200\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>toxicity</th>\n",
       "      <th>severe_toxicity</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_attack</th>\n",
       "      <th>sexual_explicit</th>\n",
       "      <th>is_valid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>So what protects from a lot of low populated centers controlling the country?  \\n\\nI think there's some use for the EC but it would be better if we went with splitting the votes like NE and ME. More people would vote on both sides since their votes could/would actually count.</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I hear they give the money directly to charity on the condition that no one ever knows.\\nAnd I have as much evidence for that as you have for your slanderous accusation.\\nWhat a partisan you are.</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ivan, neither Helms or Thurmond were ever AG. How to I back up my facts about Holder and Lynch? Ivan, I can't, I won't and I don't have a second life, to reread to you, all of their sins since Obama took over. Just reading through Yahoo is enough evidence. You have to read it. But just for example, recently four blacks in Chicago were arrested for attacking a white voter whom the blacks accused of voting for Trump. There been no reaction from the DOJ. Now we all know for sure that if the color situation were reverse, and one black voter were beaten up by four whites because he voted for Hi...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I would challenge this notion of “inattentiveness” bandied about by many parties. Rather I would point to: speed. These trucks drive too fast; it is difficult to bring a 16 wheeler to a halt when it is travelling at the speed it has adopted and at a speed which this province has condoned!  Speed governor my foot! How is it, pray tell that the speed limit on truck speed governors are set at 105 kph (by law) when the speed limit on 400 class highways is set at 100 kph (also by law). Is that not a travesty of common sense? And a degradation of public safety, if not gross mismanagement?\\n\\nAs ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Kinda feel sorry for the \"kid\"....but why the hell are Americans going to N. Korea?  Are they STUPID or just DUMB?</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      text  \\\n",
       "0                                                                                                                                                                                                                                                                                                                                     So what protects from a lot of low populated centers controlling the country?  \\n\\nI think there's some use for the EC but it would be better if we went with splitting the votes like NE and ME. More people would vote on both sides since their votes could/would actually count.   \n",
       "1                                                                                                                                                                                                                                                                                                                                                                                                                      I hear they give the money directly to charity on the condition that no one ever knows.\\nAnd I have as much evidence for that as you have for your slanderous accusation.\\nWhat a partisan you are.   \n",
       "2  Ivan, neither Helms or Thurmond were ever AG. How to I back up my facts about Holder and Lynch? Ivan, I can't, I won't and I don't have a second life, to reread to you, all of their sins since Obama took over. Just reading through Yahoo is enough evidence. You have to read it. But just for example, recently four blacks in Chicago were arrested for attacking a white voter whom the blacks accused of voting for Trump. There been no reaction from the DOJ. Now we all know for sure that if the color situation were reverse, and one black voter were beaten up by four whites because he voted for Hi...   \n",
       "3  I would challenge this notion of “inattentiveness” bandied about by many parties. Rather I would point to: speed. These trucks drive too fast; it is difficult to bring a 16 wheeler to a halt when it is travelling at the speed it has adopted and at a speed which this province has condoned!  Speed governor my foot! How is it, pray tell that the speed limit on truck speed governors are set at 105 kph (by law) when the speed limit on 400 class highways is set at 100 kph (also by law). Is that not a travesty of common sense? And a degradation of public safety, if not gross mismanagement?\\n\\nAs ...   \n",
       "4                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       Kinda feel sorry for the \"kid\"....but why the hell are Americans going to N. Korea?  Are they STUPID or just DUMB?   \n",
       "\n",
       "   toxicity  severe_toxicity  obscene  threat  insult  identity_attack  \\\n",
       "0         0                0        0       0       0                0   \n",
       "1         0                0        0       0       0                0   \n",
       "2         0                0        0       0       0                0   \n",
       "3         0                0        0       0       0                0   \n",
       "4         1                0        0       0       1                0   \n",
       "\n",
       "   sexual_explicit  is_valid  \n",
       "0                0     False  \n",
       "1                0     False  \n",
       "2                0     False  \n",
       "3                0     False  \n",
       "4                0     False  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "civil_dsd = load_dataset(\"civil_comments\", split=[\"train\", \"validation\"])\n",
    "\n",
    "# round the floats\n",
    "civil_label_names = [\"toxicity\", \"severe_toxicity\", \"obscene\", \"threat\", \"insult\", \"identity_attack\", \"sexual_explicit\"]\n",
    "\n",
    "\n",
    "def round_targs(example):\n",
    "    for lbl in civil_label_names:\n",
    "        example[lbl] = np.round(example[lbl])\n",
    "    return example\n",
    "\n",
    "\n",
    "# convert floats to ints\n",
    "def fix_dtypes(ds):\n",
    "    new_features = ds.features.copy()\n",
    "    for lbl in civil_label_names:\n",
    "        new_features[lbl] = Value(\"int32\")\n",
    "    return ds.cast(new_features)\n",
    "\n",
    "\n",
    "# build HF `Dataset` objects\n",
    "civil_train_ds = civil_dsd[0].add_column(\"is_valid\", [False] * len(civil_dsd[0])).shuffle().select(range(1000))\n",
    "civil_train_ds = civil_train_ds.map(round_targs)\n",
    "civil_train_ds = fix_dtypes(civil_train_ds)\n",
    "\n",
    "civil_valid_ds = civil_dsd[1].add_column(\"is_valid\", [True] * len(civil_dsd[1])).shuffle().select(range(200))\n",
    "civil_valid_ds = civil_valid_ds.map(round_targs)\n",
    "civil_valid_ds = fix_dtypes(civil_valid_ds)\n",
    "\n",
    "civil_ds = concatenate_datasets([civil_train_ds, civil_valid_ds])\n",
    "\n",
    "# build a `DataFrame` representation as well\n",
    "civil_df = pd.DataFrame(civil_ds)\n",
    "\n",
    "print(len(civil_train_ds), len(civil_valid_ds))\n",
    "print(len(civil_df[civil_df[\"is_valid\"] == False]), len(civil_df[civil_df[\"is_valid\"] == True]))\n",
    "civil_df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Base API"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task specific functions\n",
    "\n",
    "The below functions provide a basic way to fetch your Hugging Face objects and pretokenize your inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "def get_task_hf_objects(pretrained_model_name: str, label_names: list[str] = [\"neg\", \"pos\"], verbose: bool = False):\n",
    "    \"\"\"A helper function for getting the Hugging Face objects that works out of the box for most sequence classification tasks\"\"\"\n",
    "    model_cls = AutoModelForSequenceClassification\n",
    "    n_label_names = len(label_names)\n",
    "\n",
    "    hf_arch, hf_config, hf_tokenizer, hf_model = get_hf_objects(\n",
    "        pretrained_model_name, model_cls=model_cls, config_kwargs={\"num_labels\": n_label_names}\n",
    "    )\n",
    "\n",
    "    if verbose:\n",
    "        hf_arch, type(hf_config), type(hf_tokenizer), type(hf_model)\n",
    "\n",
    "        print(\"=== config ===\")\n",
    "        print(f\"# of labels:\\t{hf_config.num_labels}\")\n",
    "        print(\"\")\n",
    "        print(\"=== tokenizer ===\")\n",
    "        print(f\"Vocab size:\\t\\t{hf_tokenizer.vocab_size}\")\n",
    "        print(f\"Max # of tokens:\\t{hf_tokenizer.model_max_length}\")\n",
    "        print(f\"Attributes expected by model in forward pass:\\t{hf_tokenizer.model_input_names}\")\n",
    "\n",
    "    return hf_arch, hf_config, hf_tokenizer, hf_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/ohmeow/blurr/blob/dev-3.0.0 #master/blurr/data/token_classification.py#L130){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### get_task_hf_objects\n",
       "\n",
       ">      get_task_hf_objects (pretrained_model_name:str,\n",
       ">                           label_names:list[str]=['neg', 'pos'],\n",
       ">                           verbose:bool=False)\n",
       "\n",
       "A helper function for getting the Hugging Face objects that works out of the box for most sequence classification tasks"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/ohmeow/blurr/blob/dev-3.0.0 #master/blurr/data/token_classification.py#L130){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### get_task_hf_objects\n",
       "\n",
       ">      get_task_hf_objects (pretrained_model_name:str,\n",
       ">                           label_names:list[str]=['neg', 'pos'],\n",
       ">                           verbose:bool=False)\n",
       "\n",
       "A helper function for getting the Hugging Face objects that works out of the box for most sequence classification tasks"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nbdev.show_doc(get_task_hf_objects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_arch, hf_config, hf_tokenizer, hf_model = get_task_hf_objects(\n",
    "    \"microsoft/deberta-v3-small\", [\"great\", \"good\", \"bad\", \"horrific\"], verbose=False\n",
    ")\n",
    "\n",
    "test_eq(hf_arch, \"deberta_v2\")\n",
    "test_eq(hf_config.num_labels, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "def multiclass_tokenize_func(\n",
    "    examples,\n",
    "    hf_tokenizer: PreTrainedTokenizerBase,\n",
    "    text_attr: str = \"text\",\n",
    "    text_pair_attr: str = None,\n",
    "    max_length: int = None,\n",
    "    padding: bool | str = True,\n",
    "    truncation: bool | str = True,\n",
    "    tok_kwargs: dict = {},\n",
    "):\n",
    "    \"\"\"A tokenization function that works out of the box for most multiclassification tasks\"\"\"\n",
    "    txts = [examples[text_attr]] if text_pair_attr is None else [examples[text_attr], examples[text_pair_attr]]\n",
    "    return hf_tokenizer(*txts, max_length=max_length, padding=padding, truncation=truncation, **tok_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/ohmeow/blurr/blob/dev-3.0.0 #master/blurr/data/core.py#L62){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### multiclass_tokenize_func\n",
       "\n",
       ">      multiclass_tokenize_func (examples, hf_tokenizer:transformers.tokenizatio\n",
       ">                                n_utils_base.PreTrainedTokenizerBase,\n",
       ">                                text_attr:str='text', text_pair_attr:str=None,\n",
       ">                                max_length:int=None, padding:bool|str=True,\n",
       ">                                truncation:bool|str=True, tok_kwargs:dict={})\n",
       "\n",
       "A tokenization function that works out of the box for most multiclassification tasks"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/ohmeow/blurr/blob/dev-3.0.0 #master/blurr/data/core.py#L62){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### multiclass_tokenize_func\n",
       "\n",
       ">      multiclass_tokenize_func (examples, hf_tokenizer:transformers.tokenizatio\n",
       ">                                n_utils_base.PreTrainedTokenizerBase,\n",
       ">                                text_attr:str='text', text_pair_attr:str=None,\n",
       ">                                max_length:int=None, padding:bool|str=True,\n",
       ">                                truncation:bool|str=True, tok_kwargs:dict={})\n",
       "\n",
       "A tokenization function that works out of the box for most multiclassification tasks"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nbdev.show_doc(multiclass_tokenize_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b2605683f8b4f94b79a6dea543e8c4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "my_dict = {\"id\": [0, 1, 2], \"text\": [\"This is great!\", \"This is so horrible\", \"It was, uh, kinda meh.\"], \"label\": [1, 0, 0]}\n",
    "test_ds = Dataset.from_dict(my_dict)\n",
    "\n",
    "tokenize_func = partial(multiclass_tokenize_func, hf_tokenizer=hf_tokenizer)\n",
    "proc_test_ds = test_ds.map(tokenize_func, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proc_example = proc_test_ds[0]\n",
    "\n",
    "test_eq(isinstance(proc_example, dict), True)\n",
    "test_eq(\"input_ids\" in proc_example.keys(), True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "def multilabel_tokenize_func(\n",
    "    examples,\n",
    "    hf_tokenizer: PreTrainedTokenizerBase,\n",
    "    label_attrs: list[str],\n",
    "    text_attr: str = \"text\",\n",
    "    text_pair_attr: str = None,\n",
    "    max_length: int = None,\n",
    "    padding: bool | str = True,\n",
    "    truncation: bool | str = True,\n",
    "    tok_kwargs: dict = {},\n",
    "):\n",
    "    \"\"\"A tokenization function that works out of the box for most multilabel classification tasks\"\"\"\n",
    "    txts = [examples[text_attr]] if text_pair_attr is None else [examples[text_attr], examples[text_pair_attr]]\n",
    "    inputs = hf_tokenizer(*txts, max_length=max_length, padding=padding, truncation=truncation, **tok_kwargs)\n",
    "\n",
    "    label_names = torch.stack([tensor(examples[lbl]) for lbl in label_attrs], dim=-1)\n",
    "    inputs[\"label\"] = label_names\n",
    "\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/ohmeow/blurr/blob/dev-3.0.0 #master/blurr/data/core.py#L77){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### multilabel_tokenize_func\n",
       "\n",
       ">      multilabel_tokenize_func (examples, hf_tokenizer:transformers.tokenizatio\n",
       ">                                n_utils_base.PreTrainedTokenizerBase,\n",
       ">                                label_attrs:list[str], text_attr:str='text',\n",
       ">                                text_pair_attr:str=None, max_length:int=None,\n",
       ">                                padding:bool|str=True,\n",
       ">                                truncation:bool|str=True, tok_kwargs:dict={})\n",
       "\n",
       "A tokenization function that works out of the box for most multilabel classification tasks"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/ohmeow/blurr/blob/dev-3.0.0 #master/blurr/data/core.py#L77){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### multilabel_tokenize_func\n",
       "\n",
       ">      multilabel_tokenize_func (examples, hf_tokenizer:transformers.tokenizatio\n",
       ">                                n_utils_base.PreTrainedTokenizerBase,\n",
       ">                                label_attrs:list[str], text_attr:str='text',\n",
       ">                                text_pair_attr:str=None, max_length:int=None,\n",
       ">                                padding:bool|str=True,\n",
       ">                                truncation:bool|str=True, tok_kwargs:dict={})\n",
       "\n",
       "A tokenization function that works out of the box for most multilabel classification tasks"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nbdev.show_doc(multilabel_tokenize_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a57e512b7dd47e0903fcc849b8921f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "my_dict = {\n",
    "    \"id\": [0, 1, 2],\n",
    "    \"text\": [\"This is great!\", \"This is so horrible\", \"It was, uh, kinda meh.\"],\n",
    "    \"label1\": [1, 0, 0],\n",
    "    \"label2\": [0, 1, 1],\n",
    "}\n",
    "\n",
    "test_ds = Dataset.from_dict(my_dict)\n",
    "\n",
    "tokenize_func = partial(multilabel_tokenize_func, hf_tokenizer=hf_tokenizer, label_attrs=[\"label1\", \"label2\"])\n",
    "proc_test_ds = test_ds.map(tokenize_func, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proc_example = proc_test_ds[0]\n",
    "\n",
    "test_eq(isinstance(proc_example, dict), True)\n",
    "test_eq(\"input_ids\" in proc_example.keys(), True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `TextCollatorWithPadding`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "@dataclass\n",
    "class TextCollatorWithPadding:\n",
    "    def __init__(\n",
    "        self,\n",
    "        # A Hugging Face tokenizer\n",
    "        hf_tokenizer: PreTrainedTokenizerBase,\n",
    "        # The abbreviation/name of your Hugging Face transformer architecture (e.b., bert, bart, etc..)\n",
    "        hf_arch: str = None,\n",
    "        # A specific configuration instance you want to use\n",
    "        hf_config: PretrainedConfig = None,\n",
    "        # A Hugging Face model\n",
    "        hf_model: PreTrainedModel = None,\n",
    "        # The number of inputs expected by your model\n",
    "        n_inp: int = 1,\n",
    "        # Defaults to use Hugging Face's DataCollatorWithPadding(tokenizer=hf_tokenizer)\n",
    "        data_collator_cls: type = DataCollatorWithPadding,\n",
    "        # kwyargs specific for the instantiation of the `data_collator`\n",
    "        data_collator_kwargs: dict = {},\n",
    "    ):\n",
    "        \"\"\"A data collation function that can be used across blurr's base, low-level, and mid-level APIs\"\"\"\n",
    "        store_attr()\n",
    "        self.hf_tokenizer = data_collator_kwargs.pop(\"tokenizer\", self.hf_tokenizer)\n",
    "        self.data_collator = data_collator_cls(tokenizer=self.hf_tokenizer, **data_collator_kwargs)\n",
    "\n",
    "    def __call__(self, features):\n",
    "        features = L(features)\n",
    "        inputs, labels, targs = [], [], []\n",
    "\n",
    "        # features contain dictionaries\n",
    "        if isinstance(features[0], dict):\n",
    "            feature_keys = list(features[0].keys())\n",
    "            inputs = [self._build_inputs_d(features, feature_keys)]\n",
    "\n",
    "            input_labels = self._build_input_labels(inputs[0], features, feature_keys)\n",
    "            if input_labels is not None:\n",
    "                labels, targs = [input_labels], [input_labels.clone()]\n",
    "        # features contains tuples, each of which can contain multiple inputs and/or targets\n",
    "        elif isinstance(features[0], tuple):\n",
    "            for f_idx in range(self.n_inp):\n",
    "                feature_keys = list(features[0][f_idx].keys())\n",
    "                inputs.append(self._build_inputs_d(features.itemgot(f_idx), feature_keys))\n",
    "\n",
    "                input_labels = self._build_input_labels(inputs[0], features.itemgot(f_idx), feature_keys)\n",
    "                labels.append(input_labels if input_labels is not None else [])\n",
    "\n",
    "            targs = [self._proc_targets(inputs[0], list(features.itemgot(f_idx))) for f_idx in range(self.n_inp, len(features[0]))]\n",
    "\n",
    "        return self._build_batch(inputs, labels, targs)\n",
    "\n",
    "    # ----- utility methods -----\n",
    "\n",
    "    # to build the inputs dictionary\n",
    "    def _build_inputs_d(self, features, feature_keys):\n",
    "        return {fwd_arg: list(features.attrgot(fwd_arg)) for fwd_arg in self.hf_tokenizer.model_input_names if fwd_arg in feature_keys}\n",
    "\n",
    "    # to build the input \"labels\"\n",
    "    def _build_input_labels(self, inputs_d, features, feature_keys):\n",
    "        if \"label\" in feature_keys:\n",
    "            labels = list(features.attrgot(\"label\"))\n",
    "            return self._proc_targets(inputs_d, labels)\n",
    "        return None\n",
    "\n",
    "    # used to give the labels/targets the right shape\n",
    "    def _proc_targets(self, inputs_d, targs):\n",
    "        if is_listy(targs[0]):\n",
    "            targs = torch.stack([tensor(lbls) for lbls in targs])\n",
    "        elif isinstance(targs[0], torch.Tensor) and len(targs[0].size()) > 0:\n",
    "            targs = torch.stack(targs)\n",
    "        else:\n",
    "            targs = torch.tensor(targs)\n",
    "\n",
    "        return targs\n",
    "\n",
    "    # will properly assemble are batch given a list of inputs, labels, and targets\n",
    "    def _build_batch(self, inputs, labels, targs):\n",
    "        batch = []\n",
    "\n",
    "        for input, input_labels in zip(inputs, labels):\n",
    "            input_d = dict(self.data_collator(input))\n",
    "            if len(input_labels) > 0:\n",
    "                input_d[\"labels\"] = input_labels\n",
    "\n",
    "            batch.append(input_d)\n",
    "\n",
    "        for targ in targs:\n",
    "            batch.append(targ)\n",
    "\n",
    "        return tuplify(batch)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Base API: Examples\n",
    "\n",
    "This section demonstrates how you can use standard `Dataset` objects (PyTorch and Hugging Face) to build PyTorch `DataLoader`s\n",
    "\n",
    "**Note** that most fast.ai specific features such as `DataLoaders.one_batch` and `DataLoader.show_batch` are not available when using PyTorch."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multiclass"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 1: HF objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== config ===\n",
      "# of labels:\t2\n",
      "\n",
      "=== tokenizer ===\n",
      "Vocab size:\t\t128000\n",
      "Max # of tokens:\t1000000000000000019884624838656\n",
      "Attributes expected by model in forward pass:\t['input_ids', 'token_type_ids', 'attention_mask']\n"
     ]
    }
   ],
   "source": [
    "hf_arch, hf_config, hf_tokenizer, hf_model = get_task_hf_objects(\"microsoft/deberta-v3-small\", label_names, verbose=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 2: `Dataset`s (PyTorch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aec9f83211dd40db96996ee2898d390a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b988e6f10014270bb60b5fc3f18edd4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# tokenize the dataset\n",
    "tokenize_func = partial(multiclass_tokenize_func, hf_tokenizer=hf_tokenizer)\n",
    "proc_train_ds = train_ds.map(tokenize_func, batched=True)\n",
    "proc_valid_ds = valid_ds.map(tokenize_func, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define our PyTorch Dataset class\n",
    "class HFTextClassificationDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, hf_dataset, hf_tokenizer):\n",
    "        self.hf_dataset = hf_dataset\n",
    "        self.hf_tokenizer = hf_tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.hf_dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.hf_dataset[idx]\n",
    "        return item\n",
    "\n",
    "\n",
    "# build our PyTorch training and validation Datasets\n",
    "pt_proc_train_ds = HFTextClassificationDataset(proc_train_ds, hf_tokenizer=hf_tokenizer)\n",
    "pt_proc_valid_ds = HFTextClassificationDataset(proc_valid_ds, hf_tokenizer=hf_tokenizer)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 3: `DataLoader`s (PyTorch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build your fastai `DataLoaders` from Pytorch `DataLoader` objects\n",
    "batch_size = 4\n",
    "data_collator = TextCollatorWithPadding(hf_tokenizer)\n",
    "train_dl = torch.utils.data.DataLoader(pt_proc_train_ds, batch_size=batch_size, shuffle=True, collate_fn=data_collator)\n",
    "valid_dl = torch.utils.data.DataLoader(pt_proc_valid_ds, batch_size=batch_size * 2, shuffle=False, collate_fn=data_collator)\n",
    "\n",
    "dls = DataLoaders(train_dl, valid_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of batches in train|validation dataloaders: 250 25\n",
      "# of items in each batch: 2\n",
      "\n",
      "Decoded input_ids: [CLS] With rapid intercutting of scenes of insane people ... \n",
      "Targets: tensor([1, 0, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "print(\"# of batches in train|validation dataloaders:\", len(train_dl), len(valid_dl))\n",
    "\n",
    "b = next(iter(train_dl))\n",
    "print(\"# of items in each batch:\", len(b))\n",
    "print(\"\")\n",
    "print(f\"Decoded input_ids: {hf_tokenizer.decode(b[0]['input_ids'][0][:10])} ... \")\n",
    "print(\"Targets:\", b[1])\n",
    "\n",
    "# b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOPE: Won't work with PyTorch DataLoaders\n",
    "# AttributeError: 'DataLoader' object has no attribute 'show_batch'\n",
    "# dls.show_batch(dataloaders=dls, max_n=2, trunc_at=800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |echo:false\n",
    "try:\n",
    "    del dls, hf_model\n",
    "except:\n",
    "    pass\n",
    "finally:\n",
    "    clean_memory()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multilabel"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 1: HF objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== config ===\n",
      "# of labels:\t7\n",
      "\n",
      "=== tokenizer ===\n",
      "Vocab size:\t\t128000\n",
      "Max # of tokens:\t1000000000000000019884624838656\n",
      "Attributes expected by model in forward pass:\t['input_ids', 'token_type_ids', 'attention_mask']\n"
     ]
    }
   ],
   "source": [
    "hf_arch, hf_config, hf_tokenizer, hf_model = get_task_hf_objects(\"microsoft/deberta-v3-small\", civil_label_names, verbose=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 2: `Dataset`s (PyTorch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4cba356ac28745d0bf0d43afb2bd229b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12dd3275cc284243ac2cfe13fd4e3ac1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# tokenize the dataset\n",
    "tokenize_func = partial(multilabel_tokenize_func, hf_tokenizer=hf_tokenizer, label_attrs=civil_label_names)\n",
    "proc_civil_train_ds = civil_train_ds.map(tokenize_func, batched=True)\n",
    "proc_civil_valid_ds = civil_valid_ds.map(tokenize_func, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define our PyTorch Dataset class\n",
    "class HFTextMultilabelClassificationDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, hf_dataset, hf_tokenizer, label_names):\n",
    "        self.hf_dataset = hf_dataset\n",
    "        self.hf_tokenizer = hf_tokenizer\n",
    "        self.label_names = label_names\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.hf_dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.hf_dataset[idx]\n",
    "        # item[\"label\"] = [item[lbl] for lbl in self.label_names]\n",
    "        return item\n",
    "\n",
    "\n",
    "# build our PyTorch training and validation Datasets\n",
    "pt_proc_civil_train_ds = HFTextMultilabelClassificationDataset(\n",
    "    proc_civil_train_ds, hf_tokenizer=hf_tokenizer, label_names=civil_label_names\n",
    ")\n",
    "pt_proc_civil_valid_ds = HFTextMultilabelClassificationDataset(\n",
    "    proc_civil_valid_ds, hf_tokenizer=hf_tokenizer, label_names=civil_label_names\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 3: `DataLoader`s (PyTorch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build your fastai `DataLoaders` from Pytorch `DataLoader` objects\n",
    "batch_size = 4\n",
    "data_collator = TextCollatorWithPadding(hf_tokenizer)\n",
    "train_dl = torch.utils.data.DataLoader(pt_proc_civil_train_ds, batch_size=batch_size, shuffle=True, collate_fn=data_collator)\n",
    "valid_dl = torch.utils.data.DataLoader(pt_proc_civil_valid_ds, batch_size=batch_size * 2, shuffle=False, collate_fn=data_collator)\n",
    "\n",
    "dls = DataLoaders(train_dl, valid_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of batches in train|validation dataloaders: 250 25\n",
      "# of items in each batch: 2\n",
      "\n",
      "Decoded input_ids: [CLS] The right being discrimination based on race, etc ... \n",
      "Targets: tensor([[0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0]])\n"
     ]
    }
   ],
   "source": [
    "print(\"# of batches in train|validation dataloaders:\", len(train_dl), len(valid_dl))\n",
    "\n",
    "b = next(iter(train_dl))\n",
    "print(\"# of items in each batch:\", len(b))\n",
    "print(\"\")\n",
    "print(f\"Decoded input_ids: {hf_tokenizer.decode(b[0]['input_ids'][0][:10])} ... \")\n",
    "print(\"Targets:\", b[1])\n",
    "\n",
    "# b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |echo:false\n",
    "try:\n",
    "    del dls, hf_model\n",
    "except:\n",
    "    pass\n",
    "finally:\n",
    "    clean_memory()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hugging Face"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multiclass"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 1: HF objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== config ===\n",
      "# of labels:\t2\n",
      "\n",
      "=== tokenizer ===\n",
      "Vocab size:\t\t128000\n",
      "Max # of tokens:\t1000000000000000019884624838656\n",
      "Attributes expected by model in forward pass:\t['input_ids', 'token_type_ids', 'attention_mask']\n"
     ]
    }
   ],
   "source": [
    "hf_arch, hf_config, hf_tokenizer, hf_model = get_task_hf_objects(\"microsoft/deberta-v3-small\", label_names, verbose=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 2: `Datasets` (huggingface)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/wgilliam/.cache/huggingface/datasets/imdb/plain_text/1.0.0/d613c88cf8fa3bab83b4ded3713f1f74830d1100e171db75bbddb80b3345c9c0/cache-ed889df3a7475455.arrow\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e52325d7a44a41baaf56e3d797618483",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# tokenize the dataset\n",
    "tokenize_func = partial(multiclass_tokenize_func, hf_tokenizer=hf_tokenizer)\n",
    "proc_train_ds = train_ds.map(tokenize_func, batched=True)\n",
    "proc_valid_ds = valid_ds.map(tokenize_func, batched=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 3: `DataLoader`s (PyTorch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build your fastai `DataLoaders` from Pytorch `DataLoader` objects\n",
    "batch_size = 4\n",
    "data_collator = TextCollatorWithPadding(hf_tokenizer)\n",
    "train_dl = torch.utils.data.DataLoader(proc_train_ds, batch_size=batch_size, shuffle=True, collate_fn=data_collator)\n",
    "valid_dl = torch.utils.data.DataLoader(proc_valid_ds, batch_size=batch_size * 2, shuffle=False, collate_fn=data_collator)\n",
    "\n",
    "dls = DataLoaders(train_dl, valid_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of batches in train|validation dataloaders: 250 25\n",
      "# of items in each batch: 2\n",
      "\n",
      "Decoded input_ids: [CLS] Imagine that in adapting a James Bond novel into ... \n",
      "Targets: tensor([0, 0, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "print(\"# of batches in train|validation dataloaders:\", len(train_dl), len(valid_dl))\n",
    "\n",
    "b = next(iter(train_dl))\n",
    "print(\"# of items in each batch:\", len(b))\n",
    "print(\"\")\n",
    "print(f\"Decoded input_ids: {hf_tokenizer.decode(b[0]['input_ids'][0][:10])} ... \")\n",
    "print(\"Targets:\", b[1])\n",
    "\n",
    "# b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |echo:false\n",
    "try:\n",
    "    del dls, hf_model\n",
    "except:\n",
    "    pass\n",
    "finally:\n",
    "    clean_memory()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multilabel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 1: HF objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== config ===\n",
      "# of labels:\t7\n",
      "\n",
      "=== tokenizer ===\n",
      "Vocab size:\t\t128000\n",
      "Max # of tokens:\t1000000000000000019884624838656\n",
      "Attributes expected by model in forward pass:\t['input_ids', 'token_type_ids', 'attention_mask']\n"
     ]
    }
   ],
   "source": [
    "hf_arch, hf_config, hf_tokenizer, hf_model = get_task_hf_objects(\"microsoft/deberta-v3-small\", civil_label_names, verbose=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 2: `Datasets` (huggingface)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/wgilliam/.cache/huggingface/datasets/civil_comments/default/0.9.0/e7a3aacd2ab7d135fa958e7209d10b1fa03807d44c486e3c34897aa08ea8ffab/cache-0eb9a7ca462e8b22.arrow\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5bbc4247f954cdba4b9dc24afce4cc3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# tokenize the dataset\n",
    "tokenize_func = partial(multilabel_tokenize_func, hf_tokenizer=hf_tokenizer, label_attrs=civil_label_names)\n",
    "proc_civil_train_ds = civil_train_ds.map(tokenize_func, batched=True)\n",
    "proc_civil_valid_ds = civil_valid_ds.map(tokenize_func, batched=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 3: `DataLoader`s (PyTorch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build your fastai `DataLoaders` from Pytorch `DataLoader` objects\n",
    "batch_size = 4\n",
    "data_collator = TextCollatorWithPadding(hf_tokenizer)\n",
    "train_dl = torch.utils.data.DataLoader(proc_civil_train_ds, batch_size=batch_size, shuffle=True, collate_fn=data_collator)\n",
    "valid_dl = torch.utils.data.DataLoader(proc_civil_valid_ds, batch_size=batch_size * 2, shuffle=False, collate_fn=data_collator)\n",
    "\n",
    "dls = DataLoaders(train_dl, valid_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of batches in train|validation dataloaders: 250 25\n",
      "# of items in each batch: 2\n",
      "\n",
      "Decoded input_ids: [CLS] Those who do not condemn the ideology of white ... \n",
      "Targets: tensor([[0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 0, 0, 0, 1, 0, 0]])\n"
     ]
    }
   ],
   "source": [
    "print(\"# of batches in train|validation dataloaders:\", len(train_dl), len(valid_dl))\n",
    "\n",
    "b = next(iter(train_dl))\n",
    "print(\"# of items in each batch:\", len(b))\n",
    "print(\"\")\n",
    "print(f\"Decoded input_ids: {hf_tokenizer.decode(b[0]['input_ids'][0][:10])} ... \")\n",
    "print(\"Targets:\", b[1])\n",
    "\n",
    "# b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |echo:false\n",
    "try:\n",
    "    del dls, hf_model\n",
    "except:\n",
    "    pass\n",
    "finally:\n",
    "    clean_memory()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Low-Level API\n",
    "\n",
    "This section demonstrates how you can migrate from using PyTorch/Hugging Face to fast.ai `Datasets` and `DataLoaders` to recapture much of the fast.ai specific features unavailable when using basic PyTorch. This includes:\n",
    "\n",
    "- `DataLoaders.one_batch()`\n",
    "- `DataLoaders.show_batch()`\n",
    "- `Leaner.export()`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `TextInput` -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "class TextInput(TensorBase):\n",
    "    \"\"\"The base represenation of your inputs; used by the various fastai `show` methods\"\"\"\n",
    "\n",
    "    pass"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A `TextInput` object is returned from the decodes method of `BatchDecodeTransform` as a means to customize `@typedispatch`ed functions like `DataLoaders.show_batch` and `Learner.show_results`. The value will the your \"input_ids\"."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `BatchDecodeTransform` -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "class BatchDecodeTransform(Transform):\n",
    "    \"\"\"A class used to cast your inputs as `input_return_type` for fastai `show` methods\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        # A Hugging Face tokenizer (not required if passing in an instance of `BatchTokenizeTransform` to `before_batch_tfm`)\n",
    "        hf_tokenizer: PreTrainedTokenizerBase,\n",
    "        # The abbreviation/name of your Hugging Face transformer architecture (not required if passing in an instance of `BatchTokenizeTransform` to `before_batch_tfm`)\n",
    "        hf_arch: str = None,\n",
    "        # A Hugging Face configuration object (not required if passing in an instance of `BatchTokenizeTransform` to `before_batch_tfm`)\n",
    "        hf_config: PretrainedConfig = None,\n",
    "        # A Hugging Face model (not required if passing in an instance of `BatchTokenizeTransform` to `before_batch_tfm`)\n",
    "        hf_model: PreTrainedModel = None,\n",
    "        # Used by typedispatched show methods\n",
    "        input_return_type: type = TextInput,\n",
    "        # The token ID that should be ignored when calculating the loss\n",
    "        ignore_token_id: int = CrossEntropyLossFlat().ignore_index,\n",
    "        # Any other keyword arguments\n",
    "        **kwargs,\n",
    "    ):\n",
    "        store_attr()\n",
    "        self.kwargs = kwargs\n",
    "\n",
    "    def decodes(self, items: dict):\n",
    "        \"\"\"Returns the proper object and data for show related fastai methods\"\"\"\n",
    "        return self.input_return_type(items[\"input_ids\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As of fastai 2.1.5, before batch transforms no longer have a `decodes` method ... and so, I've introduced a standard batch transform here, `BatchDecodeTransform`, (one that occurs \"after\" the batch has been created) that will do the decoding for us."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility classes and methods \n",
    "\n",
    "These methods are use internally for getting blurr transforms associated to your `DataLoaders`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "def get_blurr_tfm(\n",
    "    # A list of transforms (e.g., dls.after_batch, dls.before_batch, etc...)\n",
    "    tfms_list: Pipeline,\n",
    "    # The transform to find\n",
    "    tfm_class: Transform = BatchDecodeTransform,\n",
    "):\n",
    "    \"\"\"\n",
    "    Given a fastai DataLoaders batch transforms, this method can be used to get at a transform\n",
    "    instance used in your Blurr DataBlock\n",
    "    \"\"\"\n",
    "    return next(filter(lambda el: issubclass(type(el), tfm_class), tfms_list), None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/ohmeow/blurr/blob/dev-3.0.0 #master/blurr/data/core.py#L222){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### get_blurr_tfm\n",
       "\n",
       ">      get_blurr_tfm (tfms_list:fastcore.transform.Pipeline,\n",
       ">                     tfm_class:fastcore.transform.Transform=<class\n",
       ">                     '__main__.BatchDecodeTransform'>)\n",
       "\n",
       "Given a fastai DataLoaders batch transforms, this method can be used to get at a transform\n",
       "instance used in your Blurr DataBlock\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| tfms_list | Pipeline |  | A list of transforms (e.g., dls.after_batch, dls.before_batch, etc...) |\n",
       "| tfm_class | Transform | BatchDecodeTransform | The transform to find |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/ohmeow/blurr/blob/dev-3.0.0 #master/blurr/data/core.py#L222){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### get_blurr_tfm\n",
       "\n",
       ">      get_blurr_tfm (tfms_list:fastcore.transform.Pipeline,\n",
       ">                     tfm_class:fastcore.transform.Transform=<class\n",
       ">                     '__main__.BatchDecodeTransform'>)\n",
       "\n",
       "Given a fastai DataLoaders batch transforms, this method can be used to get at a transform\n",
       "instance used in your Blurr DataBlock\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| tfms_list | Pipeline |  | A list of transforms (e.g., dls.after_batch, dls.before_batch, etc...) |\n",
       "| tfm_class | Transform | BatchDecodeTransform | The transform to find |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nbdev.show_doc(get_blurr_tfm, title_level=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "def first_blurr_tfm(\n",
    "    # Your fast.ai `DataLoaders\n",
    "    dls: DataLoaders,\n",
    "    # The Blurr transforms to look for in order\n",
    "    tfms: list[Transform] = [BatchDecodeTransform],\n",
    "):\n",
    "    \"\"\"\n",
    "    This convenience method will find the first Blurr transform required for methods such as\n",
    "    `show_batch` and `show_results`. The returned transform should have everything you need to properly\n",
    "    decode and 'show' your Hugging Face inputs/targets\n",
    "    \"\"\"\n",
    "    for tfm in tfms:\n",
    "        found_tfm = get_blurr_tfm(dls.before_batch, tfm_class=tfm)\n",
    "        if found_tfm:\n",
    "            return found_tfm\n",
    "\n",
    "        found_tfm = get_blurr_tfm(dls.after_batch, tfm_class=tfm)\n",
    "        if found_tfm:\n",
    "            return found_tfm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/ohmeow/blurr/blob/dev-3.0.0 #master/blurr/data/core.py#L235){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### first_blurr_tfm\n",
       "\n",
       ">      first_blurr_tfm (dls:fastai.data.core.DataLoaders,\n",
       ">                       tfms:list[fastcore.transform.Transform]=[<class\n",
       ">                       '__main__.BatchDecodeTransform'>])\n",
       "\n",
       "This convenience method will find the first Blurr transform required for methods such as\n",
       "`show_batch` and `show_results`. The returned transform should have everything you need to properly\n",
       "decode and 'show' your Hugging Face inputs/targets\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| dls | DataLoaders |  | Your fast.ai `DataLoaders |\n",
       "| tfms | list[Transform] | [<class '__main__.BatchDecodeTransform'>] | The Blurr transforms to look for in order |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/ohmeow/blurr/blob/dev-3.0.0 #master/blurr/data/core.py#L235){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### first_blurr_tfm\n",
       "\n",
       ">      first_blurr_tfm (dls:fastai.data.core.DataLoaders,\n",
       ">                       tfms:list[fastcore.transform.Transform]=[<class\n",
       ">                       '__main__.BatchDecodeTransform'>])\n",
       "\n",
       "This convenience method will find the first Blurr transform required for methods such as\n",
       "`show_batch` and `show_results`. The returned transform should have everything you need to properly\n",
       "decode and 'show' your Hugging Face inputs/targets\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| dls | DataLoaders |  | Your fast.ai `DataLoaders |\n",
       "| tfms | list[Transform] | [<class '__main__.BatchDecodeTransform'>] | The Blurr transforms to look for in order |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nbdev.show_doc(first_blurr_tfm, title_level=3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `show_batch` -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "@typedispatch\n",
    "def show_batch(\n",
    "    # This typedispatched `show_batch` will be called for `TextInput` typed inputs\n",
    "    x: TextInput,\n",
    "    # Your targets\n",
    "    y,\n",
    "    # Your raw inputs/targets\n",
    "    samples,\n",
    "    # Your `DataLoaders`. This is required so as to get at the Hugging Face objects for\n",
    "    # decoding them into something understandable\n",
    "    dataloaders,\n",
    "    # Your `show_batch` context\n",
    "    ctxs=None,\n",
    "    # The maximum number of items to show\n",
    "    max_n=6,\n",
    "    # Any truncation your want applied to your decoded inputs\n",
    "    trunc_at=None,\n",
    "    # Any other keyword arguments you want applied to `show_batch`\n",
    "    **kwargs,\n",
    "):\n",
    "    # grab our tokenizer\n",
    "    tfm = first_blurr_tfm(dataloaders)\n",
    "    hf_tokenizer = tfm.hf_tokenizer\n",
    "\n",
    "    # if we've included our label_names list, we'll use it to look up the value of our target(s)\n",
    "    trg_label_names = tfm.kwargs[\"label_names\"] if (\"label_names\" in tfm.kwargs) else None\n",
    "    if trg_label_names is None and dataloaders.vocab is not None:\n",
    "        trg_label_names = dataloaders.vocab\n",
    "\n",
    "    res = L()\n",
    "    n_inp = dataloaders.n_inp\n",
    "\n",
    "    n_samples = min(max_n, dataloaders.bs)\n",
    "    for idx in range(n_samples):\n",
    "        input_ids = x[idx]\n",
    "        rets = [hf_tokenizer.decode(input_ids, skip_special_tokens=True)[:trunc_at]]\n",
    "\n",
    "        sample = samples[idx] if samples is not None else None\n",
    "        for item_idx, item in enumerate(sample[n_inp:]):\n",
    "            label = y[item_idx] if y is not None else item\n",
    "\n",
    "            if torch.is_tensor(label):\n",
    "                label = list(label.numpy()) if len(label.size()) > 0 else label.item()\n",
    "\n",
    "            if is_listy(label):\n",
    "                trg = [trg_label_names[int(idx)] for idx, val in enumerate(label) if (val == 1)] if trg_label_names else label\n",
    "            else:\n",
    "                trg = trg_label_names[int(item)] if trg_label_names else item\n",
    "\n",
    "            rets.append(trg)\n",
    "        res.append(tuplify(rets))\n",
    "\n",
    "    cols = [\"text\"] + [\"target\" if (i == 0) else f\"target_{i}\" for i in range(len(res[0]) - n_inp)]\n",
    "    display_df(pd.DataFrame(res, columns=cols)[:max_n])\n",
    "    return ctxs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `sorted_dl_func` -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "def sorted_dl_func(\n",
    "    example,\n",
    "    # A Hugging Face tokenizer\n",
    "    hf_tokenizer: PreTrainedTokenizerBase,\n",
    "    # The `is_split_into_words` argument applied to your `hf_tokenizer` during tokenization. \\\n",
    "    # Set this to 'True' if your inputs are pre-tokenized (not numericalized)\n",
    "    is_split_into_words: bool = False,\n",
    "    # Any other keyword arguments you want to include during tokenization\n",
    "    tok_kwargs: dict = {},\n",
    "):\n",
    "    \"\"\"This method is used by the `SortedDL` to ensure your dataset is sorted *after* tokenization\"\"\"\n",
    "    txt = None\n",
    "    if isinstance(example[0], dict):\n",
    "        if \"input_ids\" in example[0]:\n",
    "            # if inputs are pretokenized\n",
    "            return len(example[0][\"input_ids\"])\n",
    "        else:\n",
    "            txt = example[0][\"text\"]\n",
    "    else:\n",
    "        txt = example[0]\n",
    "\n",
    "    return len(txt) if is_split_into_words else len(hf_tokenizer.tokenize(txt, **tok_kwargs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/ohmeow/blurr/blob/dev-3.0.0 #master/blurr/data/core.py#L313){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### sorted_dl_func\n",
       "\n",
       ">      sorted_dl_func (example, hf_tokenizer:transformers.tokenization_utils_bas\n",
       ">                      e.PreTrainedTokenizerBase,\n",
       ">                      is_split_into_words:bool=False, tok_kwargs:dict={})\n",
       "\n",
       "This method is used by the `SortedDL` to ensure your dataset is sorted *after* tokenization\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| example |  |  |  |\n",
       "| hf_tokenizer | PreTrainedTokenizerBase |  | A Hugging Face tokenizer |\n",
       "| is_split_into_words | bool | False | The `is_split_into_words` argument applied to your `hf_tokenizer` during tokenization. \\<br>Set this to 'True' if your inputs are pre-tokenized (not numericalized) |\n",
       "| tok_kwargs | dict | {} | Any other keyword arguments you want to include during tokenization |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/ohmeow/blurr/blob/dev-3.0.0 #master/blurr/data/core.py#L313){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### sorted_dl_func\n",
       "\n",
       ">      sorted_dl_func (example, hf_tokenizer:transformers.tokenization_utils_bas\n",
       ">                      e.PreTrainedTokenizerBase,\n",
       ">                      is_split_into_words:bool=False, tok_kwargs:dict={})\n",
       "\n",
       "This method is used by the `SortedDL` to ensure your dataset is sorted *after* tokenization\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| example |  |  |  |\n",
       "| hf_tokenizer | PreTrainedTokenizerBase |  | A Hugging Face tokenizer |\n",
       "| is_split_into_words | bool | False | The `is_split_into_words` argument applied to your `hf_tokenizer` during tokenization. \\<br>Set this to 'True' if your inputs are pre-tokenized (not numericalized) |\n",
       "| tok_kwargs | dict | {} | Any other keyword arguments you want to include during tokenization |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nbdev.show_doc(sorted_dl_func, title_level=3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Low-Level API: Examples"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiclass"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1: HF objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== config ===\n",
      "# of labels:\t2\n",
      "\n",
      "=== tokenizer ===\n",
      "Vocab size:\t\t128000\n",
      "Max # of tokens:\t1000000000000000019884624838656\n",
      "Attributes expected by model in forward pass:\t['input_ids', 'token_type_ids', 'attention_mask']\n"
     ]
    }
   ],
   "source": [
    "hf_arch, hf_config, hf_tokenizer, hf_model = get_task_hf_objects(\"microsoft/deberta-v3-small\", label_names, verbose=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: `Datasets` (fastai)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8689b571acef4055a745e39063771e33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# tokenize the dataset\n",
    "tokenize_func = partial(multiclass_tokenize_func, hf_tokenizer=hf_tokenizer)\n",
    "proc_imdb_ds = imdb_ds.map(tokenize_func, batched=True)\n",
    "\n",
    "# turn Arrow into DataFrame (`ColSplitter` only works with `DataFrame`s)\n",
    "train_df = pd.DataFrame(proc_imdb_ds)\n",
    "train_df.head()\n",
    "\n",
    "# define dataset splitter\n",
    "splitter = ColSplitter(\"is_valid\")\n",
    "splits = splitter(imdb_df)\n",
    "\n",
    "\n",
    "# define how we want to build our inputs and targets\n",
    "def _build_inputs(example):\n",
    "    return {fwd_arg_name: example[fwd_arg_name] for fwd_arg_name in hf_tokenizer.model_input_names if fwd_arg_name in list(example.keys())}\n",
    "\n",
    "\n",
    "def _build_targets(example):\n",
    "    return example[\"label\"]\n",
    "\n",
    "\n",
    "# create our fastai `Datasets` object\n",
    "dsets = Datasets(items=train_df, splits=splits, tfms=[[_build_inputs], _build_targets], n_inp=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Items in train|validation datasets:  1000 200\n",
      "Items in each example: 2\n",
      "Example inputs: ['input_ids', 'token_type_ids', 'attention_mask']\n",
      "Example target(s): 1\n"
     ]
    }
   ],
   "source": [
    "print(\"Items in train|validation datasets: \", len(dsets.train), len(dsets.valid))\n",
    "\n",
    "example = dsets.valid[0]\n",
    "# example\n",
    "\n",
    "print(f\"Items in each example: {len(example)}\")\n",
    "print(f\"Example inputs: {list(example[0].keys())}\")\n",
    "print(f\"Example target(s): {example[1]}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3: `DataLoaders` (fastai)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = TextCollatorWithPadding(hf_tokenizer)\n",
    "sort_func = partial(sorted_dl_func, hf_tokenizer=hf_tokenizer)\n",
    "batch_decode_tfm = BatchDecodeTransform(hf_tokenizer, hf_arch, hf_config, hf_model, label_names=label_names)\n",
    "\n",
    "dls = dsets.dataloaders(\n",
    "    batch_size=4,\n",
    "    create_batch=data_collator,\n",
    "    after_batch=batch_decode_tfm,\n",
    "    dl_type=partial(SortedDL, sort_func=sort_func),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of batches in train|validation dataloaders: 250 25\n",
      "# of items in each batch: 2\n",
      "\n",
      "Decoded input_ids: [CLS] Merry Christmas to you, too! :) ... \n",
      "Targets: tensor([[0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0]])\n"
     ]
    }
   ],
   "source": [
    "print(\"# of batches in train|validation dataloaders:\", len(train_dl), len(valid_dl))\n",
    "\n",
    "b = next(iter(train_dl))\n",
    "print(\"# of items in each batch:\", len(b))\n",
    "print(\"\")\n",
    "print(f\"Decoded input_ids: {hf_tokenizer.decode(b[0]['input_ids'][0][:10])} ... \")\n",
    "print(\"Targets:\", b[1])\n",
    "\n",
    "# b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Except for the Brady Variety Hour, this was some of the hokiest television I've seen in a while. The video production qualities weren't too bad, but the overall look and feel were unmistakeably early 80's. And Marie Osmond looks like she did battle with the Avon Lady.. and lost big time. WAY too much eyeliner.&lt;br /&gt;&lt;br /&gt;It was kind of embarrassing to watch veterans Danny Kaye and Eric Severeid take part in this. Even more interesting was watching Alex Haley talk about the African Pavillion in World Showcase that would be opening 'in about a year.'&lt;br /&gt;&lt;br /&gt;As of this writing it is 17 years later and it hasn't opened yet (Unless you count Disney's Animal Kingdom.) All in all though, for all the shortcomings, this still an interesting visual piece of Disney history.</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I work as a hotel concierge in Washington DC and take my word, there was nothing remotely accurate about the character played by Michael J. Fox- # 1 we simply do not walk around with our pockets bursting with theater tickets and $100 bills! #2 If I ever let anybody use a room for some 'afternoon delight' time I'd be fired on the spot! The organization to which I belong (Les Clefs d'Or) has very definite standards of ethics and conduct that we take seriously. #3 Similarly untrue was the concept, at the end of the movie, of Doug simply removing his gold key emblem and passing it on to some other employee- we earn those keys and it is a badge of honor and knowledge to be allowed to wear them. There is a whole application and vetting process to joining our organization.&lt;br /&gt;&lt;br /&gt;This film do</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dls.show_batch(dataloaders=dls, max_n=2, trunc_at=800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |echo:false\n",
    "try:\n",
    "    del dls, hf_model\n",
    "except:\n",
    "    pass\n",
    "finally:\n",
    "    clean_memory()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multilabel"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1: HF objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== config ===\n",
      "# of labels:\t7\n",
      "\n",
      "=== tokenizer ===\n",
      "Vocab size:\t\t128000\n",
      "Max # of tokens:\t1000000000000000019884624838656\n",
      "Attributes expected by model in forward pass:\t['input_ids', 'token_type_ids', 'attention_mask']\n"
     ]
    }
   ],
   "source": [
    "hf_arch, hf_config, hf_tokenizer, hf_model = get_task_hf_objects(\"microsoft/deberta-v3-small\", civil_label_names, verbose=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: `Datasets` (fastai)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c95a134afbe4c758859cfb297fe2502",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# tokenize the dataset\n",
    "tokenize_func = partial(multilabel_tokenize_func, hf_tokenizer=hf_tokenizer, label_attrs=civil_label_names)\n",
    "proc_civil_ds = civil_ds.map(tokenize_func, batched=True)\n",
    "\n",
    "# turn Arrow into DataFrame (`ColSplitter` only works with `DataFrame`s)\n",
    "train_df = pd.DataFrame(proc_civil_ds)\n",
    "train_df.head()\n",
    "\n",
    "# define dataset splitter\n",
    "splitter = ColSplitter(\"is_valid\")\n",
    "splits = splitter(civil_df)\n",
    "\n",
    "\n",
    "# define how we want to build our inputs and targets\n",
    "def _build_inputs(example):\n",
    "    return {fwd_arg_name: example[fwd_arg_name] for fwd_arg_name in hf_tokenizer.model_input_names if fwd_arg_name in list(example.keys())}\n",
    "\n",
    "\n",
    "def _build_targets(example):\n",
    "    return example[\"label\"]\n",
    "\n",
    "\n",
    "# create our fastai `Datasets` object\n",
    "dsets = Datasets(items=train_df, splits=splits, tfms=[[_build_inputs], _build_targets], n_inp=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Items in train|validation datasets:  1000 200\n",
      "Items in each example: 2\n",
      "Example inputs: ['input_ids', 'token_type_ids', 'attention_mask']\n",
      "Example target(s): [0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "print(\"Items in train|validation datasets: \", len(dsets.train), len(dsets.valid))\n",
    "\n",
    "example = dsets.valid[0]\n",
    "# example\n",
    "\n",
    "print(f\"Items in each example: {len(example)}\")\n",
    "print(f\"Example inputs: {list(example[0].keys())}\")\n",
    "print(f\"Example target(s): {example[1]}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3: `DataLoaders` (fastai)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = TextCollatorWithPadding(hf_tokenizer)\n",
    "sort_func = partial(sorted_dl_func, hf_tokenizer=hf_tokenizer)\n",
    "batch_decode_tfm = BatchDecodeTransform(hf_tokenizer, hf_arch, hf_config, hf_model, label_names=civil_label_names)\n",
    "\n",
    "dls = dsets.dataloaders(\n",
    "    batch_size=4,\n",
    "    create_batch=data_collator,\n",
    "    after_batch=batch_decode_tfm,\n",
    "    dl_type=partial(SortedDL, sort_func=sort_func),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of batches in train|validation dataloaders: 250 25\n",
      "# of items in each batch: 2\n",
      "\n",
      "Decoded input_ids: [CLS] \"I would not say so, in the ... \n",
      "Targets: tensor([[0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 0, 0, 0, 1, 0, 0]])\n"
     ]
    }
   ],
   "source": [
    "print(\"# of batches in train|validation dataloaders:\", len(train_dl), len(valid_dl))\n",
    "\n",
    "b = next(iter(train_dl))\n",
    "print(\"# of items in each batch:\", len(b))\n",
    "print(\"\")\n",
    "print(f\"Decoded input_ids: {hf_tokenizer.decode(b[0]['input_ids'][0][:10])} ... \")\n",
    "print(\"Targets:\", b[1])\n",
    "\n",
    "# b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>So what protects from a lot of low populated centers controlling the country? I think there's some use for the EC but it would be better if we went with splitting the votes like NE and ME. More people would vote on both sides since their votes could/would actually count.</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Jammeh, another one of the\"Big Man\" clowns that flourish in Africa. Can we set this continent adrift, and not send them anymore money or troops, what a waste.</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dls.show_batch(dataloaders=dls, max_n=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |echo:false\n",
    "try:\n",
    "    del dls, hf_model\n",
    "except:\n",
    "    pass\n",
    "finally:\n",
    "    clean_memory()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mid-Level API\n",
    "\n",
    "BLURR's mid-level API provides a way to build your `DataLoaders` using fast.ai's mid-level `DataBlock` API utilizing one of these three tokenization strategies:\n",
    "\n",
    "1. Using pre-tokenized data (the traditional approach)\n",
    "\n",
    "2. batch-time tokenization (the default approach in previous versions of blurr)\n",
    "\n",
    "2. item-time tokenization (e.g., to apply tokenization on individual items as they are pulled from their respective `Dataset`)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `BatchTokenizeTransform` -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "class BatchTokenizeTransform(Transform):\n",
    "    \"\"\"\n",
    "    Handles everything you need to assemble a mini-batch of inputs and targets, as well as\n",
    "    decode the dictionary produced as a byproduct of the tokenization process in the `encodes` method.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        # The abbreviation/name of your Hugging Face transformer architecture (e.b., bert, bart, etc..)\n",
    "        hf_arch: str,\n",
    "        # A specific configuration instance you want to use\n",
    "        hf_config: PretrainedConfig,\n",
    "        # A Hugging Face tokenizer\n",
    "        hf_tokenizer: PreTrainedTokenizerBase,\n",
    "        # A Hugging Face model\n",
    "        hf_model: PreTrainedModel,\n",
    "        # To control whether the \"include_labels\" are included in your inputs. If they are, the loss will be calculated in \\\n",
    "        # the model's forward function and you can simply use `PreCalculatedLoss` as your `Learner`'s loss function to use it\n",
    "        include_labels: bool = True,\n",
    "        # The token ID that should be ignored when calculating the loss\n",
    "        ignore_token_id: int = CrossEntropyLossFlat().ignore_index,\n",
    "        # To control the length of the padding/truncation. It can be an integer or None, \\\n",
    "        # in which case it will default to the maximum length the model can accept. \\\n",
    "        # If the model has no specific maximum input length, truncation/padding to max_length is deactivated. \\\n",
    "        # See [Everything you always wanted to know about padding and truncation](https://huggingface.co/transformers/preprocessing.html#everything-you-always-wanted-to-know-about-padding-and-truncation)\n",
    "        max_length: int = None,\n",
    "        # To control the `padding` applied to your `hf_tokenizer` during tokenization. \\\n",
    "        # If None, will default to 'False' or 'do_not_pad'. \\\n",
    "        # See [Everything you always wanted to know about padding and truncation](https://huggingface.co/transformers/preprocessing.html#everything-you-always-wanted-to-know-about-padding-and-truncation)\n",
    "        padding: bool | str = True,\n",
    "        # To control `truncation` applied to your `hf_tokenizer` during tokenization. \\\n",
    "        # If None, will default to 'False' or 'do_not_truncate'. \\\n",
    "        # See [Everything you always wanted to know about padding and truncation](https://huggingface.co/transformers/preprocessing.html#everything-you-always-wanted-to-know-about-padding-and-truncation)\n",
    "        truncation: bool | str = True,\n",
    "        # The `is_split_into_words` argument applied to your `hf_tokenizer` during tokenization. \\\n",
    "        # Set this to 'True' if your inputs are pre-tokenized (not numericalized) \\\n",
    "        is_split_into_words: bool = False,\n",
    "        # Any other keyword arguments you want included when using your `hf_tokenizer` to tokenize your inputs\n",
    "        tok_kwargs: dict = {},\n",
    "        # Keyword arguments to apply to `BatchTokenizeTransform`\n",
    "        **kwargs,\n",
    "    ):\n",
    "        store_attr()\n",
    "        self.kwargs = kwargs\n",
    "\n",
    "    def encodes(self, samples, return_batch_encoding=False):\n",
    "        \"\"\"\n",
    "        This method peforms on-the-fly, batch-time tokenization of your data. In other words, your raw inputs\n",
    "        are tokenized as needed for each mini-batch of data rather than requiring pre-tokenization of your full\n",
    "        dataset ahead of time.\n",
    "        \"\"\"\n",
    "        samples = L(samples)\n",
    "\n",
    "        # grab inputs\n",
    "        is_dict = isinstance(samples[0][0], dict)\n",
    "        test_inp = samples[0][0][\"text\"] if is_dict else samples[0][0]\n",
    "\n",
    "        if is_listy(test_inp) and not self.is_split_into_words:\n",
    "            if is_dict:\n",
    "                inps = [(item[\"text\"][0], item[\"text\"][1]) for item in samples.itemgot(0).items]\n",
    "            else:\n",
    "                inps = list(zip(samples.itemgot(0, 0), samples.itemgot(0, 1)))\n",
    "        else:\n",
    "            inps = [item[\"text\"] for item in samples.itemgot(0).items] if is_dict else samples.itemgot(0).items\n",
    "\n",
    "        inputs = self.hf_tokenizer(\n",
    "            inps,\n",
    "            max_length=self.max_length,\n",
    "            padding=self.padding,\n",
    "            truncation=self.truncation,\n",
    "            is_split_into_words=self.is_split_into_words,\n",
    "            return_tensors=\"pt\",\n",
    "            **self.tok_kwargs,\n",
    "        )\n",
    "\n",
    "        d_keys = inputs.keys()\n",
    "\n",
    "        # update the samples with tokenized inputs (e.g. input_ids, attention_mask, etc...), as well as extra information\n",
    "        # if the inputs is a dictionary.\n",
    "        # (< 2.0.0): updated_samples = [(*[{k: inputs[k][idx] for k in d_keys}], *sample[1:]) for idx, sample in enumerate(samples)]\n",
    "        updated_samples = []\n",
    "        for idx, sample in enumerate(samples):\n",
    "            inps = {k: inputs[k][idx] for k in d_keys}\n",
    "            if is_dict:\n",
    "                inps = {\n",
    "                    **inps,\n",
    "                    **{k: v for k, v in sample[0].items() if k not in [\"text\"]},\n",
    "                }\n",
    "\n",
    "            trgs = sample[1:]\n",
    "            if self.include_labels and len(trgs) > 0:\n",
    "                inps[\"label\"] = trgs[0]\n",
    "\n",
    "            updated_samples.append((*[inps], *trgs))\n",
    "\n",
    "        if return_batch_encoding:\n",
    "            return updated_samples, inputs\n",
    "\n",
    "        return updated_samples"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspired by this [article](https://docs.fast.ai/tutorial.transformers.html), `BatchTokenizeTransform` inputs can come in as raw **text**, **a list of words** (e.g., tasks like Named Entity Recognition (NER), where you want to predict the label of each token), or as a **dictionary** that includes extra information you want to use during post-processing.\n",
    "\n",
    "Part of the inspiration for this derives from the mechanics of Hugging Face tokenizers, in particular it can return a collated mini-batch of data given a list of sequences. As such, the collating required for our inputs can be done during tokenization ***before*** our batch transforms run in a `before_batch_tfms` transform (where we get a list of examples)! This allows users of BLURR to have everything done dynamically at batch-time without prior preprocessing with at least four potential benefits:\n",
    "\n",
    "1. No need to pretokenize your text\n",
    "2. Less code\n",
    "3. Faster mini-batch creation\n",
    "4. Less RAM utilization and time spent tokenizing beforehand (this really helps with very large datasets)\n",
    "5. Batch level flexibility"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `ItemTokenizeTransform` -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "class ItemTokenizeTransform(ItemTransform):\n",
    "    split_idx = None\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        # A Hugging Face configuration object\n",
    "        hf_config: PretrainedConfig = None,\n",
    "        # A Hugging Face tokenizer\n",
    "        hf_tokenizer: PreTrainedTokenizerBase = None,\n",
    "        # Any keyword arguments you want your Hugging Face tokenizer to use during tokenization\n",
    "        tok_kwargs: dict = {},\n",
    "        # Any keyword arguments you want applied to `ItemTokenizeTransform`\n",
    "        **kwargs,\n",
    "    ) -> None:\n",
    "        store_attr()\n",
    "\n",
    "        if tok_kwargs.get(\"truncation\", None) is None:\n",
    "            tok_kwargs[\"truncation\"] = True\n",
    "        if tok_kwargs.get(\"max_length\", None) is None:\n",
    "            tok_kwargs[\"max_length\"] = True\n",
    "\n",
    "    def encodes(self, txt, **kwargs):\n",
    "        inputs = self.hf_tokenizer(txt, **self.tok_kwargs)\n",
    "        return dict(inputs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whereas the `BatchTokenizeTransform` allows you to tokenize text at batch-time, `ItemTokenizeTransform` allows you to tokenize text at item-time (e.g., when you get an item out of the dataset). This may be very useful if applying some form of data augmentation to your inputs.\n",
    "\n",
    "In order for this transform to run when getting an item, it needs to be specified as one of your `DataBlock`s `type_tfms`.\n",
    "\n",
    "Potential benefits:\n",
    "\n",
    "1. No need to pretokenize your text\n",
    "2. The ability to apply data augmentation each time an item is pulled from your dataset\n",
    "3. Less RAM utilization and time spent tokenizing beforehand (this really helps with very large datasets)\n",
    "4. Item level flexibility"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `TextBlock` -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "class TextBlock(TransformBlock):\n",
    "    \"\"\"The core `TransformBlock` to prepare your inputs for training in Blurr with fastai's `DataBlock` API\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        # The abbreviation/name of your Hugging Face transformer architecture (not required if passing in an \\\n",
    "        # instance of `BatchTokenizeTransform` to `before_batch_tfm`)\n",
    "        hf_arch: str = None,\n",
    "        # A Hugging Face configuration object (not required if passing in an \\\n",
    "        # instance of `BatchTokenizeTransform` to `before_batch_tfm`)\n",
    "        hf_config: PretrainedConfig = None,\n",
    "        # A Hugging Face tokenizer (not required if passing in an \\\n",
    "        # instance of `BatchTokenizeTransform` to `before_batch_tfm`)\n",
    "        hf_tokenizer: PreTrainedTokenizerBase = None,\n",
    "        # A Hugging Face model (not required if passing in an \\\n",
    "        # instance of `BatchTokenizeTransform` to `before_batch_tfm`)\n",
    "        hf_model: PreTrainedModel = None,\n",
    "        # Any transforms to apply when getting an item from a dataset (useufl for item-time tokenization)\n",
    "        type_tfms: list[ItemTokenizeTransform] = None,\n",
    "        # The \"before_batch\" transform you want to use if tokenizing your raw data on the fly (optional)\n",
    "        tokenize_tfm: Transform = None,\n",
    "        # The batch_tfm you want to decode your inputs into a type that can be used in the fastai show methods, \\\n",
    "        # (defaults to BatchDecodeTransform)\n",
    "        batch_decode_tfm: BatchDecodeTransform = None,\n",
    "        # An instance of `TextCollatorWithPadding` to use when not performing batch-time tokenization, \\\n",
    "        # (defaults to `TextCollatorWithPadding` when using pretokenized or item-time tokenization)\n",
    "        data_collator: TextCollatorWithPadding = None,\n",
    "        # To control whether the \"include_labels\" are included in your inputs. If they are, the loss will be calculated in \\\n",
    "        # the model's forward function and you can simply use `PreCalculatedLoss` as your `Learner`'s loss function to use it\n",
    "        include_labels: bool = True,\n",
    "        # The `is_split_into_words` argument applied to your `hf_tokenizer` during tokenization. \\\n",
    "        # Set this to `True` if your inputs are pre-tokenized (not numericalized)\n",
    "        is_split_into_words: bool = False,\n",
    "        # The return type your decoded inputs should be cast too (used by methods such as `show_batch`)\n",
    "        input_return_type: type = TextInput,\n",
    "        # The type of `DataLoader` you want created (defaults to `SortedDL`)\n",
    "        dl_type: DataLoader = None,\n",
    "        # Any keyword arguments you want applied to your `batch_decode_tfm` (will be set as a fastai `batch_tfms`)\n",
    "        batch_decode_kwargs: dict = {},\n",
    "        # Any keyword arguments you want your Hugging Face tokenizer to use during tokenization\n",
    "        tok_kwargs: dict = {},\n",
    "        # Any keyword arguments you want to have applied with generating text\n",
    "        text_gen_kwargs: dict = {},\n",
    "        # Any keyword arguments you want applied to `TextBlock`\n",
    "        **kwargs,\n",
    "    ):\n",
    "        if (not all([hf_arch, hf_config, hf_tokenizer, hf_model])) and tokenize_tfm is None:\n",
    "            raise ValueError(\"You must supply an hf_arch, hf_config, hf_tokenizer, hf_model -or- a tokenize_tfm\")\n",
    "\n",
    "        # if we are using a transform to tokenize our inputs, grab the HF objects from it\n",
    "        if tokenize_tfm is not None:\n",
    "            hf_arch = getattr(tokenize_tfm, \"hf_arch\", hf_arch)\n",
    "            hf_config = getattr(tokenize_tfm, \"hf_config\", hf_config)\n",
    "            hf_tokenizer = getattr(tokenize_tfm, \"hf_tokenizer\", hf_tokenizer)\n",
    "            hf_model = getattr(tokenize_tfm, \"hf_model\", hf_model)\n",
    "            is_split_into_words = getattr(tokenize_tfm, \"is_split_into_words\", is_split_into_words)\n",
    "            include_labels = getattr(tokenize_tfm, \"include_labels\", include_labels)\n",
    "\n",
    "        # configure our batch decode transform (used by show_batch/results methods)\n",
    "        if batch_decode_tfm is None:\n",
    "            batch_decode_tfm = BatchDecodeTransform(\n",
    "                hf_arch=hf_arch,\n",
    "                hf_config=hf_config,\n",
    "                hf_tokenizer=hf_tokenizer,\n",
    "                hf_model=hf_model,\n",
    "                input_return_type=input_return_type,\n",
    "                **batch_decode_kwargs.copy(),\n",
    "            )\n",
    "\n",
    "        # default to SortedDL using our custom sort function if no `dl_type` is specified\n",
    "        if dl_type is None:\n",
    "            dl_sort_func = partial(\n",
    "                sorted_dl_func, hf_tokenizer=hf_tokenizer, is_split_into_words=is_split_into_words, tok_kwargs=tok_kwargs.copy()\n",
    "            )\n",
    "            dl_type = partial(SortedDL, sort_func=dl_sort_func)\n",
    "\n",
    "        # build our custom `TransformBlock`\n",
    "        if tokenize_tfm is None:\n",
    "            if data_collator is None:\n",
    "                data_collator = TextCollatorWithPadding(hf_tokenizer)\n",
    "            dl_kwargs = {\"create_batch\": data_collator}\n",
    "        else:\n",
    "            dl_kwargs = {\"before_batch\": tokenize_tfm}\n",
    "\n",
    "        return super().__init__(dl_type=dl_type, dls_kwargs=dl_kwargs, type_tfms=type_tfms, batch_tfms=batch_decode_tfm)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mid-Level API: Examples"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pretokenized"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multiclass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 1: HF objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== config ===\n",
      "# of labels:\t2\n",
      "\n",
      "=== tokenizer ===\n",
      "Vocab size:\t\t128000\n",
      "Max # of tokens:\t1000000000000000019884624838656\n",
      "Attributes expected by model in forward pass:\t['input_ids', 'token_type_ids', 'attention_mask']\n"
     ]
    }
   ],
   "source": [
    "hf_arch, hf_config, hf_tokenizer, hf_model = get_task_hf_objects(\"microsoft/deberta-v3-small\", label_names, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  Step 2: `DataBlock`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define DataBlock splitter\n",
    "def _split_func(example):\n",
    "    return example[\"is_valid\"] == True\n",
    "\n",
    "\n",
    "# define how we want to build our targets\n",
    "# note: we don't need to define how to build our inputs because we're using an HF `Dataset` in this example\n",
    "def get_y(example):\n",
    "    return example[\"label\"]\n",
    "\n",
    "\n",
    "# define the DataBlock\n",
    "txt_block = TextBlock(\n",
    "    hf_arch=hf_arch, hf_config=hf_config, hf_tokenizer=hf_tokenizer, hf_model=hf_model, batch_decode_kwargs={\"label_names\": label_names}\n",
    ")\n",
    "\n",
    "blocks = (txt_block, CategoryBlock)\n",
    "dblock = DataBlock(blocks=blocks, get_y=get_y, splitter=FuncSplitter(_split_func))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 3: `DataLoaders`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/wgilliam/.cache/huggingface/datasets/imdb/plain_text/1.0.0/d613c88cf8fa3bab83b4ded3713f1f74830d1100e171db75bbddb80b3345c9c0/cache-36a1cbe769571edf.arrow\n"
     ]
    }
   ],
   "source": [
    "# tokenize the dataset\n",
    "tokenize_func = partial(multiclass_tokenize_func, hf_tokenizer=hf_tokenizer)\n",
    "proc_imdb_ds = imdb_ds.map(tokenize_func, batched=True)\n",
    "\n",
    "# build your `DataLoaders`\n",
    "dls = dblock.dataloaders(proc_imdb_ds, bs=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of items in each batch: 2\n",
      "# of inputs in each batch: 4\n",
      "# of targets in each batch: 4\n",
      "Shape of input_ids (bsz, seq): torch.Size([4, 1423])\n"
     ]
    }
   ],
   "source": [
    "b = dls.one_batch()\n",
    "print(\"# of items in each batch:\", len(b))\n",
    "print(\"# of inputs in each batch:\", len(b[0][\"input_ids\"]))\n",
    "print(\"# of targets in each batch:\", len(b[1]))\n",
    "print(\"Shape of input_ids (bsz, seq):\", b[0][\"input_ids\"].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the actual types represented by our batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{tuple: [dict, fastai.torch_core.TensorCategory]}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "explode_types(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Except for the Brady Variety Hour, this was some of the hokiest television I've seen in a while. The video production qualities weren't too bad, but the overall look and feel were unmistakeably early 80's. And Marie Osmond looks like she did battle with the Avon Lady.. and lost big time. WAY too much eyeliner.&lt;br /&gt;&lt;br /&gt;It was kind of embarrassing to watch veterans Danny Kaye and Eric Severeid take part in this. Even more interesting was watching Alex Haley talk about the African Pavillion in W</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>***SPOILERS*** ***SPOILERS*** Well, seeing as I am a major H:LOTS fan, maybe I liked the movie more than normal people would. However, this movie is still excellent. It had tons of surprises, and it gave some more closure to the series. While I was sad that Bayliss turned into a murderer, the overall feeling I felt was satisfied.</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dls.show_batch(dataloaders=dls, max_n=2, trunc_at=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |echo:false\n",
    "try:\n",
    "    del dls, hf_model\n",
    "except:\n",
    "    pass\n",
    "finally:\n",
    "    clean_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multilabel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 1: HF objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== config ===\n",
      "# of labels:\t7\n",
      "\n",
      "=== tokenizer ===\n",
      "Vocab size:\t\t128000\n",
      "Max # of tokens:\t1000000000000000019884624838656\n",
      "Attributes expected by model in forward pass:\t['input_ids', 'token_type_ids', 'attention_mask']\n"
     ]
    }
   ],
   "source": [
    "# | output: false\n",
    "hf_arch, hf_config, hf_tokenizer, hf_model = get_task_hf_objects(\"microsoft/deberta-v3-small\", civil_label_names, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  Step 2: `DataBlock`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define DataBlock splitter\n",
    "def _split_func(example):\n",
    "    return example[\"is_valid\"] == True\n",
    "\n",
    "\n",
    "# define how we want to build our targets\n",
    "# note: we don't need to define how to build our inputs because we're using an HF `Dataset` in this example\n",
    "def get_y(example):\n",
    "    return example[\"label\"]\n",
    "\n",
    "\n",
    "# define the DataBlock\n",
    "blocks = (\n",
    "    TextBlock(hf_arch=hf_arch, hf_config=hf_config, hf_tokenizer=hf_tokenizer, hf_model=hf_model),\n",
    "    MultiCategoryBlock(encoded=True, vocab=civil_label_names),\n",
    ")\n",
    "dblock = DataBlock(blocks=blocks, get_y=get_y, splitter=FuncSplitter(_split_func))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 3: `DataLoaders`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/wgilliam/.cache/huggingface/datasets/civil_comments/default/0.9.0/e7a3aacd2ab7d135fa958e7209d10b1fa03807d44c486e3c34897aa08ea8ffab/cache-811c491bc0819778.arrow\n"
     ]
    }
   ],
   "source": [
    "# tokenize the dataset\n",
    "tokenize_func = partial(multilabel_tokenize_func, hf_tokenizer=hf_tokenizer, label_attrs=civil_label_names)\n",
    "proc_civil_ds = civil_ds.map(tokenize_func, batched=True)\n",
    "\n",
    "dls = dblock.dataloaders(proc_civil_ds, bs=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of items in each batch: 2\n",
      "# of inputs in each batch: 4\n",
      "# of targets in each batch: 4\n",
      "Shape of input_ids (bsz, seq): torch.Size([4, 237])\n"
     ]
    }
   ],
   "source": [
    "b = dls.one_batch()\n",
    "print(\"# of items in each batch:\", len(b))\n",
    "print(\"# of inputs in each batch:\", len(b[0][\"input_ids\"]))\n",
    "print(\"# of targets in each batch:\", len(b[1]))\n",
    "print(\"Shape of input_ids (bsz, seq):\", b[0][\"input_ids\"].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the actual types represented by our batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{tuple: [dict, fastai.torch_core.TensorMultiCategory]}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "explode_types(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>So what protects from a lot of low populated centers controlling the country? I think there's some use for the EC but it would be better if we went with splitting the votes like NE and ME. More people would vote on both sides since their votes could/would actually count.</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Unfortunately, we have endured 50 years of disarray, our theology of worship has degenerated to the childish while theirs is robust and our instability could undermine their relative stability. It is hard to see what we have to offer them at this point.</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dls.show_batch(dataloaders=dls, max_n=2, trunc_at=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |echo:false\n",
    "try:\n",
    "    del dls, hf_model\n",
    "except:\n",
    "    pass\n",
    "finally:\n",
    "    clean_memory()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch-Time Tokenization"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multiclass"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 1: HF objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== config ===\n",
      "# of labels:\t2\n",
      "\n",
      "=== tokenizer ===\n",
      "Vocab size:\t\t128000\n",
      "Max # of tokens:\t1000000000000000019884624838656\n",
      "Attributes expected by model in forward pass:\t['input_ids', 'token_type_ids', 'attention_mask']\n"
     ]
    }
   ],
   "source": [
    "# | output: false\n",
    "hf_arch, hf_config, hf_tokenizer, hf_model = get_task_hf_objects(\"microsoft/deberta-v3-small\", label_names, verbose=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  Step 2: `DataBlock`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenize_tfm = BatchTokenizeTransform(hf_arch, hf_config, hf_tokenizer, hf_model)\n",
    "\n",
    "# note: we pass the label_names here because the labels in the dataset are already encoded as 0 or 1\n",
    "blocks = (\n",
    "    TextBlock(tokenize_tfm=tokenize_tfm, batch_decode_kwargs={\"label_names\": label_names}),\n",
    "    CategoryBlock,\n",
    ")\n",
    "dblock = DataBlock(\n",
    "    blocks=blocks,\n",
    "    get_x=ColReader(\"text\"),\n",
    "    get_y=ColReader(\"label\"),\n",
    "    splitter=ColSplitter(),\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 3: `DataLoaders`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dls = dblock.dataloaders(imdb_df, bs=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of items in each batch: 2\n",
      "# of inputs in each batch: 4\n",
      "# of targets in each batch: 4\n",
      "Shape of input_ids (bsz, seq): torch.Size([4, 1423])\n"
     ]
    }
   ],
   "source": [
    "b = dls.one_batch()\n",
    "print(\"# of items in each batch:\", len(b))\n",
    "print(\"# of inputs in each batch:\", len(b[0][\"input_ids\"]))\n",
    "print(\"# of targets in each batch:\", len(b[1]))\n",
    "print(\"Shape of input_ids (bsz, seq):\", b[0][\"input_ids\"].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the actual types represented by our batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{tuple: [dict, fastai.torch_core.TensorCategory]}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "explode_types(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The majority of Stephen King's short stories are little gems, with original ideas that don't take a long time to develop; basically lean and mean--he sets them up quickly in a scarce number of pages, you read 'em, and you're finished before you know you've begun. They're like the equivalent of a carton of McDonald's fries--they taste Really good and you know there's not much nutritional value in them (re: from a literary standpoint, they don't say much about the universal human condition), but y</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;br /&gt;&lt;br /&gt;As usual, I was really looking forward to a new TV/film on a favourite subject of mine - makes a nice change from a *strangely familiar* documentary about Kursk or Stalingrad on the History Channel.&lt;br /&gt;&lt;br /&gt;I avidly looked forward to Pearl Harbour and Enemy at the Gates - but was rudely brought down to earth with the realisation of the malevolent, stupid-ifying power of Hollywood - and its ability to spend an absolute fortune on tripe.&lt;br /&gt;&lt;br /&gt;So yet again I got excited about '</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dls.show_batch(dataloaders=dls, max_n=2, trunc_at=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |echo:false\n",
    "try:\n",
    "    del dls, hf_model\n",
    "except:\n",
    "    pass\n",
    "finally:\n",
    "    clean_memory()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multilabel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 1: HF objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== config ===\n",
      "# of labels:\t7\n",
      "\n",
      "=== tokenizer ===\n",
      "Vocab size:\t\t128000\n",
      "Max # of tokens:\t1000000000000000019884624838656\n",
      "Attributes expected by model in forward pass:\t['input_ids', 'token_type_ids', 'attention_mask']\n"
     ]
    }
   ],
   "source": [
    "# | output: false\n",
    "hf_arch, hf_config, hf_tokenizer, hf_model = get_task_hf_objects(\"microsoft/deberta-v3-small\", civil_label_names, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  Step 2: `DataBlock`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenize_tfm = BatchTokenizeTransform(hf_arch, hf_config, hf_tokenizer, hf_model)\n",
    "\n",
    "blocks = (TextBlock(tokenize_tfm=tokenize_tfm), MultiCategoryBlock(encoded=True, vocab=civil_label_names))\n",
    "dblock = DataBlock(blocks=blocks, get_x=ColReader(\"text\"), get_y=ColReader(civil_label_names), splitter=ColSplitter())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 3: `DataLoaders`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dls = dblock.dataloaders(civil_df, bs=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of items in each batch: 2\n",
      "# of inputs in each batch: 4\n",
      "# of targets in each batch: 4\n",
      "Shape of input_ids (bsz, seq): torch.Size([4, 237])\n"
     ]
    }
   ],
   "source": [
    "b = dls.one_batch()\n",
    "print(\"# of items in each batch:\", len(b))\n",
    "print(\"# of inputs in each batch:\", len(b[0][\"input_ids\"]))\n",
    "print(\"# of targets in each batch:\", len(b[1]))\n",
    "print(\"Shape of input_ids (bsz, seq):\", b[0][\"input_ids\"].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the actual types represented by our batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{tuple: [dict, fastai.torch_core.TensorMultiCategory]}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "explode_types(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I am sure glad that we live in REAL rural Alaska. No government, no city, no zoning, no political B.S., and no nosy neighbors telling us what to do! Neighbors out here are like family, and we always help each other out. We work hard, raised our families, grandchildren come for the summers, we know were our salvation comes from, and we grow older and pass away. Here is a Alaskan thought. Instead of bashing and condemning these people. With David Szabo's permission. How about folks starting a fund</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(continuation) schools better. So without your energy, enthusiasm, and your tax money, your schools WILL continue to decline, as will your property values, your infastructure, your public services (fire, trash, etc.). It is a big issue, and you can't think only of schools, because it is all connected. That is the big picture. Now the small picture: if you feel like you want the best for your kids, and believe the public schools aren't what you consider the best, so be it. But what you and your f</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dls.show_batch(dataloaders=dls, max_n=2, trunc_at=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |echo:false\n",
    "try:\n",
    "    del dls, hf_model\n",
    "except:\n",
    "    pass\n",
    "finally:\n",
    "    clean_memory()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Item-Time Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multiclass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 1: HF objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== config ===\n",
      "# of labels:\t2\n",
      "\n",
      "=== tokenizer ===\n",
      "Vocab size:\t\t128000\n",
      "Max # of tokens:\t1000000000000000019884624838656\n",
      "Attributes expected by model in forward pass:\t['input_ids', 'token_type_ids', 'attention_mask']\n"
     ]
    }
   ],
   "source": [
    "# | output: false\n",
    "hf_arch, hf_config, hf_tokenizer, hf_model = get_task_hf_objects(\"microsoft/deberta-v3-small\", label_names, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  Step 2: `DataBlock`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfm = ItemTokenizeTransform(hf_config=hf_config, hf_tokenizer=hf_tokenizer)\n",
    "tfm.split_idx = 0\n",
    "\n",
    "tfm2 = ItemTokenizeTransform(hf_config=hf_config, hf_tokenizer=hf_tokenizer)\n",
    "tfm2.split_idx = 1\n",
    "\n",
    "blocks = (\n",
    "    TextBlock(\n",
    "        hf_arch=hf_arch,\n",
    "        hf_config=hf_config,\n",
    "        hf_tokenizer=hf_tokenizer,\n",
    "        hf_model=hf_model,\n",
    "        type_tfms=[tfm, tfm2],\n",
    "        batch_decode_kwargs={\"label_names\": label_names},\n",
    "    ),\n",
    "    CategoryBlock,\n",
    ")\n",
    "\n",
    "dblock = DataBlock(\n",
    "    blocks=blocks,\n",
    "    get_x=ColReader(\"text\"),\n",
    "    get_y=ColReader(\"label\"),\n",
    "    splitter=ColSplitter(),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 3: `DataLoaders`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dls = dblock.dataloaders(imdb_df, bs=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of items in each batch: 2\n",
      "# of inputs in each batch: 4\n",
      "# of targets in each batch: 4\n",
      "Shape of input_ids (bsz, seq): torch.Size([4, 1423])\n"
     ]
    }
   ],
   "source": [
    "b = dls.one_batch()\n",
    "print(\"# of items in each batch:\", len(b))\n",
    "print(\"# of inputs in each batch:\", len(b[0][\"input_ids\"]))\n",
    "print(\"# of targets in each batch:\", len(b[1]))\n",
    "print(\"Shape of input_ids (bsz, seq):\", b[0][\"input_ids\"].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the actual types represented by our batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{tuple: [dict, fastai.torch_core.TensorCategory]}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "explode_types(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The majority of Stephen King's short stories are little gems, with original ideas that don't take a long time to develop; basically lean and mean--he sets them up quickly in a scarce number of pages, you read 'em, and you're finished before you know you've begun. They're like the equivalent of a carton of McDonald's fries--they taste Really good and you know there's not much nutritional value in them (re: from a literary standpoint, they don't say much about the universal human condition), but y</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Scarecrows is one of those films that, with a little more acting, a little more direction, and a lot more story logic, would have been quite compelling as a horror entry. As it stands, it is still a creepy film that has solid make-up and gore effects, and a premise that sustains the mood of terror in spite of itself. And hey, there are no teenagers getting killed one by one--just dumb adults, so that is a refreshing change of pace. And the plot line is amazingly similar to Dead Birds, with a pre</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dls.show_batch(dataloaders=dls, max_n=2, trunc_at=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |echo:false\n",
    "try:\n",
    "    del dls, hf_model\n",
    "except:\n",
    "    pass\n",
    "finally:\n",
    "    clean_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multilabel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 1: HF objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== config ===\n",
      "# of labels:\t7\n",
      "\n",
      "=== tokenizer ===\n",
      "Vocab size:\t\t128000\n",
      "Max # of tokens:\t1000000000000000019884624838656\n",
      "Attributes expected by model in forward pass:\t['input_ids', 'token_type_ids', 'attention_mask']\n"
     ]
    }
   ],
   "source": [
    "# | output: false\n",
    "hf_arch, hf_config, hf_tokenizer, hf_model = get_task_hf_objects(\"microsoft/deberta-v3-small\", civil_label_names, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  Step 2: `DataBlock`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfm = ItemTokenizeTransform(hf_config=hf_config, hf_tokenizer=hf_tokenizer)\n",
    "tfm.split_idx = 0\n",
    "\n",
    "tfm2 = ItemTokenizeTransform(hf_config=hf_config, hf_tokenizer=hf_tokenizer)\n",
    "tfm2.split_idx = 1\n",
    "\n",
    "blocks = (\n",
    "    TextBlock(hf_arch=hf_arch, hf_config=hf_config, hf_tokenizer=hf_tokenizer, hf_model=hf_model, type_tfms=[tfm, tfm2]),\n",
    "    MultiCategoryBlock(encoded=True, vocab=civil_label_names),\n",
    ")\n",
    "\n",
    "dblock = DataBlock(\n",
    "    blocks=blocks,\n",
    "    get_x=ColReader(\"text\"),\n",
    "    get_y=ColReader(civil_label_names),\n",
    "    splitter=ColSplitter(),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 3: `DataLoaders`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dls = dblock.dataloaders(civil_df, bs=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of items in each batch: 2\n",
      "# of inputs in each batch: 4\n",
      "# of targets in each batch: 4\n",
      "Shape of input_ids (bsz, seq): torch.Size([4, 237])\n"
     ]
    }
   ],
   "source": [
    "b = dls.one_batch()\n",
    "print(\"# of items in each batch:\", len(b))\n",
    "print(\"# of inputs in each batch:\", len(b[0][\"input_ids\"]))\n",
    "print(\"# of targets in each batch:\", len(b[1]))\n",
    "print(\"Shape of input_ids (bsz, seq):\", b[0][\"input_ids\"].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the actual types represented by our batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{tuple: [dict, fastai.torch_core.TensorMultiCategory]}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "explode_types(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I am sure glad that we live in REAL rural Alaska. No government, no city, no zoning, no political B.S., and no nosy neighbors telling us what to do! Neighbors out here are like family, and we always help each other out. We work hard, raised our families, grandchildren come for the summers, we know were our salvation comes from, and we grow older and pass away. Here is a Alaskan thought. Instead of bashing and condemning these people. With David Szabo's permission. How about folks starting a fund</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"Another example of their manipulation of information is their belief that political correctness is a big problem.\" Yes, I keep hearing this from people on the right, presumably because they think it's a big problem on the left. Here's what I think people really mean when they talk about political correctness in a pejorative sense: \"I'm mad because I can't say what I'm really thinking about certain people because it will upset people and then backfire on me.\" The issue here, therefore, is not on</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dls.show_batch(dataloaders=dls, max_n=2, trunc_at=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |echo:false\n",
    "try:\n",
    "    del dls, hf_model\n",
    "except:\n",
    "    pass\n",
    "finally:\n",
    "    clean_memory()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tests\n",
    "\n",
    "The tests below to ensure the core DataBlock code above works for **all** pretrained sequence classification models available in Hugging Face.  These tests are excluded from the CI workflow because of how long they would take to run and the amount of data that would be required to download.\n",
    "\n",
    "**Note**: Feel free to modify the code below to test whatever pretrained classification models you are working with ... and if any of your pretrained sequence classification models fail, please submit a github issue *(or a PR if you'd like to fix it yourself)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "import nbdev\n",
    "\n",
    "nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
